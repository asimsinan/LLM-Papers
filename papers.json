[{"title": "\"I Want It That Way\": Enabling Interactive Decision Support Using Large\n  Language Models and Constraint Programming", "url": "http://arxiv.org/abs/2312.06908v1", "authors": "Connor Lawless, Jakob Schoeffer, Lindy Le, Kael Rowan, Shilad Sen, Cristina St. Hill, Jina Suh, Bahar Sarrafzadeh", "abstract": "  A critical factor in the success of decision support systems is the accurate\nmodeling of user preferences. Psychology research has demonstrated that users\noften develop their preferences during the elicitation process, highlighting\nthe pivotal role of system-user interaction in developing personalized systems.\nThis paper introduces a novel approach, combining Large Language Models (LLMs)\nwith Constraint Programming to facilitate interactive decision support. We\nstudy this hybrid framework through the lens of meeting scheduling, a\ntime-consuming daily activity faced by a multitude of information workers. We\nconduct three studies to evaluate the novel framework, including a diary study\n(n=64) to characterize contextual scheduling preferences, a quantitative\nevaluation of the system's performance, and a user study (n=10) with a\nprototype system. Our work highlights the potential for a hybrid LLM and\noptimization approach for iterative preference elicitation and design\nconsiderations for building systems that support human-system collaborative\ndecision-making processes.\n", "published": "12-12-2023", "year": "2023", "categories": []}, {"title": "A Bibliometric Review of Large Language Models Research from 2017 to\n  2023", "url": "http://arxiv.org/abs/2304.02020v1", "authors": "Lizhou Fan, Lingyao Li, Zihui Ma, Sanggyu Lee, Huizi Yu, Libby Hemphill", "abstract": "  Large language models (LLMs) are a class of language models that have\ndemonstrated outstanding performance across a range of natural language\nprocessing (NLP) tasks and have become a highly sought-after research area,\nbecause of their ability to generate human-like language and their potential to\nrevolutionize science and technology. In this study, we conduct bibliometric\nand discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000\npublications, this paper serves as a roadmap for researchers, practitioners,\nand policymakers to navigate the current landscape of LLMs research. We present\nthe research trends from 2017 to early 2023, identifying patterns in research\nparadigms and collaborations. We start with analyzing the core algorithm\ndevelopments and NLP tasks that are fundamental in LLMs research. We then\ninvestigate the applications of LLMs in various fields and domains including\nmedicine, engineering, social science, and humanities. Our review also reveals\nthe dynamic, fast-paced evolution of LLMs research. Overall, this paper offers\nvaluable insights into the current state, impact, and potential of LLMs\nresearch and its applications.\n", "published": "03-04-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "A Comprehensive Study of Knowledge Editing for Large Language Models", "url": "http://arxiv.org/abs/2401.01286v3", "authors": "Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen", "abstract": "  Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\ngive a deeper understanding of the knowledge structures inherent within LLMs.\nFinally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.\n", "published": "02-01-2024", "year": "2024", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "A Comprehensive Survey of Attack Techniques, Implementation, and\n  Mitigation Strategies in Large Language Models", "url": "http://arxiv.org/abs/2312.10982v1", "authors": "Aysan Esmradi, Daniel Wankit Yip, Chun Fai Chan", "abstract": "  Ensuring the security of large language models (LLMs) is an ongoing challenge\ndespite their widespread popularity. Developers work to enhance LLMs security,\nbut vulnerabilities persist, even in advanced versions like GPT-4. Attackers\nexploit these weaknesses, highlighting the need for proactive cybersecurity\nmeasures in AI model development. This article explores two attack categories:\nattacks on models themselves and attacks on model applications. The former\nrequires expertise, access to model data, and significant implementation time,\nwhile the latter is more accessible to attackers and has seen increased\nattention. Our study reviews over 100 recent research works, providing an\nin-depth analysis of each attack type. We identify the latest attack methods\nand explore various approaches to carry them out. We thoroughly investigate\nmitigation techniques, assessing their effectiveness and limitations.\nFurthermore, we summarize future defenses against these attacks. We also\nexamine real-world techniques, including reported and our implemented attacks\non LLMs, to consolidate our findings. Our research highlights the urgency of\naddressing security concerns and aims to enhance the understanding of LLM\nattacks, contributing to robust defense development in this evolving domain.\n", "published": "18-12-2023", "year": "2023", "categories": []}, {"title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large\n  Language Models", "url": "http://arxiv.org/abs/2401.01313v3", "authors": "S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, Amitava Das", "abstract": "  As Large Language Models (LLMs) continue to advance in their ability to write\nhuman-like text, a key challenge remains around their tendency to hallucinate\ngenerating content that appears factual but is ungrounded. This issue of\nhallucination is arguably the biggest hindrance to safely deploying these\npowerful LLMs into real-world production systems that impact people's lives.\nThe journey toward widespread adoption of LLMs in practical settings heavily\nrelies on addressing and mitigating hallucinations. Unlike traditional AI\nsystems focused on limited tasks, LLMs have been exposed to vast amounts of\nonline text data during training. While this allows them to display impressive\nlanguage fluency, it also means they are capable of extrapolating information\nfrom the biases in training data, misinterpreting ambiguous prompts, or\nmodifying the information to align superficially with the input. This becomes\nhugely alarming when we rely on language generation capabilities for sensitive\napplications, such as summarizing medical records, financial analysis reports,\netc. This paper presents a comprehensive survey of over 32 techniques developed\nto mitigate hallucination in LLMs. Notable among these are Retrieval Augmented\nGeneration (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),\nCoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we\nintroduce a detailed taxonomy categorizing these methods based on various\nparameters, such as dataset utilization, common tasks, feedback mechanisms, and\nretriever types. This classification helps distinguish the diverse approaches\nspecifically designed to tackle hallucination issues in LLMs. Additionally, we\nanalyze the challenges and limitations inherent in these techniques, providing\na solid foundation for future research in addressing hallucinations and related\nphenomena within the realm of LLMs.\n", "published": "02-01-2024", "year": "2024", "categories": ["Computation and Language"]}, {"title": "A Computational Framework for Behavioral Assessment of LLM Therapists", "url": "http://arxiv.org/abs/2401.00820v1", "authors": "Yu Ying Chiu, Ashish Sharma, Inna Wanyin Lin, Tim Althoff", "abstract": "  The emergence of ChatGPT and other large language models (LLMs) has greatly\nincreased interest in utilizing LLMs as therapists to support individuals\nstruggling with mental health challenges. However, due to the lack of\nsystematic studies, our understanding of how LLM therapists behave, i.e., ways\nin which they respond to clients, is significantly limited. Understanding their\nbehavior across a wide range of clients and situations is crucial to accurately\nassess their capabilities and limitations in the high-risk setting of mental\nhealth, where undesirable behaviors can lead to severe consequences. In this\npaper, we propose BOLT, a novel computational framework to study the\nconversational behavior of LLMs when employed as therapists. We develop an\nin-context learning method to quantitatively measure the behavior of LLMs based\non 13 different psychotherapy techniques including reflections, questions,\nsolutions, normalizing, and psychoeducation. Subsequently, we compare the\nbehavior of LLM therapists against that of high- and low-quality human therapy,\nand study how their behavior can be modulated to better reflect behaviors\nobserved in high-quality therapy. Our analysis of GPT and Llama-variants\nreveals that these LLMs often resemble behaviors more commonly exhibited in\nlow-quality therapy rather than high-quality therapy, such as offering a higher\ndegree of problem-solving advice when clients share emotions, which is against\ntypical recommendations. At the same time, unlike low-quality therapy, LLMs\nreflect significantly more upon clients' needs and strengths. Our analysis\nframework suggests that despite the ability of LLMs to generate anecdotal\nexamples that appear similar to human therapists, LLM therapists are currently\nnot fully consistent with high-quality care, and thus require additional\nresearch to ensure quality care.\n", "published": "01-01-2024", "year": "2024", "categories": ["Computation and Language"]}, {"title": "A Generalist Agent", "url": "http://arxiv.org/abs/2205.06175v3", "authors": "Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, Nando de Freitas", "abstract": "  Inspired by progress in large-scale language modeling, we apply a similar\napproach towards building a single generalist agent beyond the realm of text\noutputs. The agent, which we refer to as Gato, works as a multi-modal,\nmulti-task, multi-embodiment generalist policy. The same network with the same\nweights can play Atari, caption images, chat, stack blocks with a real robot\narm and much more, deciding based on its context whether to output text, joint\ntorques, button presses, or other tokens. In this report we describe the model\nand the data, and document the current capabilities of Gato.\n", "published": "12-05-2022", "year": "2022", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT", "url": "http://arxiv.org/abs/2302.11382v1", "authors": "Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, Douglas C. Schmidt", "abstract": "  Prompt engineering is an increasingly important skill set needed to converse\neffectively with large language models (LLMs), such as ChatGPT. Prompts are\ninstructions given to an LLM to enforce rules, automate processes, and ensure\nspecific qualities (and quantities) of generated output. Prompts are also a\nform of programming that can customize the outputs and interactions with an\nLLM. This paper describes a catalog of prompt engineering techniques presented\nin pattern form that have been applied to solve common problems when conversing\nwith LLMs. Prompt patterns are a knowledge transfer method analogous to\nsoftware patterns since they provide reusable solutions to common problems\nfaced in a particular context, i.e., output generation and interaction when\nworking with LLMs. This paper provides the following contributions to research\non prompt engineering that apply LLMs to automate software development tasks.\nFirst, it provides a framework for documenting patterns for structuring prompts\nto solve a range of problems so that they can be adapted to different domains.\nSecond, it presents a catalog of patterns that have been applied successfully\nto improve the outputs of LLM conversations. Third, it explains how prompts can\nbe built from multiple patterns and illustrates prompt patterns that benefit\nfrom combination with other prompt patterns.\n", "published": "21-02-2023", "year": "2023", "categories": ["Artificial Intelligence"]}, {"title": "A Real-World WebAgent with Planning, Long Context Understanding, and\n  Program Synthesis", "url": "http://arxiv.org/abs/2307.12856v3", "authors": "Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust", "abstract": "  Pre-trained large language models (LLMs) have recently achieved better\ngeneralization and sample efficiency in autonomous web automation. However, the\nperformance on real-world websites has still suffered from (1) open domainness,\n(2) limited context length, and (3) lack of inductive bias on HTML. We\nintroduce WebAgent, an LLM-driven agent that learns from self-experience to\ncomplete tasks on real websites following natural language instructions.\nWebAgent plans ahead by decomposing instructions into canonical\nsub-instructions, summarizes long HTML documents into task-relevant snippets,\nand acts on websites via Python programs generated from those. We design\nWebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new\npre-trained LLMs for long HTML documents using local and global attention\nmechanisms and a mixture of long-span denoising objectives, for planning and\nsummarization. We empirically demonstrate that our modular recipe improves the\nsuccess on real websites by over 50%, and that HTML-T5 is the best model to\nsolve various HTML understanding tasks; achieving 18.7% higher success rate\nthan the prior method on MiniWoB web automation benchmark, and SoTA performance\non Mind2Web, an offline task planning evaluation.\n", "published": "24-07-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "A Survey of Large Language Models in Medicine: Principles, Applications,\n  and Challenges", "url": "http://arxiv.org/abs/2311.05112v2", "authors": "Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge Wu, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Xian Wu, Yefeng Zheng, Lei Clifton, Zheng Li, Jiebo Luo, David A. Clifton", "abstract": "  Large language models (LLMs), such as ChatGPT, have received substantial\nattention due to their impressive human language understanding and generation\ncapabilities. Therefore, the application of LLMs in medicine to assist\nphysicians and patient care emerges as a promising research direction in both\nartificial intelligence and clinical medicine. To reflect this trend, this\nsurvey provides a comprehensive overview of the principles, applications, and\nchallenges faced by LLMs in medicine. Specifically, we aim to address the\nfollowing questions: 1) How can medical LLMs be built? 2) What are the\ndownstream performances of medical LLMs? 3) How can medical LLMs be utilized in\nreal-world clinical practice? 4) What challenges arise from the use of medical\nLLMs? and 5) How can we better construct and utilize medical LLMs? As a result,\nthis survey aims to provide insights into the opportunities and challenges of\nLLMs in medicine and serve as a valuable resource for constructing practical\nand effective medical LLMs. A regularly updated list of practical guides on\nmedical LLMs can be found at\nhttps://github.com/AI-in-Health/MedLLMsPracticalGuide.\n", "published": "09-11-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "A Survey of Reasoning with Foundation Models", "url": "http://arxiv.org/abs/2312.11562v4", "authors": "Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe Ren, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang, Pheng Ann Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Ji-Rong Wen, Xipeng Qiu, Yike Guo, Hui Xiong, Qun Liu, Zhenguo Li", "abstract": "  Reasoning, a crucial ability for complex problem-solving, plays a pivotal\nrole in various real-world settings such as negotiation, medical diagnosis, and\ncriminal investigation. It serves as a fundamental methodology in the field of\nArtificial General Intelligence (AGI). With the ongoing development of\nfoundation models, there is a growing interest in exploring their abilities in\nreasoning tasks. In this paper, we introduce seminal foundation models proposed\nor adaptable for reasoning, highlighting the latest advancements in various\nreasoning tasks, methods, and benchmarks. We then delve into the potential\nfuture directions behind the emergence of reasoning abilities within foundation\nmodels. We also discuss the relevance of multimodal learning, autonomous\nagents, and super alignment in the context of reasoning. By discussing these\nfuture research directions, we hope to inspire researchers in their exploration\nof this field, stimulate further advancements in reasoning with foundation\nmodels, and contribute to the development of AGI.\n", "published": "17-12-2023", "year": "2023", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "A Survey on Hallucination in Large Language Models: Principles,\n  Taxonomy, Challenges, and Open Questions", "url": "http://arxiv.org/abs/2311.05232v1", "authors": "Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu", "abstract": "  The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), leading to remarkable\nadvancements in text understanding and generation. Nevertheless, alongside\nthese strides, LLMs exhibit a critical tendency to produce hallucinations,\nresulting in content that is inconsistent with real-world facts or user inputs.\nThis phenomenon poses substantial challenges to their practical deployment and\nraises concerns over the reliability of LLMs in real-world scenarios, which\nattracts increasing attention to detect and mitigate these hallucinations. In\nthis survey, we aim to provide a thorough and in-depth overview of recent\nadvances in the field of LLM hallucinations. We begin with an innovative\ntaxonomy of LLM hallucinations, then delve into the factors contributing to\nhallucinations. Subsequently, we present a comprehensive overview of\nhallucination detection methods and benchmarks. Additionally, representative\napproaches designed to mitigate hallucinations are introduced accordingly.\nFinally, we analyze the challenges that highlight the current limitations and\nformulate open questions, aiming to delineate pathways for future research on\nhallucinations in LLMs.\n", "published": "09-11-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "A collection of principles for guiding and evaluating large language\n  models", "url": "http://arxiv.org/abs/2312.10059v1", "authors": "Konstantin Hebenstreit, Robert Praas, Matthias Samwald", "abstract": "  Large language models (LLMs) demonstrate outstanding capabilities, but\nchallenges remain regarding their ability to solve complex reasoning tasks, as\nwell as their transparency, robustness, truthfulness, and ethical alignment. In\nthis preliminary study, we compile a set of core principles for steering and\nevaluating the reasoning of LLMs by curating literature from several relevant\nstrands of work: structured reasoning in LLMs, self-evaluation/self-reflection,\nexplainability, AI system safety/security, guidelines for human critical\nthinking, and ethical/regulatory guidelines for AI. We identify and curate a\nlist of 220 principles from literature, and derive a set of 37 core principles\norganized into seven categories: assumptions and perspectives, reasoning,\ninformation and evidence, robustness and security, ethics, utility, and\nimplications. We conduct a small-scale expert survey, eliciting the subjective\nimportance experts assign to different principles and lay out avenues for\nfuture work beyond our preliminary results. We envision that the development of\na shared model of principles can serve multiple purposes: monitoring and\nsteering models at inference time, improving model behavior during training,\nand guiding human evaluation of model reasoning.\n", "published": "04-12-2023", "year": "2023", "categories": []}, {"title": "ADaPT: As-Needed Decomposition and Planning with Language Models", "url": "http://arxiv.org/abs/2311.05772v1", "authors": "Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, Tushar Khot", "abstract": "  Large Language Models (LLMs) are increasingly being used for interactive\ndecision-making tasks requiring planning and adapting to the environment.\nRecent works employ LLMs-as-agents in broadly two ways: iteratively determining\nthe next action (iterative executors) or generating plans and executing\nsub-tasks using LLMs (plan-and-execute). However, these methods struggle with\ntask complexity, as the inability to execute any sub-task may lead to task\nfailure. To address these shortcomings, we introduce As-Needed Decomposition\nand Planning for complex Tasks (ADaPT), an approach that explicitly plans and\ndecomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute\nthem. ADaPT recursively decomposes sub-tasks to adapt to both task complexity\nand LLM capability. Our results demonstrate that ADaPT substantially\noutperforms established strong baselines, achieving success rates up to 28.3%\nhigher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel\ncompositional dataset that we introduce. Through extensive analysis, we\nillustrate the importance of multilevel decomposition and establish that ADaPT\ndynamically adjusts to the capabilities of the executor LLM as well as to task\ncomplexity.\n", "published": "08-11-2023", "year": "2023", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "ALCUNA: Large Language Models Meet New Knowledge", "url": "http://arxiv.org/abs/2310.14820v1", "authors": "Xunjian Yin, Baizhou Huang, Xiaojun Wan", "abstract": "  With the rapid development of NLP, large-scale language models (LLMs) excel\nin various tasks across multiple domains now. However, existing benchmarks may\nnot adequately measure these models' capabilities, especially when faced with\nnew knowledge. In this paper, we address the lack of benchmarks to evaluate\nLLMs' ability to handle new knowledge, an important and challenging aspect in\nthe rapidly evolving world. We propose an approach called KnowGen that\ngenerates new knowledge by altering existing entity attributes and\nrelationships, resulting in artificial entities that are distinct from\nreal-world entities. With KnowGen, we introduce a benchmark named ALCUNA to\nassess LLMs' abilities in knowledge understanding, differentiation, and\nassociation. We benchmark several LLMs, reveals that their performance in face\nof new knowledge is not satisfactory, particularly in reasoning between new and\ninternal knowledge. We also explore the impact of entity similarity on the\nmodel's understanding of entity knowledge and the influence of contextual\nentities. We appeal to the need for caution when using LLMs in new scenarios or\nwith new knowledge, and hope that our benchmarks can help drive the development\nof LLMs in face of new knowledge.\n", "published": "23-10-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "ART: Automatic multi-step reasoning and tool-use for large language\n  models", "url": "http://arxiv.org/abs/2303.09014v1", "authors": "Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio Ribeiro", "abstract": "  Large language models (LLMs) can perform complex reasoning in few- and\nzero-shot settings by generating intermediate chain of thought (CoT) reasoning\nsteps. Further, each reasoning step can rely on external tools to support\ncomputation beyond the core LLM capabilities (e.g. search/running code). Prior\nwork on CoT prompting and tool use typically requires hand-crafting\ntask-specific demonstrations and carefully scripted interleaving of model\ngenerations with tool use. We introduce Automatic Reasoning and Tool-use (ART),\na framework that uses frozen LLMs to automatically generate intermediate\nreasoning steps as a program. Given a new task to solve, ART selects\ndemonstrations of multi-step reasoning and tool use from a task library. At\ntest time, ART seamlessly pauses generation whenever external tools are called,\nand integrates their output before resuming generation. ART achieves a\nsubstantial improvement over few-shot prompting and automatic CoT on unseen\ntasks in the BigBench and MMLU benchmarks, and matches performance of\nhand-crafted CoT prompts on a majority of these tasks. ART is also extensible,\nand makes it easy for humans to improve performance by correcting errors in\ntask-specific programs or incorporating new tools, which we demonstrate by\ndrastically improving performance on select tasks with minimal human\nintervention.\n", "published": "16-03-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Active Prompting with Chain-of-Thought for Large Language Models", "url": "http://arxiv.org/abs/2302.12246v3", "authors": "Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang", "abstract": "  The increasing scale of large language models (LLMs) brings emergent\nabilities to various complex tasks requiring reasoning, such as arithmetic and\ncommonsense reasoning. It is known that the effective design of task-specific\nprompts is critical for LLMs' ability to produce high-quality answers. In\nparticular, an effective approach for complex question-and-answer tasks is\nexample-based prompting with chain-of-thought (CoT) reasoning, which\nsignificantly improves the performance of LLMs. However, current CoT methods\nrely on a fixed set of human-annotated exemplars, which are not necessarily the\nmost effective examples for different tasks. This paper proposes a new method,\nActive-Prompt, to adapt LLMs to different tasks with task-specific example\nprompts (annotated with human-designed CoT reasoning). For this purpose, we\npropose a solution to the key problem of determining which questions are the\nmost important and helpful ones to annotate from a pool of task-specific\nqueries. By borrowing ideas from the related problem of uncertainty-based\nactive learning, we introduce several metrics to characterize the uncertainty\nso as to select the most uncertain questions for annotation. Experimental\nresults demonstrate the superiority of our proposed method, achieving\nstate-of-the-art on eight complex reasoning tasks. Further analyses of\ndifferent uncertainty metrics, pool sizes, zero-shot learning, and\naccuracy-uncertainty relationship demonstrate the effectiveness of our method.\nOur code will be available at https://github.com/shizhediao/active-prompt.\n", "published": "23-02-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning", "url": "http://arxiv.org/abs/2303.10512v2", "authors": "Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, Tuo Zhao", "abstract": "  Fine-tuning large pre-trained language models on downstream tasks has become\nan important paradigm in NLP. However, common practice fine-tunes all of the\nparameters in a pre-trained model, which becomes prohibitive when a large\nnumber of downstream tasks are present. Therefore, many fine-tuning methods are\nproposed to learn incremental updates of pre-trained weights in a parameter\nefficient way, e.g., low-rank increments. These methods often evenly distribute\nthe budget of incremental updates across all pre-trained weight matrices, and\noverlook the varying importance of different weight parameters. As a\nconsequence, the fine-tuning performance is suboptimal. To bridge this gap, we\npropose AdaLoRA, which adaptively allocates the parameter budget among weight\nmatrices according to their importance score. In particular, AdaLoRA\nparameterizes the incremental updates in the form of singular value\ndecomposition. Such a novel approach allows us to effectively prune the\nsingular values of unimportant updates, which is essentially to reduce their\nparameter budget but circumvent intensive exact SVD computations. We conduct\nextensive experiments with several pre-trained models on natural language\nprocessing, question answering, and natural language generation to validate the\neffectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable\nimprovement over baselines, especially in the low budget settings. Our code is\npublicly available at https://github.com/QingruZhang/AdaLoRA .\n", "published": "18-03-2023", "year": "2023", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on\n  Dataset and Prompt Collections", "url": "http://arxiv.org/abs/2104.04670v5", "authors": "Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein", "abstract": "  Large pre-trained language models (LMs) such as GPT-3 have acquired a\nsurprising ability to perform zero-shot learning. For example, to classify\nsentiment without any training examples, we can \"prompt\" the LM with the review\nand the label description \"Does the user like this movie?\", and ask whether the\nnext word is \"yes\" or \"no\". However, the next word prediction training\nobjective is still misaligned with the target zero-shot learning objective. To\naddress this weakness, we propose meta-tuning, which directly optimizes the\nzero-shot learning objective by fine-tuning pre-trained language models on a\ncollection of datasets. We focus on classification tasks, and construct the\nmeta-dataset by aggregating 43 existing datasets and annotating 441 label\ndescriptions in a question-answering (QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-sized QA model and the previous SOTA\nzero-shot learning system based on natural language inference. Additionally,\nincreasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,\nand we forecast that even larger models would perform better. Therefore,\nmeasuring zero-shot learning performance on language models out-of-the-box\nmight underestimate their true potential, and community-wide efforts on\naggregating datasets and unifying their formats can help build models that\nanswer prompts better.\n", "published": "10-04-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Agent Instructs Large Language Models to be General Zero-Shot Reasoners", "url": "http://arxiv.org/abs/2310.03710v1", "authors": "Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn Song, Chenguang Wang", "abstract": "  We introduce a method to improve the zero-shot reasoning abilities of large\nlanguage models on general language understanding tasks. Specifically, we build\nan autonomous agent to instruct the reasoning process of large language models.\nWe show this approach further unleashes the zero-shot reasoning abilities of\nlarge language models to more tasks. We study the performance of our method on\na wide set of datasets spanning generation, classification, and reasoning. We\nshow that our method generalizes to most tasks and obtains state-of-the-art\nzero-shot performance on 20 of the 29 datasets that we evaluate. For instance,\nour method boosts the performance of state-of-the-art large language models by\na large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and\nGPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement\nin reasoning is striking, with an average increase of 10.5%. With our method,\nLlama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.\n", "published": "05-10-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Agents: An Open-source Framework for Autonomous Language Agents", "url": "http://arxiv.org/abs/2309.07870v3", "authors": "Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang, Huajun Chen, Peng Cui, Mrinmaya Sachan", "abstract": "  Recent advances on large language models (LLMs) enable researchers and\ndevelopers to build autonomous language agents that can automatically solve\nvarious tasks and interact with environments, humans, and other agents using\nnatural language interfaces. We consider language agents as a promising\ndirection towards artificial general intelligence and release Agents, an\nopen-source library with the goal of opening up these advances to a wider\nnon-specialist audience. Agents is carefully engineered to support important\nfeatures including planning, memory, tool usage, multi-agent communication, and\nfine-grained symbolic control. Agents is user-friendly as it enables\nnon-specialists to build, customize, test, tune, and deploy state-of-the-art\nautonomous language agents without much coding. The library is also\nresearch-friendly as its modularized design makes it easily extensible for\nresearchers. Agents is available at https://github.com/aiwaves-cn/agents.\n", "published": "14-09-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Algorithm Evolution Using Large Language Model", "url": "http://arxiv.org/abs/2311.15249v1", "authors": "Fei Liu, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang", "abstract": "  Optimization can be found in many real-life applications. Designing an\neffective algorithm for a specific optimization problem typically requires a\ntedious amount of effort from human experts with domain knowledge and algorithm\ndesign skills. In this paper, we propose a novel approach called Algorithm\nEvolution using Large Language Model (AEL). It utilizes a large language model\n(LLM) to automatically generate optimization algorithms via an evolutionary\nframework. AEL does algorithm-level evolution without model training. Human\neffort and requirements for domain knowledge can be significantly reduced. We\ntake constructive methods for the salesman traveling problem as a test example,\nwe show that the constructive algorithm obtained by AEL outperforms simple\nhand-crafted and LLM-generated heuristics. Compared with other domain deep\nlearning model-based algorithms, these methods exhibit excellent scalability\nacross different problem sizes. AEL is also very different from previous\nattempts that utilize LLMs as search operators in algorithms.\n", "published": "26-11-2023", "year": "2023", "categories": ["Artificial Intelligence", "Machine Learning"]}, {"title": "Alignment for Honesty", "url": "http://arxiv.org/abs/2312.07000v1", "authors": "Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, Pengfei Liu", "abstract": "  Recent research has made significant strides in applying alignment techniques\nto enhance the helpfulness and harmlessness of large language models (LLMs) in\naccordance with human intentions. In this paper, we argue for the importance of\nalignment for honesty, ensuring that LLMs proactively refuse to answer\nquestions when they lack knowledge, while still not being overly conservative.\nHowever, a pivotal aspect of alignment for honesty involves discerning the\nlimits of an LLM's knowledge, which is far from straightforward. This challenge\ndemands comprehensive solutions in terms of metric development, benchmark\ncreation, and training methodologies. In this paper, we address these\nchallenges by first establishing a precise problem definition and defining\n``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone\nfor developing metrics that effectively measure an LLM's honesty by quantifying\nits progress post-alignment. Furthermore, we introduce a flexible training\nframework which is further instantiated by several efficient fine-tuning\ntechniques that emphasize honesty without sacrificing performance on other\ntasks. Our extensive experiments reveal that these aligned models show a marked\nincrease in honesty, as indicated by our proposed metrics. We open-source a\nwealth of resources to facilitate future research at\nhttps://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned\nmodels, training and evaluation datasets for honesty alignment, concept\nglossary, as well as all relevant source code.\n", "published": "12-12-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "An In-depth Look at Gemini's Language Abilities", "url": "http://arxiv.org/abs/2312.11444v2", "authors": "Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex B\u00e4uerle, \u00c1ngel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig", "abstract": "  The recently released Google Gemini class of models are the first to\ncomprehensively report results that rival the OpenAI GPT series across a wide\nvariety of tasks. In this paper, we do an in-depth exploration of Gemini's\nlanguage abilities, making two contributions. First, we provide a third-party,\nobjective comparison of the abilities of the OpenAI GPT and Google Gemini\nmodels with reproducible code and fully transparent results. Second, we take a\ncloser look at the results, identifying areas where one of the two model\nclasses excels. We perform this analysis over 10 datasets testing a variety of\nlanguage abilities, including reasoning, answering knowledge-based questions,\nsolving math problems, translating between languages, generating code, and\nacting as instruction-following agents. From this analysis, we find that Gemini\nPro achieves accuracy that is close but slightly inferior to the corresponding\nGPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations\nfor some of this under-performance, including failures in mathematical\nreasoning with many digits, sensitivity to multiple-choice answer ordering,\naggressive content filtering, and others. We also identify areas where Gemini\ndemonstrates comparably high performance, including generation into non-English\nlanguages, and handling longer and more complex reasoning chains. Code and data\nfor reproduction can be found at https://github.com/neulab/gemini-benchmark\n", "published": "18-12-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "An LLM Compiler for Parallel Function Calling", "url": "http://arxiv.org/abs/2312.04511v1", "authors": "Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W. Mahoney, Kurt Keutzer, Amir Gholami", "abstract": "  Large Language Models (LLMs) have shown remarkable results on various complex\nreasoning benchmarks. The reasoning capabilities of LLMs enable them to execute\nfunction calls, using user-provided functions to overcome their inherent\nlimitations, such as knowledge cutoffs, poor arithmetic skills, or lack of\naccess to private data. This development has expanded LLMs' scope to include\nmulti-function calling, where LLMs are equipped with a variety of functions and\nselect the proper functions based on the context. Multi-function calling\nabilities of LLMs have catalyzed LLM-based software development, allowing them\nto tackle more complex problems. However, current methods for multi-function\ncalling often require sequential reasoning and acting for each function which\ncan result in high latency, cost, and sometimes inaccurate behavior. To address\nthis, we introduce LLMCompiler, which executes functions in parallel to\nefficiently orchestrate multi-function calling. Drawing from the principles of\nclassical compilers, LLMCompiler streamlines parallel function calling with\nthree components: (i) an LLM Planner, formulating execution strategies and\ndependencies; (ii) a Task Fetching Unit, dispatching function calling tasks;\nand (iii) an Executor, executing these tasks in parallel. LLMCompiler\nautomatically computes an optimized orchestration for the function calls and\ncan be used with open-source models such as LLaMA-2. We have benchmarked\nLLMCompiler on a range of tasks including cases with non-trivial\ninter-dependency between function calls, as well as cases that require dynamic\nreplanning based on intermediate results. We observe consistent latency speedup\nof up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to\n~9% as compared to ReAct. Additionally, LLMCompiler achieves up to 1.35x\nlatency gain over OpenAI's recent parallel function calling, while achieving\nsimilar accuracy.\n", "published": "07-12-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "AppAgent: Multimodal Agents as Smartphone Users", "url": "http://arxiv.org/abs/2312.13771v2", "authors": "Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, Gang Yu", "abstract": "  Recent advancements in large language models (LLMs) have led to the creation\nof intelligent agents capable of performing complex tasks. This paper\nintroduces a novel LLM-based multimodal agent framework designed to operate\nsmartphone applications. Our framework enables the agent to operate smartphone\napplications through a simplified action space, mimicking human-like\ninteractions such as tapping and swiping. This novel approach bypasses the need\nfor system back-end access, thereby broadening its applicability across diverse\napps. Central to our agent's functionality is its innovative learning method.\nThe agent learns to navigate and use new apps either through autonomous\nexploration or by observing human demonstrations. This process generates a\nknowledge base that the agent refers to for executing complex tasks across\ndifferent applications. To demonstrate the practicality of our agent, we\nconducted extensive testing over 50 tasks in 10 different applications,\nincluding social media, email, maps, shopping, and sophisticated image editing\ntools. The results affirm our agent's proficiency in handling a diverse array\nof high-level tasks.\n", "published": "21-12-2023", "year": "2023", "categories": []}, {"title": "Are We Testing or Being Tested? Exploring the Practical Applications of\n  Large Language Models in Software Testing", "url": "http://arxiv.org/abs/2312.04860v1", "authors": "Robson Santos, Italo Santos, Cleyton Magalhaes, Ronnie de Souza Santos", "abstract": "  A Large Language Model (LLM) represents a cutting-edge artificial\nintelligence model that generates coherent content, including grammatically\nprecise sentences, human-like paragraphs, and syntactically accurate code\nsnippets. LLMs can play a pivotal role in software development, including\nsoftware testing. LLMs go beyond traditional roles such as requirement analysis\nand documentation and can support test case generation, making them valuable\ntools that significantly enhance testing practices within the field. Hence, we\nexplore the practical application of LLMs in software testing within an\nindustrial setting, focusing on their current use by professional testers. In\nthis context, rather than relying on existing data, we conducted a\ncross-sectional survey and collected data within real working contexts,\nspecifically, engaging with practitioners in industrial settings. We applied\nquantitative and qualitative techniques to analyze and synthesize our collected\ndata. Our findings demonstrate that LLMs effectively enhance testing documents\nand significantly assist testing professionals in programming tasks like\ndebugging and test case automation. LLMs can support individuals engaged in\nmanual testing who need to code. However, it is crucial to emphasize that, at\nthis early stage, software testing professionals should use LLMs with caution\nwhile well-defined methods and guidelines are being built for the secure\nadoption of these tools.\n", "published": "08-12-2023", "year": "2023", "categories": []}, {"title": "Ask Me Anything: A simple strategy for prompting language models", "url": "http://arxiv.org/abs/2210.02441v3", "authors": "Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, Christopher R\u00e9", "abstract": "  Large language models (LLMs) transfer well to new tasks out-of-the-box simply\ngiven a natural language prompt that demonstrates how to perform the task and\nno additional training. Prompting is a brittle process wherein small\nmodifications to the prompt can cause large variations in the model\npredictions, and therefore significant effort is dedicated towards designing a\npainstakingly \"perfect prompt\" for a task. To mitigate the high degree of\neffort involved in prompt-design, we instead ask whether producing multiple\neffective, yet imperfect, prompts and aggregating them can lead to a high\nquality prompting strategy. Our observations motivate our proposed prompting\nmethod, ASK ME ANYTHING (AMA). We first develop an understanding of the\neffective prompt formats, finding that question-answering (QA) prompts, which\nencourage open-ended generation (\"Who went to the park?\") tend to outperform\nthose that restrict the model outputs (\"John went to the park. Output True or\nFalse.\"). Our approach recursively uses the LLM itself to transform task inputs\nto the effective QA format. We apply the collected prompts to obtain several\nnoisy votes for the input's true label. We find that the prompts can have very\ndifferent accuracies and complex dependencies and thus propose to use weak\nsupervision, a procedure for combining the noisy predictions, to produce the\nfinal predictions for the inputs. We evaluate AMA across open-source model\nfamilies (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B\nparameters), demonstrating an average performance lift of 10.2% over the\nfew-shot baseline. This simple strategy enables the open-source GPT-J-6B model\nto match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular\nbenchmarks. Averaged across these tasks, the GPT-J-6B model outperforms\nfew-shot GPT3-175B. We release our code here:\nhttps://github.com/HazyResearch/ama_prompting\n", "published": "05-10-2022", "year": "2022", "categories": ["Computation and Language"]}, {"title": "Attention Is All You Need", "url": "http://arxiv.org/abs/1706.03762v7", "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin", "abstract": "  The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.\n", "published": "12-06-2017", "year": "2017", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "AutoAgents: A Framework for Automatic Agent Generation", "url": "http://arxiv.org/abs/2309.17288v2", "authors": "Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, B\u00f6rje F. Karlsson, Jie Fu, Yemin Shi", "abstract": "  Large language models (LLMs) have enabled remarkable advances in automated\ntask-solving with multi-agent systems. However, most existing LLM-based\nmulti-agent approaches rely on predefined agents to handle simple tasks,\nlimiting the adaptability of multi-agent collaboration to different scenarios.\nTherefore, we introduce AutoAgents, an innovative framework that adaptively\ngenerates and coordinates multiple specialized agents to build an AI team\naccording to different tasks. Specifically, AutoAgents couples the relationship\nbetween tasks and roles by dynamically generating multiple required agents\nbased on task content and planning solutions for the current task based on the\ngenerated expert agents. Multiple specialized agents collaborate with each\nother to efficiently accomplish tasks. Concurrently, an observer role is\nincorporated into the framework to reflect on the designated plans and agents'\nresponses and improve upon them. Our experiments on various benchmarks\ndemonstrate that AutoAgents generates more coherent and accurate solutions than\nthe existing multi-agent methods. This underscores the significance of\nassigning different roles to different tasks and of team cooperation, offering\nnew perspectives for tackling complex tasks. The repository of this project is\navailable at https://github.com/Link-AGI/AutoAgents.\n", "published": "29-09-2023", "year": "2023", "categories": ["Artificial Intelligence"]}, {"title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically\n  Generated Prompts", "url": "http://arxiv.org/abs/2010.15980v2", "authors": "Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, Sameer Singh", "abstract": "  The remarkable success of pretrained language models has motivated the study\nof what kinds of knowledge these models learn during pretraining. Reformulating\ntasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach\nfor gauging such knowledge, however, its usage is limited by the manual effort\nand guesswork required to write suitable prompts. To address this, we develop\nAutoPrompt, an automated method to create prompts for a diverse set of tasks,\nbased on a gradient-guided search. Using AutoPrompt, we show that masked\nlanguage models (MLMs) have an inherent capability to perform sentiment\nanalysis and natural language inference without additional parameters or\nfinetuning, sometimes achieving performance on par with recent state-of-the-art\nsupervised models. We also show that our prompts elicit more accurate factual\nknowledge from MLMs than the manually created prompts on the LAMA benchmark,\nand that MLMs can be used as relation extractors more effectively than\nsupervised relation extraction models. These results demonstrate that\nautomatically generated prompts are a viable parameter-free alternative to\nexisting probing methods, and as pretrained LMs become more sophisticated and\ncapable, potentially a replacement for finetuning.\n", "published": "29-10-2020", "year": "2020", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Automatic Chain of Thought Prompting in Large Language Models", "url": "http://arxiv.org/abs/2210.03493v1", "authors": "Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola", "abstract": "  Large language models (LLMs) can perform complex reasoning by generating\nintermediate reasoning steps. Providing these steps for prompting\ndemonstrations is called chain-of-thought (CoT) prompting. CoT prompting has\ntwo major paradigms. One leverages a simple prompt like \"Let's think step by\nstep\" to facilitate step-by-step thinking before answering a question. The\nother uses a few manual demonstrations one by one, each composed of a question\nand a reasoning chain that leads to an answer. The superior performance of the\nsecond paradigm hinges on the hand-crafting of task-specific demonstrations one\nby one. We show that such manual efforts may be eliminated by leveraging LLMs\nwith the \"Let's think step by step\" prompt to generate reasoning chains for\ndemonstrations one by one, i.e., let's think not just step by step, but also\none by one. However, these generated chains often come with mistakes. To\nmitigate the effect of such mistakes, we find that diversity matters for\nautomatically constructing demonstrations. We propose an automatic CoT\nprompting method: Auto-CoT. It samples questions with diversity and generates\nreasoning chains to construct demonstrations. On ten public benchmark reasoning\ntasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of\nthe CoT paradigm that requires manual designs of demonstrations. Code is\navailable at https://github.com/amazon-research/auto-cot\n", "published": "07-10-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search", "url": "http://arxiv.org/abs/2305.03495v2", "authors": "Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng", "abstract": "  Large Language Models (LLMs) have shown impressive performance as general\npurpose agents, but their abilities remain highly dependent on prompts which\nare hand written with onerous trial-and-error effort. We propose a simple and\nnonparametric solution to this problem, Automatic Prompt Optimization (APO),\nwhich is inspired by numerical gradient descent to automatically improve\nprompts, assuming access to training data and an LLM API. The algorithm uses\nminibatches of data to form natural language \"gradients\" that criticize the\ncurrent prompt. The gradients are then \"propagated\" into the prompt by editing\nthe prompt in the opposite semantic direction of the gradient. These gradient\ndescent steps are guided by a beam search and bandit selection procedure which\nsignificantly improves algorithmic efficiency. Preliminary results across three\nbenchmark NLP tasks and the novel problem of LLM jailbreak detection suggest\nthat Automatic Prompt Optimization can outperform prior prompt editing\ntechniques and improve an initial prompt's performance by up to 31%, by using\ndata to rewrite vague task descriptions into more precise annotation\ninstructions.\n", "published": "04-05-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Automatically Identifying Words That Can Serve as Labels for Few-Shot\n  Text Classification", "url": "http://arxiv.org/abs/2010.13641v1", "authors": "Timo Schick, Helmut Schmid, Hinrich Sch\u00fctze", "abstract": "  A recent approach for few-shot text classification is to convert textual\ninputs to cloze questions that contain some form of task description, process\nthem with a pretrained language model and map the predicted words to labels.\nManually defining this mapping between words and labels requires both domain\nexpertise and an understanding of the language model's abilities. To mitigate\nthis issue, we devise an approach that automatically finds such a mapping given\nsmall amounts of training data. For a number of tasks, the mapping found by our\napproach performs almost as well as hand-crafted label-to-word mappings.\n", "published": "26-10-2020", "year": "2020", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning", "url": "http://arxiv.org/abs/2109.04144v1", "authors": "Prasetya Ajie Utama, Nafise Sadat Moosavi, Victor Sanh, Iryna Gurevych", "abstract": "  Recent prompt-based approaches allow pretrained language models to achieve\nstrong performances on few-shot finetuning by reformulating downstream tasks as\na language modeling problem. In this work, we demonstrate that, despite its\nadvantages on low data regimes, finetuned prompt-based models for sentence pair\nclassification tasks still suffer from a common pitfall of adopting inference\nheuristics based on lexical overlap, e.g., models incorrectly assuming a\nsentence pair is of the same meaning because they consist of the same set of\nwords. Interestingly, we find that this particular inference heuristic is\nsignificantly less present in the zero-shot evaluation of the prompt-based\nmodel, indicating how finetuning can be destructive to useful knowledge learned\nduring the pretraining. We then show that adding a regularization that\npreserves pretraining weights is effective in mitigating this destructive\ntendency of few-shot finetuning. Our evaluation on three datasets demonstrates\npromising improvements on the three corresponding challenge datasets used to\ndiagnose the inference heuristics.\n", "published": "09-09-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\n  Generation, Translation, and Comprehension", "url": "http://arxiv.org/abs/1910.13461v1", "authors": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer", "abstract": "  We present BART, a denoising autoencoder for pretraining sequence-to-sequence\nmodels. BART is trained by (1) corrupting text with an arbitrary noising\nfunction, and (2) learning a model to reconstruct the original text. It uses a\nstandard Tranformer-based neural machine translation architecture which,\ndespite its simplicity, can be seen as generalizing BERT (due to the\nbidirectional encoder), GPT (with the left-to-right decoder), and many other\nmore recent pretraining schemes. We evaluate a number of noising approaches,\nfinding the best performance by both randomly shuffling the order of the\noriginal sentences and using a novel in-filling scheme, where spans of text are\nreplaced with a single mask token. BART is particularly effective when fine\ntuned for text generation but also works well for comprehension tasks. It\nmatches the performance of RoBERTa with comparable training resources on GLUE\nand SQuAD, achieves new state-of-the-art results on a range of abstractive\ndialogue, question answering, and summarization tasks, with gains of up to 6\nROUGE. BART also provides a 1.1 BLEU increase over a back-translation system\nfor machine translation, with only target language pretraining. We also report\nablation experiments that replicate other pretraining schemes within the BART\nframework, to better measure which factors most influence end-task performance.\n", "published": "29-10-2019", "year": "2019", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding", "url": "http://arxiv.org/abs/1810.04805v2", "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova", "abstract": "  We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n", "published": "11-10-2018", "year": "2018", "categories": ["Computation and Language"]}, {"title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "url": "http://arxiv.org/abs/2211.05100v4", "authors": "BigScience Workshop,  :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren\u00e7on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz\u00e1lez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J\u00f6rg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu\u00f1oz, Maraim Masoud, Mar\u00eda Grandury, Mario \u0160a\u0161ko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L\u00f3pez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Ta\u015far, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Fran\u00e7ois Lavall\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur\u00e9lie N\u00e9v\u00e9ol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zden\u011bk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu\u00f1oz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u00f3n Peri\u00f1\u00e1n, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Maria A Castillo, Marianna Nezhurina, Mario S\u00e4nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th\u00e9o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, Thomas Wolf", "abstract": "  Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.\n", "published": "09-11-2022", "year": "2022", "categories": ["Computation and Language"]}, {"title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on\n  Self-Chat Data", "url": "http://arxiv.org/abs/2304.01196v4", "authors": "Canwen Xu, Daya Guo, Nan Duan, Julian McAuley", "abstract": "  Chat models, such as ChatGPT, have shown impressive capabilities and have\nbeen rapidly adopted across numerous domains. However, these models are only\naccessible through a restricted API, creating barriers for new research and\nprogress in the field. We propose a pipeline that can automatically generate a\nhigh-quality multi-turn chat corpus by leveraging ChatGPT to engage in a\nconversation with itself. Subsequently, we employ parameter-efficient tuning to\nenhance LLaMA, an open-source large language model. The resulting model, named\nBaize, demonstrates good performance in multi-turn dialogues with guardrails\nthat minimize potential risks. Furthermore, we propose a new technique called\nSelf-Distill with Feedback, to further improve the performance of the Baize\nmodels with feedback from ChatGPT. The Baize models and data are released for\nresearch purposes only at https://github.com/project-baize/baize-chatbot. An\nonline demo is also available at\nhttps://huggingface.co/spaces/project-baize/chat-with-baize.\n", "published": "03-04-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Beyond Efficiency: A Systematic Survey of Resource-Efficient Large\n  Language Models", "url": "http://arxiv.org/abs/2401.00625v2", "authors": "Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, Carl Yang, Yue Cheng, Liang Zhao", "abstract": "  The burgeoning field of Large Language Models (LLMs), exemplified by\nsophisticated models like OpenAI's ChatGPT, represents a significant\nadvancement in artificial intelligence. These models, however, bring forth\nsubstantial challenges in the high consumption of computational, memory,\nenergy, and financial resources, especially in environments with limited\nresource capabilities. This survey aims to systematically address these\nchallenges by reviewing a broad spectrum of techniques designed to enhance the\nresource efficiency of LLMs. We categorize methods based on their optimization\nfocus: computational, memory, energy, financial, and network resources and\ntheir applicability across various stages of an LLM's lifecycle, including\narchitecture design, pretraining, finetuning, and system design. Additionally,\nthe survey introduces a nuanced categorization of resource efficiency\ntechniques by their specific resource types, which uncovers the intricate\nrelationships and mappings between various resources and corresponding\noptimization techniques. A standardized set of evaluation metrics and datasets\nis also presented to facilitate consistent and fair comparisons across\ndifferent models and techniques. By offering a comprehensive overview of the\ncurrent sota and identifying open research avenues, this survey serves as a\nfoundational reference for researchers and practitioners, aiding them in\ndeveloping more sustainable and efficient LLMs in a rapidly evolving landscape.\n", "published": "01-01-2024", "year": "2024", "categories": ["Machine Learning"]}, {"title": "Beyond Memorization: Violating Privacy Via Inference with Large Language\n  Models", "url": "http://arxiv.org/abs/2310.07298v1", "authors": "Robin Staab, Mark Vero, Mislav Balunovi\u0107, Martin Vechev", "abstract": "  Current privacy research on large language models (LLMs) primarily focuses on\nthe issue of extracting memorized training data. At the same time, models'\ninference capabilities have increased drastically. This raises the key question\nof whether current LLMs could violate individuals' privacy by inferring\npersonal attributes from text given at inference time. In this work, we present\nthe first comprehensive study on the capabilities of pretrained LLMs to infer\npersonal attributes from text. We construct a dataset consisting of real Reddit\nprofiles, and show that current LLMs can infer a wide range of personal\nattributes (e.g., location, income, sex), achieving up to $85\\%$ top-1 and\n$95.8\\%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time\n($240\\times$) required by humans. As people increasingly interact with\nLLM-powered chatbots across all aspects of life, we also explore the emerging\nthreat of privacy-invasive chatbots trying to extract personal information\nthrough seemingly benign questions. Finally, we show that common mitigations,\ni.e., text anonymization and model alignment, are currently ineffective at\nprotecting user privacy against LLM inference. Our findings highlight that\ncurrent LLMs can infer personal data at a previously unattainable scale. In the\nabsence of working defenses, we advocate for a broader discussion around LLM\nprivacy implications beyond memorization, striving for a wider privacy\nprotection.\n", "published": "11-10-2023", "year": "2023", "categories": ["Artificial Intelligence", "Machine Learning"]}, {"title": "BlenderBot 3: a deployed conversational agent that continually learns to\n  responsibly engage", "url": "http://arxiv.org/abs/2208.03188v3", "authors": "Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, Jason Weston", "abstract": "  We present BlenderBot 3, a 175B parameter dialogue model capable of\nopen-domain conversation with access to the internet and a long-term memory,\nand having been trained on a large number of user defined tasks. We release\nboth the model weights and code, and have also deployed the model on a public\nweb page to interact with organic users. This technical report describes how\nthe model was built (architecture, model and training scheme), and details of\nits deployment, including safety mechanisms. Human evaluations show its\nsuperiority to existing open-domain dialogue agents, including its predecessors\n(Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for\ncontinual learning using the data collected from deployment, which will also be\npublicly released. The goal of this research program is thus to enable the\ncommunity to study ever-improving responsible agents that learn through\ninteraction.\n", "published": "05-08-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "BloombergGPT: A Large Language Model for Finance", "url": "http://arxiv.org/abs/2303.17564v3", "authors": "Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann", "abstract": "  The use of NLP in the realm of financial technology is broad and complex,\nwith applications ranging from sentiment analysis and named entity recognition\nto question answering. Large Language Models (LLMs) have been shown to be\neffective on a variety of tasks; however, no LLM specialized for the financial\ndomain has been reported in literature. In this work, we present BloombergGPT,\na 50 billion parameter language model that is trained on a wide range of\nfinancial data. We construct a 363 billion token dataset based on Bloomberg's\nextensive data sources, perhaps the largest domain-specific dataset yet,\naugmented with 345 billion tokens from general purpose datasets. We validate\nBloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite\nof internal benchmarks that most accurately reflect our intended usage. Our\nmixed dataset training leads to a model that outperforms existing models on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. Additionally, we explain our modeling choices, training\nprocess, and evaluation methodology. We release Training Chronicles (Appendix\nC) detailing our experience in training BloombergGPT.\n", "published": "30-03-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented\n  Dialog Systems", "url": "http://arxiv.org/abs/2109.04645v4", "authors": "Fei Mi, Yitong Li, Yasheng Wang, Xin Jiang, Qun Liu", "abstract": "  As labeling cost for different modules in task-oriented dialog (ToD) systems\nis high, a major challenge in practice is to learn different tasks with the\nleast amount of labeled data. Recently, prompting methods over pre-trained\nlanguage models (PLMs) have shown promising results for few-shot learning in\nToD. To better utilize the power of PLMs, this paper proposes Comprehensive\nInstruction (CINS) that exploits PLMs with extra task-specific instructions. We\ndesign a schema (definition, constraint, prompt) of instructions and their\ncustomized realizations for three important downstream tasks in ToD, i.e.\nintent classification, dialog state tracking, and natural language generation.\nA sequence-to-sequence model (T5) is adopted to solve these three tasks in a\nunified framework. Extensive experiments are conducted on these ToD tasks in\nrealistic few-shot learning scenarios with small validation data. Empirical\nresults demonstrate that the proposed CINS approach consistently improves\ntechniques that finetune PLMs with raw input or short prompts.\n", "published": "10-09-2021", "year": "2021", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "url": "http://arxiv.org/abs/2102.09690v2", "authors": "Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh", "abstract": "  GPT-3 can perform numerous tasks when provided a natural language prompt that\ncontains a few training examples. We show that this type of few-shot learning\ncan be unstable: the choice of prompt format, training examples, and even the\norder of the training examples can cause accuracy to vary from near chance to\nnear state-of-the-art. We demonstrate that this instability arises from the\nbias of language models towards predicting certain answers, e.g., those that\nare placed near the end of the prompt or are common in the pre-training data.\nTo mitigate this, we first estimate the model's bias towards each answer by\nasking for its prediction when given the training prompt and a content-free\ntest input such as \"N/A\". We then fit calibration parameters that cause the\nprediction for this input to be uniform across answers. On a diverse set of\ntasks, this contextual calibration procedure substantially improves GPT-3 and\nGPT-2's average accuracy (up to 30.0% absolute) and reduces variance across\ndifferent choices of the prompt.\n", "published": "19-02-2021", "year": "2021", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4\n  on mock CFA Exams", "url": "http://arxiv.org/abs/2310.08678v1", "authors": "Ethan Callanan, Amarachi Mbakwe, Antony Papadimitriou, Yulong Pei, Mathieu Sibue, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu, Sameena Shah", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable performance on a\nwide range of Natural Language Processing (NLP) tasks, often matching or even\nbeating state-of-the-art task-specific models. This study aims at assessing the\nfinancial reasoning capabilities of LLMs. We leverage mock exam questions of\nthe Chartered Financial Analyst (CFA) Program to conduct a comprehensive\nevaluation of ChatGPT and GPT-4 in financial analysis, considering Zero-Shot\n(ZS), Chain-of-Thought (CoT), and Few-Shot (FS) scenarios. We present an\nin-depth analysis of the models' performance and limitations, and estimate\nwhether they would have a chance at passing the CFA exams. Finally, we outline\ninsights into potential strategies and improvements to enhance the\napplicability of LLMs in finance. In this perspective, we hope this work paves\nthe way for future studies to continue enhancing LLMs for financial reasoning\nthrough rigorous evaluation.\n", "published": "12-10-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Can LLMs Follow Simple Rules?", "url": "http://arxiv.org/abs/2311.04235v1", "authors": "Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, David Wagner", "abstract": "  As Large Language Models (LLMs) are deployed with increasing real-world\nresponsibilities, it is important to be able to specify and constrain the\nbehavior of these systems in a reliable manner. Model developers may wish to\nset explicit rules for the model, such as \"do not generate abusive content\",\nbut these may be circumvented by jailbreaking techniques. Evaluating how well\nLLMs follow developer-provided rules in the face of adversarial inputs\ntypically requires manual review, which slows down monitoring and methods\ndevelopment. To address this issue, we propose Rule-following Language\nEvaluation Scenarios (RuLES), a programmatic framework for measuring\nrule-following ability in LLMs. RuLES consists of 15 simple text scenarios in\nwhich the model is instructed to obey a set of rules in natural language while\ninteracting with the human user. Each scenario has a concise evaluation program\nto determine whether the model has broken any rules in a conversation. Through\nmanual exploration of model behavior in our scenarios, we identify 6 categories\nof attack strategies and collect two suites of test cases: one consisting of\nunique conversations from manual testing and one that systematically implements\nstrategies from the 6 categories. Across various popular proprietary and open\nmodels such as GPT-4 and Llama 2, we find that all models are susceptible to a\nwide variety of adversarial hand-crafted user inputs, though GPT-4 is the\nbest-performing model. Additionally, we evaluate open models under\ngradient-based attacks and find significant vulnerabilities. We propose RuLES\nas a challenging new setting for research into exploring and defending against\nboth manual and automatic attacks on LLMs.\n", "published": "06-11-2023", "year": "2023", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "Can Language Models be Biomedical Knowledge Bases?", "url": "http://arxiv.org/abs/2109.07154v1", "authors": "Mujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sungdong Kim, Jaewoo Kang", "abstract": "  Pre-trained language models (LMs) have become ubiquitous in solving various\nnatural language processing (NLP) tasks. There has been increasing interest in\nwhat knowledge these LMs contain and how we can extract that knowledge,\ntreating LMs as knowledge bases (KBs). While there has been much work on\nprobing LMs in the general domain, there has been little attention to whether\nthese powerful LMs can be used as domain-specific KBs. To this end, we create\nthe BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge\ntriples for probing biomedical LMs. We find that biomedical LMs with recently\nproposed probing methods can achieve up to 18.51% Acc@5 on retrieving\nbiomedical knowledge. Although this seems promising given the task difficulty,\nour detailed analyses reveal that most predictions are highly correlated with\nprompt templates without any subjects, hence producing similar results on each\nrelation and hindering their capabilities to be used as domain-specific KBs. We\nhope that BioLAMA can serve as a challenging benchmark for biomedical factual\nprobing.\n", "published": "15-09-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "Can large language models provide useful feedback on research papers? A\n  large-scale empirical analysis", "url": "http://arxiv.org/abs/2310.01783v1", "authors": "Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Smith, Yian Yin, Daniel McFarland, James Zou", "abstract": "  Expert feedback lays the foundation of rigorous research. However, the rapid\ngrowth of scholarly production and intricate knowledge specialization challenge\nthe conventional scientific feedback mechanisms. High-quality peer reviews are\nincreasingly difficult to obtain. Researchers who are more junior or from\nunder-resourced settings have especially hard times getting timely feedback.\nWith the breakthrough of large language models (LLM) such as GPT-4, there is\ngrowing interest in using LLMs to generate scientific feedback on research\nmanuscripts. However, the utility of LLM-generated feedback has not been\nsystematically studied. To address this gap, we created an automated pipeline\nusing GPT-4 to provide comments on the full PDFs of scientific papers. We\nevaluated the quality of GPT-4's feedback through two large-scale studies. We\nfirst quantitatively compared GPT-4's generated feedback with human peer\nreviewer feedback in 15 Nature family journals (3,096 papers in total) and the\nICLR machine learning conference (1,709 papers). The overlap in the points\nraised by GPT-4 and by human reviewers (average overlap 30.85% for Nature\njournals, 39.23% for ICLR) is comparable to the overlap between two human\nreviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The\noverlap between GPT-4 and human reviewers is larger for the weaker papers. We\nthen conducted a prospective user study with 308 researchers from 110 US\ninstitutions in the field of AI and computational biology to understand how\nresearchers perceive feedback generated by our GPT-4 system on their own\npapers. Overall, more than half (57.4%) of the users found GPT-4 generated\nfeedback helpful/very helpful and 82.4% found it more beneficial than feedback\nfrom at least some human reviewers. While our findings show that LLM-generated\nfeedback can help researchers, we also identify several limitations.\n", "published": "03-10-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "Catwalk: A Unified Language Model Evaluation Framework for Many Datasets", "url": "http://arxiv.org/abs/2312.10253v1", "authors": "Dirk Groeneveld, Anas Awadalla, Iz Beltagy, Akshita Bhagia, Ian Magnusson, Hao Peng, Oyvind Tafjord, Pete Walsh, Kyle Richardson, Jesse Dodge", "abstract": "  The success of large language models has shifted the evaluation paradigms in\nnatural language processing (NLP). The community's interest has drifted towards\ncomparing NLP models across many tasks, domains, and datasets, often at an\nextreme scale. This imposes new engineering challenges: efforts in constructing\ndatasets and models have been fragmented, and their formats and interfaces are\nincompatible. As a result, it often takes extensive (re)implementation efforts\nto make fair and controlled comparisons at scale.\n  Catwalk aims to address these issues. Catwalk provides a unified interface to\na broad range of existing NLP datasets and models, ranging from both canonical\nsupervised training and fine-tuning, to more modern paradigms like in-context\nlearning. Its carefully-designed abstractions allow for easy extensions to many\nothers. Catwalk substantially lowers the barriers to conducting controlled\nexperiments at scale. For example, we finetuned and evaluated over 64 models on\nover 86 datasets with a single command, without writing any code. Maintained by\nthe AllenNLP team at the Allen Institute for Artificial Intelligence (AI2),\nCatwalk is an ongoing open-source effort: https://github.com/allenai/catwalk.\n", "published": "15-12-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Chain-Of-Thought Prompting Under Streaming Batch: A Case Study", "url": "http://arxiv.org/abs/2306.00550v1", "authors": "Yuxin Tang", "abstract": "  Recently, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities. Chain-of-Thought (CoT) has been proposed as a way of assisting\nLLMs in performing complex reasoning. However, developing effective prompts can\nbe a challenging and labor-intensive task. Many studies come out of some way to\nautomatically construct CoT from test data. Most of them assume that all test\ndata is visible before testing and only select a small subset to generate\nrationales, which is an unrealistic assumption. In this paper, we present a\ncase study on how to construct and optimize chain-of-thought prompting using\nbatch data in streaming settings.\n", "published": "01-06-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table\n  Understanding", "url": "http://arxiv.org/abs/2401.04398v1", "authors": "Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, Tomas Pfister", "abstract": "  Table-based reasoning with large language models (LLMs) is a promising\ndirection to tackle many table understanding tasks, such as table-based\nquestion answering and fact verification. Compared with generic reasoning,\ntable-based reasoning requires the extraction of underlying semantics from both\nfree-form questions and semi-structured tabular data. Chain-of-Thought and its\nsimilar approaches incorporate the reasoning chain in the form of textual\ncontext, but it is still an open question how to effectively leverage tabular\ndata in the reasoning chain. We propose the Chain-of-Table framework, where\ntabular data is explicitly used in the reasoning chain as a proxy for\nintermediate thoughts. Specifically, we guide LLMs using in-context learning to\niteratively generate operations and update the table to represent a tabular\nreasoning chain. LLMs can therefore dynamically plan the next operation based\non the results of the previous ones. This continuous evolution of the table\nforms a chain, showing the reasoning process for a given tabular problem. The\nchain carries structured information of the intermediate results, enabling more\naccurate and reliable predictions. Chain-of-Table achieves new state-of-the-art\nperformance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM\nchoices.\n", "published": "09-01-2024", "year": "2024", "categories": ["Computation and Language"]}, {"title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "url": "http://arxiv.org/abs/2201.11903v6", "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou", "abstract": "  We explore how generating a chain of thought -- a series of intermediate\nreasoning steps -- significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.\n", "published": "28-01-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Chain-of-Verification Reduces Hallucination in Large Language Models", "url": "http://arxiv.org/abs/2309.11495v2", "authors": "Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston", "abstract": "  Generation of plausible yet incorrect factual information, termed\nhallucination, is an unsolved issue in large language models. We study the\nability of language models to deliberate on the responses they give in order to\ncorrect their mistakes. We develop the Chain-of-Verification (CoVe) method\nwhereby the model first (i) drafts an initial response; then (ii) plans\nverification questions to fact-check its draft; (iii) answers those questions\nindependently so the answers are not biased by other responses; and (iv)\ngenerates its final verified response. In experiments, we show CoVe decreases\nhallucinations across a variety of tasks, from list-based questions from\nWikidata, closed book MultiSpanQA and longform text generation.\n", "published": "20-09-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil\n  Cognitive Depth in LLMs", "url": "http://arxiv.org/abs/2312.17080v1", "authors": "Zhongshen Zeng, Pengguang Chen, Haiyun Jiang, Jiaya Jia", "abstract": "  In this work, we introduce a novel evaluation paradigm for Large Language\nModels, one that challenges them to engage in meta-reasoning. This approach\naddresses critical shortcomings in existing math problem-solving benchmarks,\ntraditionally used to evaluate the cognitive capabilities of agents. Our\nparadigm shifts the focus from result-oriented assessments, which often\noverlook the reasoning process, to a more holistic evaluation that effectively\ndifferentiates the cognitive capabilities among models. For example, in our\nbenchmark, GPT-4 demonstrates a performance ten times more accurate than\nGPT3-5. The significance of this new paradigm lies in its ability to reveal\npotential cognitive deficiencies in LLMs that current benchmarks, such as\nGSM8K, fail to uncover due to their saturation and lack of effective\ndifferentiation among varying reasoning abilities. Our comprehensive analysis\nincludes several state-of-the-art math models from both open-source and\nclosed-source communities, uncovering fundamental deficiencies in their\ntraining and evaluation approaches. This paper not only advocates for a\nparadigm shift in the assessment of LLMs but also contributes to the ongoing\ndiscourse on the trajectory towards Artificial General Intelligence (AGI). By\npromoting the adoption of meta-reasoning evaluation methods similar to ours, we\naim to facilitate a more accurate assessment of the true cognitive abilities of\nLLMs.\n", "published": "28-12-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models\n  Catching up?", "url": "http://arxiv.org/abs/2311.16989v4", "authors": "Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut, Ruochen Zhao, Caiming Xiong, Shafiq Joty", "abstract": "  Upon its release in late 2022, ChatGPT has brought a seismic shift in the\nentire landscape of AI, both in research and commerce. Through\ninstruction-tuning a large language model (LLM) with supervised fine-tuning and\nreinforcement learning from human feedback, it showed that a model could answer\nhuman questions and follow instructions on a broad panel of tasks. Following\nthis success, interests in LLMs have intensified, with new LLMs flourishing at\nfrequent interval across academia and industry, including many start-ups\nfocused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's\nClaude) generally outperform their open-source counterparts, the progress on\nthe latter has been rapid with claims of achieving parity or even better on\ncertain tasks. This has crucial implications not only on research but also on\nbusiness. In this work, on the first anniversary of ChatGPT, we provide an\nexhaustive overview of this success, surveying all tasks where an open-source\nLLM has claimed to be on par or better than ChatGPT.\n", "published": "28-11-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models\n  Catching up?", "url": "http://arxiv.org/abs/2311.16989v4", "authors": "Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut, Ruochen Zhao, Caiming Xiong, Shafiq Joty", "abstract": "  Upon its release in late 2022, ChatGPT has brought a seismic shift in the\nentire landscape of AI, both in research and commerce. Through\ninstruction-tuning a large language model (LLM) with supervised fine-tuning and\nreinforcement learning from human feedback, it showed that a model could answer\nhuman questions and follow instructions on a broad panel of tasks. Following\nthis success, interests in LLMs have intensified, with new LLMs flourishing at\nfrequent interval across academia and industry, including many start-ups\nfocused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's\nClaude) generally outperform their open-source counterparts, the progress on\nthe latter has been rapid with claims of achieving parity or even better on\ncertain tasks. This has crucial implications not only on research but also on\nbusiness. In this work, on the first anniversary of ChatGPT, we provide an\nexhaustive overview of this success, surveying all tasks where an open-source\nLLM has claimed to be on par or better than ChatGPT.\n", "published": "28-11-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model\n  System for Answering Medical Questions using Scientific Literature", "url": "http://arxiv.org/abs/2310.16146v1", "authors": "Alejandro Lozano, Scott L Fleming, Chia-Chun Chiang, Nigam Shah", "abstract": "  The quickly-expanding nature of published medical literature makes it\nchallenging for clinicians and researchers to keep up with and summarize\nrecent, relevant findings in a timely manner. While several closed-source\nsummarization tools based on large language models (LLMs) now exist, rigorous\nand systematic evaluations of their outputs are lacking. Furthermore, there is\na paucity of high-quality datasets and appropriate benchmark tasks with which\nto evaluate these tools. We address these issues with four contributions: we\nrelease Clinfo.ai, an open-source WebApp that answers clinical questions based\non dynamically retrieved scientific literature; we specify an information\nretrieval and abstractive summarization task to evaluate the performance of\nsuch retrieval-augmented LLM systems; we release a dataset of 200 questions and\ncorresponding answers derived from published systematic reviews, which we name\nPubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for\nClinfo.ai and other publicly available OpenQA systems on PubMedRS-200.\n", "published": "24-10-2023", "year": "2023", "categories": ["Artificial Intelligence", "Computation and Language"]}, {"title": "Clinical Text Summarization: Adapting Large Language Models Can\n  Outperform Human Experts", "url": "http://arxiv.org/abs/2309.07430v3", "authors": "Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad Aali, Christian Bluethgen, Anuj Pareek, Malgorzata Polacin, Eduardo Pontes Reis, Anna Seehofnerova, Nidhi Rohatgi, Poonam Hosamani, William Collins, Neera Ahuja, Curtis P. Langlotz, Jason Hom, Sergios Gatidis, John Pauly, Akshay S. Chaudhari", "abstract": "  Sifting through vast textual data and summarizing key information from\nelectronic health records (EHR) imposes a substantial burden on how clinicians\nallocate their time. Although large language models (LLMs) have shown immense\npromise in natural language processing (NLP) tasks, their efficacy on a diverse\nrange of clinical summarization tasks has not yet been rigorously demonstrated.\nIn this work, we apply domain adaptation methods to eight LLMs, spanning six\ndatasets and four distinct clinical summarization tasks: radiology reports,\npatient questions, progress notes, and doctor-patient dialogue. Our thorough\nquantitative assessment reveals trade-offs between models and adaptation\nmethods in addition to instances where recent advances in LLMs may not improve\nresults. Further, in a clinical reader study with ten physicians, we show that\nsummaries from our best-adapted LLMs are preferable to human summaries in terms\nof completeness and correctness. Our ensuing qualitative analysis highlights\nchallenges faced by both LLMs and human experts. Lastly, we correlate\ntraditional quantitative NLP metrics with reader study scores to enhance our\nunderstanding of how these metrics align with physician preferences. Our\nresearch marks the first evidence of LLMs outperforming human experts in\nclinical text summarization across multiple tasks. This implies that\nintegrating LLMs into clinical workflows could alleviate documentation burden,\nempowering clinicians to focus more on personalized patient care and the\ninherently human aspects of medicine.\n", "published": "14-09-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "CogAgent: A Visual Language Model for GUI Agents", "url": "http://arxiv.org/abs/2312.08914v2", "authors": "Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang", "abstract": "  People are spending an enormous amount of time on digital devices through\ngraphical user interfaces (GUIs), e.g., computer or smartphone screens. Large\nlanguage models (LLMs) such as ChatGPT can assist people in tasks like writing\nemails, but struggle to understand and interact with GUIs, thus limiting their\npotential to increase automation levels. In this paper, we introduce CogAgent,\nan 18-billion-parameter visual language model (VLM) specializing in GUI\nunderstanding and navigation. By utilizing both low-resolution and\nhigh-resolution image encoders, CogAgent supports input at a resolution of\n1120*1120, enabling it to recognize tiny page elements and text. As a\ngeneralist visual language model, CogAgent achieves the state of the art on\nfive text-rich and four general VQA benchmarks, including VQAv2, OK-VQA,\nText-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using\nonly screenshots as input, outperforms LLM-based methods that consume extracted\nHTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW,\nadvancing the state of the art. The model and codes are available at\nhttps://github.com/THUDM/CogVLM .\n", "published": "14-12-2023", "year": "2023", "categories": []}, {"title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late\n  Interaction over BERT", "url": "http://arxiv.org/abs/2004.12832v2", "authors": "Omar Khattab, Matei Zaharia", "abstract": "  Recent progress in Natural Language Understanding (NLU) is driving fast-paced\nadvances in Information Retrieval (IR), largely owed to fine-tuning deep\nlanguage models (LMs) for document ranking. While remarkably effective, the\nranking models based on these LMs increase computational cost by orders of\nmagnitude over prior approaches, particularly as they must feed each\nquery-document pair through a massive neural network to compute a single\nrelevance score. To tackle this, we present ColBERT, a novel ranking model that\nadapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT\nintroduces a late interaction architecture that independently encodes the query\nand the document using BERT and then employs a cheap yet powerful interaction\nstep that models their fine-grained similarity. By delaying and yet retaining\nthis fine-granular interaction, ColBERT can leverage the expressiveness of deep\nLMs while simultaneously gaining the ability to pre-compute document\nrepresentations offline, considerably speeding up query processing. Beyond\nreducing the cost of re-ranking the documents retrieved by a traditional model,\nColBERT's pruning-friendly interaction mechanism enables leveraging\nvector-similarity indexes for end-to-end retrieval directly from a large\ndocument collection. We extensively evaluate ColBERT using two recent passage\nsearch datasets. Results show that ColBERT's effectiveness is competitive with\nexisting BERT-based models (and outperforms every non-BERT baseline), while\nexecuting two orders-of-magnitude faster and requiring four orders-of-magnitude\nfewer FLOPs per query.\n", "published": "27-04-2020", "year": "2020", "categories": ["Computation and Language"]}, {"title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late\n  Interaction", "url": "http://arxiv.org/abs/2112.01488v3", "authors": "Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia", "abstract": "  Neural information retrieval (IR) has greatly advanced search and other\nknowledge-intensive language tasks. While many neural IR methods encode queries\nand documents into single-vector representations, late interaction models\nproduce multi-vector representations at the granularity of each token and\ndecompose relevance modeling into scalable token-level computations. This\ndecomposition has been shown to make late interaction more effective, but it\ninflates the space footprint of these models by an order of magnitude. In this\nwork, we introduce ColBERTv2, a retriever that couples an aggressive residual\ncompression mechanism with a denoised supervision strategy to simultaneously\nimprove the quality and space footprint of late interaction. We evaluate\nColBERTv2 across a wide range of benchmarks, establishing state-of-the-art\nquality within and outside the training domain while reducing the space\nfootprint of late interaction models by 6--10$\\times$.\n", "published": "02-12-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "Collaborating with language models for embodied reasoning", "url": "http://arxiv.org/abs/2302.00763v1", "authors": "Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, Rob Fergus", "abstract": "  Reasoning in a complex and ambiguous environment is a key goal for\nReinforcement Learning (RL) agents. While some sophisticated RL agents can\nsuccessfully solve difficult tasks, they require a large amount of training\ndata and often struggle to generalize to new unseen environments and new tasks.\nOn the other hand, Large Scale Language Models (LSLMs) have exhibited strong\nreasoning ability and the ability to to adapt to new tasks through in-context\nlearning. However, LSLMs do not inherently have the ability to interrogate or\nintervene on the environment. In this work, we investigate how to combine these\ncomplementary abilities in a single system consisting of three parts: a\nPlanner, an Actor, and a Reporter. The Planner is a pre-trained language model\nthat can issue commands to a simple embodied agent (the Actor), while the\nReporter communicates with the Planner to inform its next command. We present a\nset of tasks that require reasoning, test this system's ability to generalize\nzero-shot and investigate failure cases, and demonstrate how components of this\nsystem can be trained with reinforcement-learning to improve performance.\n", "published": "01-02-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM\n  Inference with Transferable Prompt", "url": "http://arxiv.org/abs/2305.11186v2", "authors": "Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava", "abstract": "  While the numerous parameters in Large Language Models (LLMs) contribute to\ntheir superior performance, this massive scale makes them inefficient and\nmemory-hungry. Thus, they are hard to deploy on commodity hardware, such as one\nsingle GPU. Given the memory and power constraints of such devices, model\ncompression methods are widely employed to reduce both the model size and\ninference latency, which essentially trades off model quality in return for\nimproved efficiency. Thus, optimizing this accuracy-efficiency trade-off is\ncrucial for the LLM deployment on commodity hardware. In this paper, we\nintroduce a new perspective to optimize this trade-off by prompting compressed\nmodels. Specifically, we first observe that for certain questions, the\ngeneration quality of a compressed LLM can be significantly improved by adding\ncarefully designed hard prompts, though this isn't the case for all questions.\nBased on this observation, we propose a soft prompt learning method where we\nexpose the compressed model to the prompt learning process, aiming to enhance\nthe performance of prompts. Our experimental analysis suggests our soft prompt\nstrategy greatly improves the performance of the 8x compressed LLaMA-7B model\n(with a joint 4-bit quantization and 50% weight pruning compression), allowing\nthem to match their uncompressed counterparts on popular benchmarks. Also, we\ndemonstrate that these learned prompts can be transferred across various\ndatasets, tasks, and compression levels. Hence with this transferability, we\ncan stitch the soft prompt to a newly compressed model to improve the test-time\naccuracy in an ``in-situ'' way.\n", "published": "17-05-2023", "year": "2023", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Compressing Context to Enhance Inference Efficiency of Large Language\n  Models", "url": "http://arxiv.org/abs/2310.06201v1", "authors": "Yucheng Li, Bo Dong, Chenghua Lin, Frank Guerin", "abstract": "  Large language models (LLMs) achieved remarkable performance across various\ntasks. However, they face challenges in managing long documents and extended\nconversations, due to significantly increased computational requirements, both\nin memory and inference time, and potential context truncation when the input\nexceeds the LLM's fixed context length. This paper proposes a method called\nSelective Context that enhances the inference efficiency of LLMs by identifying\nand pruning redundancy in the input context to make the input more compact. We\ntest our approach using common data sources requiring long context processing:\narXiv papers, news articles, and long conversations, on tasks of summarisation,\nquestion answering, and response generation. Experimental results show that\nSelective Context significantly reduces memory cost and decreases generation\nlatency while maintaining comparable performance compared to that achieved when\nfull context is used. Specifically, we achieve a 50\\% reduction in context\ncost, resulting in a 36\\% reduction in inference memory usage and a 32\\%\nreduction in inference time, while observing only a minor drop of .023 in\nBERTscore and .038 in faithfulness on four downstream applications, indicating\nthat our method strikes a good balance between efficiency and performance.\n", "published": "09-10-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Connecting Large Language Models with Evolutionary Algorithms Yields\n  Powerful Prompt Optimizers", "url": "http://arxiv.org/abs/2309.08532v1", "authors": "Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang", "abstract": "  Large Language Models (LLMs) excel in various tasks, but they rely on\ncarefully crafted prompts that often demand substantial human effort. To\nautomate this process, in this paper, we propose a novel framework for discrete\nprompt optimization, called EvoPrompt, which borrows the idea of evolutionary\nalgorithms (EAs) as they exhibit good performance and fast convergence. To\nenable EAs to work on discrete prompts, which are natural language expressions\nthat need to be coherent and human-readable, we connect LLMs with EAs. This\napproach allows us to simultaneously leverage the powerful language processing\ncapabilities of LLMs and the efficient optimization performance of EAs.\nSpecifically, abstaining from any gradients or parameters, EvoPrompt starts\nfrom a population of prompts and iteratively generates new prompts with LLMs\nbased on the evolutionary operators, improving the population based on the\ndevelopment set. We optimize prompts for both closed- and open-source LLMs\nincluding GPT-3.5 and Alpaca, on 9 datasets spanning language understanding and\ngeneration tasks. EvoPrompt significantly outperforms human-engineered prompts\nand existing methods for automatic prompt generation by up to 25% and 14%\nrespectively. Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs\ncreates synergies, which could inspire further research on the combination of\nLLMs and conventional algorithms.\n", "published": "15-09-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Constitutional AI: Harmlessness from AI Feedback", "url": "http://arxiv.org/abs/2212.08073v1", "authors": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Jared Kaplan", "abstract": "  As AI systems become more capable, we would like to enlist their help to\nsupervise other AIs. We experiment with methods for training a harmless AI\nassistant through self-improvement, without any human labels identifying\nharmful outputs. The only human oversight is provided through a list of rules\nor principles, and so we refer to the method as 'Constitutional AI'. The\nprocess involves both a supervised learning and a reinforcement learning phase.\nIn the supervised phase we sample from an initial model, then generate\nself-critiques and revisions, and then finetune the original model on revised\nresponses. In the RL phase, we sample from the finetuned model, use a model to\nevaluate which of the two samples is better, and then train a preference model\nfrom this dataset of AI preferences. We then train with RL using the preference\nmodel as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a\nresult we are able to train a harmless but non-evasive AI assistant that\nengages with harmful queries by explaining its objections to them. Both the SL\nand RL methods can leverage chain-of-thought style reasoning to improve the\nhuman-judged performance and transparency of AI decision making. These methods\nmake it possible to control AI behavior more precisely and with far fewer human\nlabels.\n", "published": "15-12-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Contrastive Chain-of-Thought Prompting", "url": "http://arxiv.org/abs/2311.09277v1", "authors": "Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, Lidong Bing", "abstract": "  Despite the success of chain of thought in enhancing language model\nreasoning, the underlying process remains less well understood. Although\nlogically sound reasoning appears inherently crucial for chain of thought,\nprior studies surprisingly reveal minimal impact when using invalid\ndemonstrations instead. Furthermore, the conventional chain of thought does not\ninform language models on what mistakes to avoid, which potentially leads to\nmore errors. Hence, inspired by how humans can learn from both positive and\nnegative examples, we propose contrastive chain of thought to enhance language\nmodel reasoning. Compared to the conventional chain of thought, our approach\nprovides both valid and invalid reasoning demonstrations, to guide the model to\nreason step-by-step while reducing reasoning mistakes. To improve\ngeneralization, we introduce an automatic method to construct contrastive\ndemonstrations. Our experiments on reasoning benchmarks demonstrate that\ncontrastive chain of thought can serve as a general enhancement of\nchain-of-thought prompting.\n", "published": "15-11-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Contrastive Preference Learning: Learning from Human Feedback without RL", "url": "http://arxiv.org/abs/2310.13639v2", "authors": "Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, Dorsa Sadigh", "abstract": "  Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular\nparadigm for aligning models with human intent. Typically RLHF algorithms\noperate in two phases: first, use human preferences to learn a reward function\nand second, align the model by optimizing the learned reward via reinforcement\nlearning (RL). This paradigm assumes that human preferences are distributed\naccording to reward, but recent work suggests that they instead follow the\nregret under the user's optimal policy. Thus, learning a reward function from\nfeedback is not only based on a flawed assumption of human preference, but also\nleads to unwieldy optimization challenges that stem from policy gradients or\nbootstrapping in the RL phase. Because of these optimization challenges,\ncontemporary RLHF methods restrict themselves to contextual bandit settings\n(e.g., as in large language models) or limit observation dimensionality (e.g.,\nstate-based robotics). We overcome these limitations by introducing a new\nfamily of algorithms for optimizing behavior from human feedback using the\nregret-based model of human preferences. Using the principle of maximum\nentropy, we derive Contrastive Preference Learning (CPL), an algorithm for\nlearning optimal policies from preferences without learning reward functions,\ncircumventing the need for RL. CPL is fully off-policy, uses only a simple\ncontrastive objective, and can be applied to arbitrary MDPs. This enables CPL\nto elegantly scale to high-dimensional and sequential RLHF problems while being\nsimpler than prior methods.\n", "published": "20-10-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence"]}, {"title": "Conversational Health Agents: A Personalized LLM-Powered Agent Framework", "url": "http://arxiv.org/abs/2310.02374v3", "authors": "Mahyar Abbasian, Iman Azimi, Amir M. Rahmani, Ramesh Jain", "abstract": "  Conversational Health Agents (CHAs) are interactive systems that provide\nhealthcare services, such as assistance, self-awareness, and diagnosis. Current\nCHAs, especially those utilizing Large Language Models (LLMs), primarily focus\non conversation aspects. However, they offer limited agent capabilities\nspecifically lacking multi-step problem-solving, empathetic conversations, and\nmultimodal data analysis. Our aim is to overcome these limitations. In this\npaper, we propose an LLM-powered framework to empower CHAs to generate a\npersonalized response for users' healthcare queries. This framework provides\ncritical thinking, knowledge acquisition, and problem-solving abilities by\nintegrating healthcare data sources, enabling multilingual and multimodal\nconversations, and interacting with various user data analysis tools. We\nillustrate the framework's proficiency in handling complex healthcare tasks via\na case study on stress level estimation, showcasing the agent's cognitive and\noperational capabilities. Powered by our framework, the CHA can provide\nappropriate responses, when the user inquires about their stress level. To\nachieve this, it learns to collect photoplethysmogram signals, converts them\ninto heart rate variability, and interprets them as indicators of stress\nlevels.\n", "published": "03-10-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Correctness Comparison of ChatGPT-4, Bard, Claude-2, and Copilot for\n  Spatial Tasks", "url": "http://arxiv.org/abs/2401.02404v2", "authors": "Hartwig H. Hochmair, Levente Juhasz, Takoda Kemp", "abstract": "  Generative AI including large language models (LLMs) have recently gained\nsignificant interest in the geo-science community through its versatile\ntask-solving capabilities including coding, spatial computations, generation of\nsample data, time-series forecasting, toponym recognition, or image\nclassification. So far, the assessment of LLMs for spatial tasks has primarily\nfocused on ChatGPT, arguably the most prominent AI chatbot, whereas other\nchatbots received less attention. To narrow this research gap, this study\nevaluates the correctness of responses for a set of 54 spatial tasks assigned\nto four prominent chatbots, i.e., ChatGPT-4, Bard, Claude-2, and Copilot.\nOverall, the chatbots performed well on spatial literacy, GIS theory, and\ninterpretation of programming code and given functions, but revealed weaknesses\nin mapping, code generation, and code translation. ChatGPT-4 outperformed other\nchatbots across most task categories.\n", "published": "04-01-2024", "year": "2024", "categories": []}, {"title": "Cramming: Training a Language Model on a Single GPU in One Day", "url": "http://arxiv.org/abs/2212.14034v1", "authors": "Jonas Geiping, Tom Goldstein", "abstract": "  Recent trends in language modeling have focused on increasing performance\nthrough scaling, and have resulted in an environment where training language\nmodels is out of reach for most researchers and practitioners. While most in\nthe community are asking how to push the limits of extreme computation, we ask\nthe opposite question: How far can we get with a single GPU in just one day?\n  We investigate the downstream performance achievable with a transformer-based\nlanguage model trained completely from scratch with masked language modeling\nfor a single day on a single consumer GPU. Aside from re-analyzing nearly all\ncomponents of the pretraining pipeline for this scenario and providing a\nmodified pipeline with performance close to BERT, we investigate why scaling\ndown is hard, and which modifications actually improve performance in this\nscenario. We provide evidence that even in this constrained setting,\nperformance closely follows scaling laws observed in large-compute settings.\nThrough the lens of scaling laws, we categorize a range of recent improvements\nto training and architecture and discuss their merit and practical\napplicability (or lack thereof) for the limited compute setting.\n", "published": "28-12-2022", "year": "2022", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Creative Agents: Empowering Agents with Imagination for Creative Tasks", "url": "http://arxiv.org/abs/2312.02519v1", "authors": "Chi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, Zongqing Lu", "abstract": "  We study building embodied agents for open-ended creative tasks. While\nexisting methods build instruction-following agents that can perform diverse\nopen-ended tasks, none of them demonstrates creativity -- the ability to give\nnovel and diverse task solutions implicit in the language instructions. This\nlimitation comes from their inability to convert abstract language instructions\ninto concrete task goals in the environment and perform long-horizon planning\nfor such complicated goals. Given the observation that humans perform creative\ntasks with the help of imagination, we propose a class of solutions for\ncreative agents, where the controller is enhanced with an imaginator that\ngenerates detailed imaginations of task outcomes conditioned on language\ninstructions. We introduce several approaches to implementing the components of\ncreative agents. We implement the imaginator with either a large language model\nfor textual imagination or a diffusion model for visual imagination. The\ncontroller can either be a behavior-cloning policy learned from data or a\npre-trained foundation model generating executable codes in the environment. We\nbenchmark creative tasks with the challenging open-world game Minecraft, where\nthe agents are asked to create diverse buildings given free-form language\ninstructions. In addition, we propose novel evaluation metrics for open-ended\ncreative tasks utilizing GPT-4V, which holds many advantages over existing\nmetrics. We perform a detailed experimental analysis of creative agents,\nshowing that creative agents are the first AI agents accomplishing diverse\nbuilding creation in the survival mode of Minecraft. Our benchmark and models\nare open-source for future research on creative agents\n(https://github.com/PKU-RL/Creative-Agents).\n", "published": "05-12-2023", "year": "2023", "categories": ["Artificial Intelligence", "Machine Learning"]}, {"title": "Crosslingual Generalization through Multitask Finetuning", "url": "http://arxiv.org/abs/2211.01786v2", "authors": "Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel", "abstract": "  Multitask prompted finetuning (MTF) has been shown to help large language\nmodels generalize to new tasks in a zero-shot setting, but so far explorations\nof MTF have focused on English data and models. We apply MTF to the pretrained\nmultilingual BLOOM and mT5 model families to produce finetuned variants called\nBLOOMZ and mT0. We find finetuning large multilingual language models on\nEnglish tasks with English prompts allows for task generalization to\nnon-English languages that appear only in the pretraining corpus. Finetuning on\nmultilingual tasks with English prompts further improves performance on English\nand non-English tasks leading to various state-of-the-art zero-shot results. We\nalso investigate finetuning on multilingual tasks with prompts that have been\nmachine-translated from English to match the language of each dataset. We find\ntraining on these machine-translated prompts leads to better performance on\nhuman-written prompts in the respective languages. Surprisingly, we find models\nare capable of zero-shot generalization to tasks in languages they have never\nintentionally seen. We conjecture that the models are learning higher-level\ncapabilities that are both task- and language-agnostic. In addition, we\nintroduce xP3, a composite of supervised datasets in 46 languages with English\nand machine-translated prompts. Our code, datasets and models are freely\navailable at https://github.com/bigscience-workshop/xmtf.\n", "published": "03-11-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Cumulative Reasoning with Large Language Models", "url": "http://arxiv.org/abs/2308.04371v5", "authors": "Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao", "abstract": "  While language models are powerful and versatile, they often fail to address\nhighly complex problems. This is because solving complex problems requires\ndeliberate thinking, which has been only minimally guided during training. In\nthis paper, we propose a new method called Cumulative Reasoning (CR), which\nemploys language models in a cumulative and iterative manner to emulate human\nthought processes. By decomposing tasks into smaller components, CR streamlines\nthe problem-solving process, rendering it both more manageable and effective.\nFor logical inference tasks, CR consistently outperforms existing methods with\nan improvement up to 9.3%, and achieves an accuracy of 98.04% on the curated\nFOLIO wiki dataset. In the context of the Game of 24, CR achieves an accuracy\nof 98%, which signifies a substantial enhancement of 24% over the previous\nstate-of-the-art method. Finally, on the MATH dataset, we establish new\nstate-of-the-art results with 58.0% overall accuracy, surpassing the previous\nbest approach by a margin of 4.2%, and achieving 43% relative improvement on\nthe hardest level 5 problems (22.4% to 32.1%). Additionally, we expand the\nconcept of Cumulative Reasoning to incorporate a Python code environment,\ndeliberately omitting external aids such as retrieval and web browsing and\nfocusing solely on the LLM's intrinsic reasoning capabilities within a Python\ncode environment. Our experiments in this setting yielded impressive results,\nwith an overall accuracy of 72.2% on the MATH dataset, significantly\noutperforming the PAL method with 38.8% relative improvement. Code is available\nat https://github.com/iiis-ai/cumulative-reasoning.\n", "published": "08-08-2023", "year": "2023", "categories": ["Artificial Intelligence"]}, {"title": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with\n  Language Models", "url": "http://arxiv.org/abs/2106.13353v2", "authors": "Robert L. Logan IV, Ivana Bala\u017eevi\u0107, Eric Wallace, Fabio Petroni, Sameer Singh, Sebastian Riedel", "abstract": "  Prompting language models (LMs) with training examples and task descriptions\nhas been seen as critical to recent successes in few-shot learning. In this\nwork, we show that finetuning LMs in the few-shot setting can considerably\nreduce the need for prompt engineering. In fact, one can use null prompts,\nprompts that contain neither task-specific templates nor training examples, and\nachieve competitive accuracy to manually-tuned prompts across a wide range of\ntasks. While finetuning LMs does introduce new parameters for each downstream\ntask, we show that this memory overhead can be substantially reduced:\nfinetuning only the bias terms can achieve comparable or better accuracy than\nstandard finetuning while only updating 0.1% of the parameters. All in all, we\nrecommend finetuning LMs for few-shot learning as it is more accurate, robust\nto different prompts, and can be made nearly as efficient as using frozen LMs.\n", "published": "24-06-2021", "year": "2021", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Data Management For Large Language Models: A Survey", "url": "http://arxiv.org/abs/2312.01700v2", "authors": "Zige Wang, Wanjun Zhong, Yufei Wang, Qi Zhu, Fei Mi, Baojun Wang, Lifeng Shang, Xin Jiang, Qun Liu", "abstract": "  Data plays a fundamental role in the training of Large Language Models\n(LLMs). Effective data management, particularly in the formulation of a\nwell-suited training dataset, holds significance for enhancing model\nperformance and improving training efficiency during pretraining and supervised\nfine-tuning phases. Despite the considerable importance of data management, the\ncurrent research community still falls short in providing a systematic analysis\nof the rationale behind management strategy selection, its consequential\neffects, methodologies for evaluating curated datasets, and the ongoing pursuit\nof improved strategies. Consequently, the exploration of data management has\nattracted more and more attention among the research community. This survey\nprovides a comprehensive overview of current research in data management within\nboth the pretraining and supervised fine-tuning stages of LLMs, covering\nvarious noteworthy aspects of data management strategy design: data quantity,\ndata quality, domain/task composition, etc. Looking toward the future, we\nextrapolate existing challenges and outline promising directions for\ndevelopment in this field. Therefore, this survey serves as a guiding resource\nfor practitioners aspiring to construct powerful LLMs through effective data\nmanagement practices. The collection of the latest papers is available at\nhttps://github.com/ZigeW/data_management_LLM.\n", "published": "04-12-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Deduplicating Training Data Makes Language Models Better", "url": "http://arxiv.org/abs/2107.06499v2", "authors": "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini", "abstract": "  We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.\n", "published": "14-07-2021", "year": "2021", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism", "url": "http://arxiv.org/abs/2401.02954v1", "authors": " DeepSeek-AI,  :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou", "abstract": "  The rapid development of open-source large language models (LLMs) has been\ntruly remarkable. However, the scaling law described in previous literature\npresents varying conclusions, which casts a dark cloud over scaling LLMs. We\ndelve into the study of scaling laws and present our distinctive findings that\nfacilitate scaling of large scale models in two commonly used open-source\nconfigurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek\nLLM, a project dedicated to advancing open-source language models with a\nlong-term perspective. To support the pre-training phase, we have developed a\ndataset that currently consists of 2 trillion tokens and is continuously\nexpanding. We further conduct supervised fine-tuning (SFT) and Direct\nPreference Optimization (DPO) on DeepSeek LLM Base models, resulting in the\ncreation of DeepSeek Chat models. Our evaluation results demonstrate that\nDeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in\nthe domains of code, mathematics, and reasoning. Furthermore, open-ended\nevaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance\ncompared to GPT-3.5.\n", "published": "05-01-2024", "year": "2024", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Describe, Explain, Plan and Select: Interactive Planning with Large\n  Language Models Enables Open-World Multi-Task Agents", "url": "http://arxiv.org/abs/2302.01560v2", "authors": "Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang", "abstract": "  We investigate the challenge of task planning for multi-task embodied agents\nin open-world environments. Two main difficulties are identified: 1) executing\nplans in an open-world environment (e.g., Minecraft) necessitates accurate and\nmulti-step reasoning due to the long-term nature of tasks, and 2) as vanilla\nplanners do not consider how easy the current agent can achieve a given\nsub-task when ordering parallel sub-goals within a complicated plan, the\nresulting plan could be inefficient or even infeasible. To this end, we propose\n\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and\n$\\underline{S}$elect\" ($\\textbf{DEPS}$), an interactive planning approach based\non Large Language Models (LLMs). DEPS facilitates better error correction on\ninitial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of\nthe plan execution process and providing self-$\\textit{explanation}$ of\nfeedback when encountering failures during the extended planning phases.\nFurthermore, it includes a goal $\\textit{selector}$, which is a trainable\nmodule that ranks parallel candidate sub-goals based on the estimated steps of\ncompletion, consequently refining the initial plan. Our experiments mark the\nmilestone of the first zero-shot multi-task agent that can robustly accomplish\n70+ Minecraft tasks and nearly double the overall performances. Further testing\nreveals our method's general effectiveness in popularly adopted non-open-ended\ndomains as well (i.e., ALFWorld and tabletop manipulation). The ablation and\nexploratory studies detail how our design beats the counterparts and provide a\npromising update on the $\\texttt{ObtainDiamond}$ grand challenge with our\napproach. The code is released at https://github.com/CraftJarvis/MC-Planner.\n", "published": "03-02-2023", "year": "2023", "categories": ["Artificial Intelligence"]}, {"title": "Differentiable Prompt Makes Pre-trained Language Models Better Few-shot\n  Learners", "url": "http://arxiv.org/abs/2108.13161v7", "authors": "Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, Huajun Chen", "abstract": "  Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance. Code is available in\nhttps://github.com/zjunlp/DART.\n", "published": "30-08-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Discrete and Soft Prompting for Multilingual Models", "url": "http://arxiv.org/abs/2109.03630v1", "authors": "Mengjie Zhao, Hinrich Sch\u00fctze", "abstract": "  It has been shown for English that discrete and soft prompting perform\nstrongly in few-shot learning with pretrained language models (PLMs). In this\npaper, we show that discrete and soft prompting perform better than finetuning\nin multilingual cases: Crosslingual transfer and in-language training of\nmultilingual natural language inference. For example, with 48 English training\nexamples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely\nsurpassing the majority baseline (33.33%). In contrast, discrete and soft\nprompting outperform finetuning, achieving 36.43% and 38.79%. We also\ndemonstrate good performance of prompting with training data in multiple\nlanguages other than English.\n", "published": "08-09-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "Distributed Inference and Fine-tuning of Large Language Models Over The\n  Internet", "url": "http://arxiv.org/abs/2312.08361v1", "authors": "Alexander Borzunov, Max Ryabinin, Artem Chumachenko, Dmitry Baranchuk, Tim Dettmers, Younes Belkada, Pavel Samygin, Colin Raffel", "abstract": "  Large language models (LLMs) are useful in many NLP tasks and become more\ncapable with size, with the best open-source models having over 50 billion\nparameters. However, using these 50B+ models requires high-end hardware, making\nthem inaccessible to most researchers. In this work, we investigate methods for\ncost-efficient inference and fine-tuning of LLMs, comparing local and\ndistributed strategies. We observe that a large enough model (50B+) can run\nefficiently even on geodistributed devices in a consumer-grade network. This\ncould allow running LLM efficiently by pooling together idle compute resources\nof multiple research groups and volunteers. We address two open problems: (1)\nhow to perform inference and fine-tuning reliably if any device can disconnect\nabruptly and (2) how to partition LLMs between devices with uneven hardware,\njoining and leaving at will. In order to do that, we develop special\nfault-tolerant inference algorithms and load-balancing protocols that\nautomatically assign devices to maximize the total system throughput. We\nshowcase these algorithms in Petals - a decentralized system that runs Llama 2\n(70B) and BLOOM (176B) over the Internet up to 10x faster than offloading for\ninteractive generation. We evaluate the performance of our system in simulated\nconditions and a real-world setup spanning two continents.\n", "published": "13-12-2023", "year": "2023", "categories": ["Machine Learning"]}, {"title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances", "url": "http://arxiv.org/abs/2204.01691v2", "authors": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng", "abstract": "  Large language models can encode a wealth of semantic knowledge about the\nworld. Such knowledge could be extremely useful to robots aiming to act upon\nhigh-level, temporally extended instructions expressed in natural language.\nHowever, a significant weakness of language models is that they lack real-world\nexperience, which makes it difficult to leverage them for decision making\nwithin a given embodiment. For example, asking a language model to describe how\nto clean a spill might result in a reasonable narrative, but it may not be\napplicable to a particular agent, such as a robot, that needs to perform this\ntask in a particular environment. We propose to provide real-world grounding by\nmeans of pretrained skills, which are used to constrain the model to propose\nnatural language actions that are both feasible and contextually appropriate.\nThe robot can act as the language model's \"hands and eyes,\" while the language\nmodel supplies high-level semantic knowledge about the task. We show how\nlow-level skills can be combined with large language models so that the\nlanguage model provides high-level knowledge about the procedures for\nperforming complex and temporally-extended instructions, while value functions\nassociated with these skills provide the grounding necessary to connect this\nknowledge to a particular physical environment. We evaluate our method on a\nnumber of real-world robotic tasks, where we show the need for real-world\ngrounding and that this approach is capable of completing long-horizon,\nabstract, natural language instructions on a mobile manipulator. The project's\nwebsite and the video can be found at https://say-can.github.io/.\n", "published": "04-04-2022", "year": "2022", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Do Prompt-Based Models Really Understand the Meaning of their Prompts?", "url": "http://arxiv.org/abs/2109.01247v2", "authors": "Albert Webson, Ellie Pavlick", "abstract": "  Recently, a boom of papers has shown extraordinary progress in zero-shot and\nfew-shot learning with various prompt-based models. It is commonly argued that\nprompts help models to learn faster in the same way that humans learn faster\nwhen provided with task instructions expressed in natural language. In this\nstudy, we experiment with over 30 prompt templates manually written for natural\nlanguage inference (NLI). We find that models learn just as fast with many\nprompts that are intentionally irrelevant or even pathologically misleading as\nthey do with instructively \"good\" prompts. Further, such patterns hold even for\nmodels as large as 175 billion parameters (Brown et al., 2020) as well as the\nrecently proposed instruction-tuned models which are trained on hundreds of\nprompts (Sanh et al., 2022). That is, instruction-tuned models often produce\ngood predictions with irrelevant and misleading prompts even at zero shots. In\nsum, notwithstanding prompt-based models' impressive improvement, we find\nevidence of serious limitations that question the degree to which such\nimprovement is derived from models understanding task instructions in ways\nanalogous to humans' use of task instructions.\n", "published": "02-09-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "DocLLM: A layout-aware generative language model for multimodal document\n  understanding", "url": "http://arxiv.org/abs/2401.00908v1", "authors": "Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu", "abstract": "  Enterprise documents such as forms, invoices, receipts, reports, contracts,\nand other similar records, often carry rich semantics at the intersection of\ntextual and spatial modalities. The visual cues offered by their complex\nlayouts play a crucial role in comprehending these documents effectively. In\nthis paper, we present DocLLM, a lightweight extension to traditional large\nlanguage models (LLMs) for reasoning over visual documents, taking into account\nboth textual semantics and spatial layout. Our model differs from existing\nmultimodal LLMs by avoiding expensive image encoders and focuses exclusively on\nbounding box information to incorporate the spatial layout structure.\nSpecifically, the cross-alignment between text and spatial modalities is\ncaptured by decomposing the attention mechanism in classical transformers to a\nset of disentangled matrices. Furthermore, we devise a pre-training objective\nthat learns to infill text segments. This approach allows us to address\nirregular layouts and heterogeneous content frequently encountered in visual\ndocuments. The pre-trained model is fine-tuned using a large-scale instruction\ndataset, covering four core document intelligence tasks. We demonstrate that\nour solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks,\nand generalizes well to 4 out of 5 previously unseen datasets.\n", "published": "31-12-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "EHRTutor: Enhancing Patient Understanding of Discharge Instructions", "url": "http://arxiv.org/abs/2310.19212v1", "authors": "Zihao Zhang, Zonghai Yao, Huixue Zhou, Feiyun ouyang, Hong Yu", "abstract": "  Large language models have shown success as a tutor in education in various\nfields. Educating patients about their clinical visits plays a pivotal role in\npatients' adherence to their treatment plans post-discharge. This paper\npresents EHRTutor, an innovative multi-component framework leveraging the Large\nLanguage Model (LLM) for patient education through conversational\nquestion-answering. EHRTutor first formulates questions pertaining to the\nelectronic health record discharge instructions. It then educates the patient\nthrough conversation by administering each question as a test. Finally, it\ngenerates a summary at the end of the conversation. Evaluation results using\nLLMs and domain experts have shown a clear preference for EHRTutor over the\nbaseline. Moreover, EHRTutor also offers a framework for generating synthetic\npatient education dialogues that can be used for future in-house system\ntraining.\n", "published": "30-10-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "EcoAssistant: Using LLM Assistant More Affordably and Accurately", "url": "http://arxiv.org/abs/2310.03046v1", "authors": "Jieyu Zhang, Ranjay Krishna, Ahmed H. Awadallah, Chi Wang", "abstract": "  Today, users ask Large language models (LLMs) as assistants to answer queries\nthat require external knowledge; they ask about the weather in a specific city,\nabout stock prices, and even about where specific locations are within their\nneighborhood. These queries require the LLM to produce code that invokes\nexternal APIs to answer the user's question, yet LLMs rarely produce correct\ncode on the first try, requiring iterative code refinement upon execution\nresults. In addition, using LLM assistants to support high query volumes can be\nexpensive. In this work, we contribute a framework, EcoAssistant, that enables\nLLMs to answer code-driven queries more affordably and accurately. EcoAssistant\ncontains three components. First, it allows the LLM assistants to converse with\nan automatic code executor to iteratively refine code or to produce answers\nbased on the execution results. Second, we use a hierarchy of LLM assistants,\nwhich attempts to answer the query with weaker, cheaper LLMs before backing off\nto stronger, expensive ones. Third, we retrieve solutions from past successful\nqueries as in-context demonstrations to help subsequent queries. Empirically,\nwe show that EcoAssistant offers distinct advantages for affordability and\naccuracy, surpassing GPT-4 by 10 points of success rate with less than 50% of\nGPT-4's cost.\n", "published": "03-10-2023", "year": "2023", "categories": ["Artificial Intelligence"]}, {"title": "Ecosystem Graphs: The Social Footprint of Foundation Models", "url": "http://arxiv.org/abs/2303.15772v1", "authors": "Rishi Bommasani, Dilara Soylu, Thomas I. Liao, Kathleen A. Creel, Percy Liang", "abstract": "  Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence\nsociety, warranting immediate social attention. While the models themselves\ngarner much attention, to accurately characterize their impact, we must\nconsider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a\ndocumentation framework to transparently centralize knowledge of this\necosystem. Ecosystem Graphs is composed of assets (datasets, models,\napplications) linked together by dependencies that indicate technical (e.g. how\nBing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI)\nrelationships. To supplement the graph structure, each asset is further\nenriched with fine-grained metadata (e.g. the license or training emissions).\nWe document the ecosystem extensively at\nhttps://crfm.stanford.edu/ecosystem-graphs/. As of March 16, 2023, we annotate\n262 assets (64 datasets, 128 models, 70 applications) from 63 organizations\nlinked by 356 dependencies. We show Ecosystem Graphs functions as a powerful\nabstraction and interface for achieving the minimum transparency required to\naddress myriad use cases. Therefore, we envision Ecosystem Graphs will be a\ncommunity-maintained resource that provides value to stakeholders spanning AI\nresearchers, industry professionals, social scientists, auditors and\npolicymakers.\n", "published": "28-03-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence"]}, {"title": "Efficient Few-Shot Clinical Task Adaptation with Large Language Models", "url": "http://arxiv.org/abs/2312.07125v1", "authors": "Kaipeng Zheng, Weiran Huang, Lichao Sun", "abstract": "  Few-shot learning has been studied to adapt models to tasks with very few\nsamples. It holds profound significance, particularly in clinical tasks, due to\nthe high annotation cost of medical images. Several works have explored\nfew-shot learning on medical images, yet they still require a large number of\nmedical images for pre-training models to gain domain-specific priors. Vision\nfoundation models recently have achieved remarkable success in natural images.\nHence, adapting rapidly advancing vision foundation models from natural images\nto few-shot clinical tasks holds great promise. MedFMC has recently organized a\nchallenge to shed more light on this topic at NeurIPS 2023. In this work, we\npresent our challenge solution. We observe that a simple variant of fine-tuning\nwith partial freezing shows remarkable performance. Empirical evidence\ndemonstrates that this approach could outperform various common fine-tuning\nmethods under limited sample sizes. Additionally, we explore enhanced\nutilization of semantic supervision to boost performance. We propose a novel\napproach that contextualizes labels via large language models (LLMs). Our\nfindings reveal that the context generated by LLMs significantly enhances the\ndiscrimination of semantic embeddings for similar categories, resulting in a\nnotable performance improvement of 3%-5% in 1-shot settings compared to\ncommonly employed one-hot labels and other semantic supervision methods. Our\nsolution secures the 1st place in the MedFMC challenge.\n", "published": "12-12-2023", "year": "2023", "categories": []}, {"title": "Emergent Abilities of Large Language Models", "url": "http://arxiv.org/abs/2206.07682v2", "authors": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus", "abstract": "  Scaling up language models has been shown to predictably improve performance\nand sample efficiency on a wide range of downstream tasks. This paper instead\ndiscusses an unpredictable phenomenon that we refer to as emergent abilities of\nlarge language models. We consider an ability to be emergent if it is not\npresent in smaller models but is present in larger models. Thus, emergent\nabilities cannot be predicted simply by extrapolating the performance of\nsmaller models. The existence of such emergence implies that additional scaling\ncould further expand the range of capabilities of language models.\n", "published": "15-06-2022", "year": "2022", "categories": ["Computation and Language"]}, {"title": "Enhancing the medical foundation model with multi-scale and\n  cross-modality feature learning", "url": "http://arxiv.org/abs/2401.01583v1", "authors": "Weijian Huang, Cheng Li, Hong-Yu Zhou, Jiarun Liu, Hao Yang, Yong Liang, Shanshan Wang", "abstract": "  The development of multi-modal medical foundation models has attracted\nsignificant attention in the field of medicine and healthcare due to their\npromising prospects in various clinical applications. One area of focus in this\nresearch direction is the extractions of features at different scales. While\nprevious studies have explored feature learning at individual scales,\ninvestigation on integrating the diverse scales and modalities of information\nis lacking, which may hinder the potential for mutual reinforcement among these\nfeatures. This paper aims to bridge this gap by proposing a method that\neffectively exploits multi-scale and cross-modality information to enhance the\nperformance of medical foundation models. The proposed method simultaneously\nexploit features at the local, instance, modality and global aspects,\nfacilitating comprehensive representation learning within the models. We\nevaluate the effectiveness of the proposed method on six open-source datasets\nacross different clinical tasks, demonstrating its ability to enhance the\nperformance of medical foundation models.\n", "published": "03-01-2024", "year": "2024", "categories": []}, {"title": "Evaluating ChatGPT text-mining of clinical records for obesity\n  monitoring", "url": "http://arxiv.org/abs/2308.01666v1", "authors": "Ivo S. Fins, Heather Davies, Sean Farrell, Jose R. Torres, Gina Pinchbeck, Alan D. Radford, Peter-John Noble", "abstract": "  Background: Veterinary clinical narratives remain a largely untapped resource\nfor addressing complex diseases. Here we compare the ability of a large\nlanguage model (ChatGPT) and a previously developed regular expression (RegexT)\nto identify overweight body condition scores (BCS) in veterinary narratives.\nMethods: BCS values were extracted from 4,415 anonymised clinical narratives\nusing either RegexT or by appending the narrative to a prompt sent to ChatGPT\ncoercing the model to return the BCS information. Data were manually reviewed\nfor comparison. Results: The precision of RegexT was higher (100%, 95% CI\n94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recall\nof ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that of\nRegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering is\nneeded to improve ChatGPT output. Conclusions: Large language models create\ndiverse opportunities and, whilst complex, present an intuitive interface to\ninformation but require careful implementation to avoid unpredictable errors.\n", "published": "03-08-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Evaluating Large Language Models: A Comprehensive Survey", "url": "http://arxiv.org/abs/2310.19736v3", "authors": "Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi,  Supryadi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities across\na broad spectrum of tasks. They have attracted significant attention and been\ndeployed in numerous downstream applications. Nevertheless, akin to a\ndouble-edged sword, LLMs also present potential risks. They could suffer from\nprivate data leaks or yield inappropriate, harmful, or misleading content.\nAdditionally, the rapid progress of LLMs raises concerns about the potential\nemergence of superintelligent systems without adequate safeguards. To\neffectively capitalize on LLM capacities as well as ensure their safe and\nbeneficial development, it is critical to conduct a rigorous and comprehensive\nevaluation of LLMs.\n  This survey endeavors to offer a panoramic perspective on the evaluation of\nLLMs. We categorize the evaluation of LLMs into three major groups: knowledge\nand capability evaluation, alignment evaluation and safety evaluation. In\naddition to the comprehensive review on the evaluation methodologies and\nbenchmarks on these three aspects, we collate a compendium of evaluations\npertaining to LLMs' performance in specialized domains, and discuss the\nconstruction of comprehensive evaluation platforms that cover LLM evaluations\non capabilities, alignment, safety, and applicability.\n  We hope that this comprehensive overview will stimulate further research\ninterests in the evaluation of LLMs, with the ultimate goal of making\nevaluation serve as a cornerstone in guiding the responsible development of\nLLMs. We envision that this will channel their evolution into a direction that\nmaximizes societal benefit while minimizing potential risks. A curated list of\nrelated papers has been publicly available at\nhttps://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.\n", "published": "30-10-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Experiential Co-Learning of Software-Developing Agents", "url": "http://arxiv.org/abs/2312.17025v2", "authors": "Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun", "abstract": "  Recent advancements in large language models (LLMs) have brought significant\nchanges to various domains, especially through LLM-driven autonomous agents.\nThese agents are now capable of collaborating seamlessly, splitting tasks and\nenhancing accuracy, thus minimizing the need for human involvement. However,\nthese agents often approach a diverse range of tasks in isolation, without\nbenefiting from past experiences. This isolation can lead to repeated mistakes\nand inefficient trials in task solving. To this end, this paper introduces\nExperiential Co-Learning, a novel framework in which instructor and assistant\nagents gather shortcut-oriented experiences from their historical trajectories\nand use these past experiences for mutual reasoning. This paradigm, enriched\nwith previous experiences, equips agents to more effectively address unseen\ntasks.\n", "published": "28-12-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "ExpertPrompting: Instructing Large Language Models to be Distinguished\n  Experts", "url": "http://arxiv.org/abs/2305.14688v1", "authors": "Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, Zhendong Mao", "abstract": "  The answering quality of an aligned large language model (LLM) can be\ndrastically improved if treated with proper crafting of prompts. In this paper,\nwe propose ExpertPrompting to elicit the potential of LLMs to answer as\ndistinguished experts. We first utilize In-Context Learning to automatically\nsynthesize detailed and customized descriptions of the expert identity for each\nspecific instruction, and then ask LLMs to provide answer conditioned on such\nagent background. Based on this augmented prompting strategy, we produce a new\nset of instruction-following data using GPT-3.5, and train a competitive\nopen-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation\nto show that 1) the expert data is of significantly higher quality than vanilla\nanswers, and 2) ExpertLLaMA outperforms existing open-source opponents and\nachieves 96\\% of the original ChatGPT's capability. All data and the\nExpertLLaMA model will be made publicly available at\n\\url{https://github.com/OFA-Sys/ExpertLLaMA}.\n", "published": "24-05-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Exploring Human-Like Translation Strategy with Large Language Models", "url": "http://arxiv.org/abs/2305.04118v3", "authors": "Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, Xing Wang", "abstract": "  Large language models (LLMs) have demonstrated impressive capabilities in\ngeneral scenarios, exhibiting a level of aptitude that approaches, in some\naspects even surpasses, human-level intelligence. Among their numerous skills,\nthe translation abilities of LLMs have received considerable attention.\nCompared to typical machine translation that focuses solely on source-to-target\nmapping, LLM-based translation can potentially mimic the human translation\nprocess which might take preparatory steps to ensure high-quality translation.\nThis work explores this possibility by proposing the MAPS framework, which\nstands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs\nfirst to analyze the given source sentence and induce three aspects of\ntranslation-related knowledge: keywords, topics, and relevant demonstrations to\nguide the final translation process. Moreover, we employ a selection mechanism\nbased on quality estimation to filter out noisy and unhelpful knowledge. Both\nautomatic (3 LLMs x 11 directions x 2 automatic metrics) and human evaluation\n(preference study and MQM) demonstrate the effectiveness of MAPS. Further\nanalysis shows that by mimicking the human translation process, MAPS reduces\nvarious translation errors such as hallucination, ambiguity, mistranslation,\nawkward style, untranslated text, and omission. Source code is available at\nhttps://github.com/zwhe99/MAPS-mt.\n", "published": "06-05-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation", "url": "http://arxiv.org/abs/2109.06513v2", "authors": "Chujie Zheng, Minlie Huang", "abstract": "  Dialog models can be greatly strengthened through grounding on various\nexternal information, but grounded dialog corpora are usually not naturally\naccessible. In this work, we focus on the few-shot learning for grounded dialog\ngeneration (GDG). We first propose a simple prompting method for GDG tasks,\nwhere different constructs of model input, such as the grounding source and the\nconversation context, are distinguished through continuous or discrete prompts.\nOn three typical GDG tasks, we empirically demonstrate and analyze in-depth the\neffectiveness of our method. We then conduct extensive experiments to\nthoroughly investigate how our prompting method works with different\npre-trained models. We show that prompted language models perform superiorly to\nconversational models, and further analyze various factors that influence the\neffects of prompting. Overall, our work introduces a prompt-based perspective\nto the few-shot learning for GDG tasks, and provides valuable findings and\ninsights for future research.\n", "published": "14-09-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "Exploring the Frontiers of LLMs in Psychological Applications: A\n  Comprehensive Review", "url": "http://arxiv.org/abs/2401.01519v2", "authors": "Luoma Ke, Song Tong, Peng Cheng, Kaiping Peng", "abstract": "  This paper explores the frontiers of large language models (LLMs) in\npsychology applications. Psychology has undergone several theoretical changes,\nand the current use of Artificial Intelligence (AI) and Machine Learning,\nparticularly LLMs, promises to open up new research directions. We provide a\ndetailed exploration of how LLMs like ChatGPT are transforming psychological\nresearch. It discusses the impact of LLMs across various branches of\npsychology, including cognitive and behavioral, clinical and counseling,\neducational and developmental, and social and cultural psychology, highlighting\ntheir potential to simulate aspects of human cognition and behavior. The paper\ndelves into the capabilities of these models to emulate human-like text\ngeneration, offering innovative tools for literature review, hypothesis\ngeneration, experimental design, experimental subjects, data analysis, academic\nwriting, and peer review in psychology. While LLMs are essential in advancing\nresearch methodologies in psychology, the paper also cautions about their\ntechnical and ethical challenges. There are issues like data privacy, the\nethical implications of using LLMs in psychological research, and the need for\na deeper understanding of these models' limitations. Researchers should\nresponsibly use LLMs in psychological studies, adhering to ethical standards\nand considering the potential consequences of deploying these technologies in\nsensitive areas. Overall, the article provides a comprehensive overview of the\ncurrent state of LLMs in psychology, exploring potential benefits and\nchallenges. It serves as a call to action for researchers to leverage LLMs'\nadvantages responsibly while addressing associated risks.\n", "published": "03-01-2024", "year": "2024", "categories": ["Machine Learning", "Artificial Intelligence"]}, {"title": "Exploring the Landscape of Large Language Models In Medical Question\n  Answering: Observations and Open Questions", "url": "http://arxiv.org/abs/2310.07225v1", "authors": "Karolina Korgul, Andrew M. Bean, Felix Krones, Robert McCraith, Adam Mahdi", "abstract": "  Large Language Models (LLMs) have shown promise in medical question answering\nby achieving passing scores in standardised exams and have been suggested as\ntools for supporting healthcare workers. Deploying LLMs into such a high-risk\ncontext requires a clear understanding of the limitations of these models. With\nthe rapid development and release of new LLMs, it is especially valuable to\nidentify patterns which exist across models and may, therefore, continue to\nappear in newer versions. In this paper, we evaluate a wide range of popular\nLLMs on their knowledge of medical questions in order to better understand\ntheir properties as a group. From this comparison, we provide preliminary\nobservations and raise open questions for further research.\n", "published": "11-10-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "url": "http://arxiv.org/abs/1910.10683v4", "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu", "abstract": "  Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.\n", "published": "23-10-2019", "year": "2019", "categories": ["Machine Learning", "Computation and Language"]}, {"title": "Exploring the intersection of Generative AI and Software Development", "url": "http://arxiv.org/abs/2312.14262v1", "authors": "Filipe Calegario, Vanilson Bur\u00e9gio, Francisco Erivaldo, Daniel Moraes Costa Andrade, Kailane Felix, Nathalia Barbosa, Pedro Lucas da Silva Lucena, C\u00e9sar Fran\u00e7a", "abstract": "  In the ever-evolving landscape of Artificial Intelligence (AI), the synergy\nbetween generative AI and Software Engineering emerges as a transformative\nfrontier. This whitepaper delves into the unexplored realm, elucidating how\ngenerative AI techniques can revolutionize software development. Spanning from\nproject management to support and updates, we meticulously map the demands of\neach development stage and unveil the potential of generative AI in addressing\nthem. Techniques such as zero-shot prompting, self-consistency, and multimodal\nchain-of-thought are explored, showcasing their unique capabilities in\nenhancing generative AI models. The significance of vector embeddings, context,\nplugins, tools, and code assistants is underscored, emphasizing their role in\ncapturing semantic information and amplifying generative AI capabilities.\nLooking ahead, this intersection promises to elevate productivity, improve code\nquality, and streamline the software development process. This whitepaper\nserves as a guide for stakeholders, urging discussions and experiments in the\napplication of generative AI in Software Engineering, fostering innovation and\ncollaboration for a qualitative leap in the efficiency and effectiveness of\nsoftware development.\n", "published": "21-12-2023", "year": "2023", "categories": ["Artificial Intelligence"]}, {"title": "Extending Context Window of Large Language Models via Semantic\n  Compression", "url": "http://arxiv.org/abs/2312.09571v1", "authors": "Weizhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han", "abstract": "  Transformer-based Large Language Models (LLMs) often impose limitations on\nthe length of the text input to ensure the generation of fluent and relevant\nresponses. This constraint restricts their applicability in scenarios involving\nlong texts. We propose a novel semantic compression method that enables\ngeneralization to texts that are 6-8 times longer, without incurring\nsignificant computational costs or requiring fine-tuning. Our proposed\nframework draws inspiration from source coding in information theory and\nemploys a pre-trained model to reduce the semantic redundancy of long inputs\nbefore passing them to the LLMs for downstream tasks. Experimental results\ndemonstrate that our method effectively extends the context window of LLMs\nacross a range of tasks including question answering, summarization, few-shot\nlearning, and information retrieval. Furthermore, the proposed semantic\ncompression method exhibits consistent fluency in text generation while\nreducing the associated computational overhead.\n", "published": "15-12-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "FLIN: A Flexible Natural Language Interface for Web Navigation", "url": "http://arxiv.org/abs/2010.12844v2", "authors": "Sahisnu Mazumder, Oriana Riva", "abstract": "  AI assistants can now carry out tasks for users by directly interacting with\nwebsite UIs. Current semantic parsing and slot-filling techniques cannot\nflexibly adapt to many different websites without being constantly re-trained.\nWe propose FLIN, a natural language interface for web navigation that maps user\ncommands to concept-level actions (rather than low-level UI actions), thus\nbeing able to flexibly adapt to different websites and handle their transient\nnature. We frame this as a ranking problem: given a user command and a webpage,\nFLIN learns to score the most relevant navigation instruction (involving action\nand parameter values). To train and evaluate FLIN, we collect a dataset using\nnine popular websites from three domains. Our results show that FLIN was able\nto adapt to new websites in a given domain.\n", "published": "24-10-2020", "year": "2020", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Faithful Chain-of-Thought Reasoning", "url": "http://arxiv.org/abs/2301.13379v3", "authors": "Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch", "abstract": "  While Chain-of-Thought (CoT) prompting boosts Language Models' (LM)\nperformance on a gamut of complex reasoning tasks, the generated reasoning\nchain does not necessarily reflect how the model arrives at the answer (aka.\nfaithfulness). We propose Faithful CoT, a reasoning framework involving two\nstages: Translation (Natural Language query $\\rightarrow$ symbolic reasoning\nchain) and Problem Solving (reasoning chain $\\rightarrow$ answer), using an LM\nand a deterministic solver respectively. This guarantees that the reasoning\nchain provides a faithful explanation of the final answer. Aside from\ninterpretability, Faithful CoT also improves empirical performance: it\noutperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a\nrelative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning,\n5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference.\nFurthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot\nperformance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong\nsynergy between faithfulness and accuracy.\n", "published": "31-01-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Faithful Persona-based Conversational Dataset Generation with Large\n  Language Models", "url": "http://arxiv.org/abs/2312.10007v1", "authors": "Pegah Jandaghi, XiangHai Sheng, Xinyi Bai, Jay Pujara, Hakim Sidahmed", "abstract": "  High-quality conversational datasets are essential for developing AI models\nthat can communicate with users. One way to foster deeper interactions between\na chatbot and its user is through personas, aspects of the user's character\nthat provide insights into their personality, motivations, and behaviors.\nTraining Natural Language Processing (NLP) models on a diverse and\ncomprehensive persona-based dataset can lead to conversational models that\ncreate a deeper connection with the user, and maintain their engagement. In\nthis paper, we leverage the power of Large Language Models (LLMs) to create a\nlarge, high-quality conversational dataset from a seed dataset. We propose a\nGenerator-Critic architecture framework to expand the initial dataset, while\nimproving the quality of its conversations. The Generator is an LLM prompted to\noutput conversations. The Critic consists of a mixture of expert LLMs that\ncontrol the quality of the generated conversations. These experts select the\nbest generated conversations, which we then use to improve the Generator. We\nrelease Synthetic-Persona-Chat, consisting of 20k conversations seeded from\nPersona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our\ngeneration framework on different dimensions through extensive experiments, and\nobserve that the losing rate of Synthetic-Persona-Chat against Persona-Chat\nduring Turing test decreases from 17.2% to 8.8% over three iterations.\n", "published": "15-12-2023", "year": "2023", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming\n  Few-Shot Prompt Order Sensitivity", "url": "http://arxiv.org/abs/2104.08786v2", "authors": "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp", "abstract": "  When primed with only a handful of training samples, very large, pretrained\nlanguage models such as GPT-3 have shown competitive results when compared to\nfully-supervised, fine-tuned, large, pretrained language models. We demonstrate\nthat the order in which the samples are provided can make the difference\nbetween near state-of-the-art and random guess performance: essentially some\npermutations are \"fantastic\" and some not. We analyse this phenomenon in\ndetail, establishing that: it is present across model sizes (even for the\nlargest current models), it is not related to a specific subset of samples, and\nthat a given good permutation for one model is not transferable to another.\nWhile one could use a development set to determine which permutations are\nperformant, this would deviate from the true few-shot setting as it requires\nadditional annotated data. Instead, we use the generative nature of language\nmodels to construct an artificial development set and based on entropy\nstatistics of the candidate permutations on this set, we identify performant\nprompts. Our method yields a 13% relative improvement for GPT-family models\nacross eleven different established text classification tasks.\n", "published": "18-04-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads\n  to Answers Faster", "url": "http://arxiv.org/abs/2311.08263v1", "authors": "Hongxuan Zhang, Zhining Liu, Jiaqi Zheng, Chenyi Zhuang, Jinjie Gu, Guihai Chen", "abstract": "  In this work, we propose FastCoT, a model-agnostic framework based on\nparallel decoding without any further training of an auxiliary model or\nmodification to the LLM itself. FastCoT uses a size-varying context window\nwhose size changes with position to conduct parallel decoding and\nauto-regressive decoding simultaneously, thus fully utilizing GPU computation\nresources. In FastCoT, the parallel decoding part provides the LLM with a quick\nglance of the future composed of approximate tokens, which could lead to faster\nanswers compared to regular autoregressive decoding used by causal\ntransformers. We also provide an implementation of parallel decoding within\nLLM, which supports KV-cache generation and batch processing. Through extensive\nexperiments, we demonstrate that FastCoT saves inference time by nearly 20%\nwith only a negligible performance drop compared to the regular approach.\nAdditionally, we show that the context window size exhibits considerable\nrobustness for different tasks.\n", "published": "14-11-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated\n  Learning", "url": "http://arxiv.org/abs/2108.06098v3", "authors": "Nam Hyeon-Woo, Moon Ye-Bin, Tae-Hyun Oh", "abstract": "  In this work, we propose a communication-efficient parameterization, FedPara,\nfor federated learning (FL) to overcome the burdens on frequent model uploads\nand downloads. Our method re-parameterizes weight parameters of layers using\nlow-rank weights followed by the Hadamard product. Compared to the conventional\nlow-rank parameterization, our FedPara method is not restricted to low-rank\nconstraints, and thereby it has a far larger capacity. This property enables to\nachieve comparable performance while requiring 3 to 10 times lower\ncommunication costs than the model with the original layers, which is not\nachievable by the traditional low-rank methods. The efficiency of our method\ncan be further improved by combining with other efficient FL optimizers. In\naddition, we extend our method to a personalized FL application, pFedPara,\nwhich separates parameters into global and local ones. We show that pFedPara\noutperforms competing personalized FL methods with more than three times fewer\nparameters.\n", "published": "13-08-2021", "year": "2021", "categories": ["Machine Learning"]}, {"title": "Few-shot Adaptation of Multi-modal Foundation Models: A Survey", "url": "http://arxiv.org/abs/2401.01736v2", "authors": "Fan Liu, Tianshu Zhang, Wenwen Dai, Wenwen Cai, Xiaocong Zhou, Delong Chen", "abstract": "  Multi-modal (vision-language) models, such as CLIP, are replacing traditional\nsupervised pre-training models (e.g., ImageNet-based pre-training) as the new\ngeneration of visual foundation models. These models with robust and aligned\nsemantic representations learned from billions of internet image-text pairs and\ncan be applied to various downstream tasks in a zero-shot manner. However, in\nsome fine-grained domains like medical imaging and remote sensing, the\nperformance of multi-modal foundation models often leaves much to be desired.\nConsequently, many researchers have begun to explore few-shot adaptation\nmethods for these models, gradually deriving three main technical approaches:\n1) prompt-based methods, 2) adapter-based methods, and 3) external\nknowledge-based methods. Nevertheless, this rapidly developing field has\nproduced numerous results without a comprehensive survey to systematically\norganize the research progress. Therefore, in this survey, we introduce and\nanalyze the research advancements in few-shot adaptation methods for\nmulti-modal models, summarizing commonly used datasets and experimental setups,\nand comparing the results of different methods. In addition, due to the lack of\nreliable theoretical support for existing methods, we derive the few-shot\nadaptation generalization error bound for multi-modal models. The theorem\nreveals that the generalization error of multi-modal foundation models is\nconstrained by three factors: domain gap, model capacity, and sample size.\nBased on this, we propose three possible solutions from the following aspects:\n1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive\nknowledge utilization.\n", "published": "03-01-2024", "year": "2024", "categories": []}, {"title": "Finetuned Language Models Are Zero-Shot Learners", "url": "http://arxiv.org/abs/2109.01652v5", "authors": "Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le", "abstract": "  This paper explores a simple method for improving the zero-shot learning\nabilities of language models. We show that instruction tuning -- finetuning\nlanguage models on a collection of tasks described via instructions --\nsubstantially improves zero-shot performance on unseen tasks.\n  We take a 137B parameter pretrained language model and instruction-tune it on\nover 60 NLP tasks verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\ntypes. FLAN substantially improves the performance of its unmodified\ncounterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\nof finetuning datasets, model scale, and natural language instructions are key\nto the success of instruction tuning.\n", "published": "03-09-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "FlashAttention: Fast and Memory-Efficient Exact Attention with\n  IO-Awareness", "url": "http://arxiv.org/abs/2205.14135v2", "authors": "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R\u00e9", "abstract": "  Transformers are slow and memory-hungry on long sequences, since the time and\nmemory complexity of self-attention are quadratic in sequence length.\nApproximate attention methods have attempted to address this problem by trading\noff model quality to reduce the compute complexity, but often do not achieve\nwall-clock speedup. We argue that a missing principle is making attention\nalgorithms IO-aware -- accounting for reads and writes between levels of GPU\nmemory. We propose FlashAttention, an IO-aware exact attention algorithm that\nuses tiling to reduce the number of memory reads/writes between GPU high\nbandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of\nFlashAttention, showing that it requires fewer HBM accesses than standard\nattention, and is optimal for a range of SRAM sizes. We also extend\nFlashAttention to block-sparse attention, yielding an approximate attention\nalgorithm that is faster than any existing approximate attention method.\nFlashAttention trains Transformers faster than existing baselines: 15%\nend-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the\nMLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K),\nand 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention\nand block-sparse FlashAttention enable longer context in Transformers, yielding\nhigher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on\nlong-document classification) and entirely new capabilities: the first\nTransformers to achieve better-than-chance performance on the Path-X challenge\n(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1%\naccuracy).\n", "published": "27-05-2022", "year": "2022", "categories": ["Machine Learning"]}, {"title": "Foundation Models for Decision Making: Problems, Methods, and\n  Opportunities", "url": "http://arxiv.org/abs/2303.04129v1", "authors": "Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, Dale Schuurmans", "abstract": "  Foundation models pretrained on diverse data at scale have demonstrated\nextraordinary capabilities in a wide range of vision and language tasks. When\nsuch models are deployed in real world environments, they inevitably interface\nwith other entities and agents. For example, language models are often used to\ninteract with human beings through dialogue, and visual perception models are\nused to autonomously navigate neighborhood streets. In response to these\ndevelopments, new paradigms are emerging for training foundation models to\ninteract with other agents and perform long-term reasoning. These paradigms\nleverage the existence of ever-larger datasets curated for multimodal,\nmultitask, and generalist interaction. Research at the intersection of\nfoundation models and decision making holds tremendous promise for creating\npowerful new systems that can interact effectively across a diverse range of\napplications such as dialogue, autonomous driving, healthcare, education, and\nrobotics. In this manuscript, we examine the scope of foundation models for\ndecision making, and provide conceptual tools and technical background for\nunderstanding the problem space and exploring new research directions. We\nreview recent approaches that ground foundation models in practical decision\nmaking applications through a variety of methods such as prompting, conditional\ngenerative modeling, planning, optimal control, and reinforcement learning, and\ndiscuss common challenges and open problems in the field.\n", "published": "07-03-2023", "year": "2023", "categories": ["Artificial Intelligence", "Machine Learning"]}, {"title": "Foundation Models for Weather and Climate Data Understanding: A\n  Comprehensive Survey", "url": "http://arxiv.org/abs/2312.03014v1", "authors": "Shengchao Chen, Guodong Long, Jing Jiang, Dikai Liu, Chengqi Zhang", "abstract": "  As artificial intelligence (AI) continues to rapidly evolve, the realm of\nEarth and atmospheric sciences is increasingly adopting data-driven models,\npowered by progressive developments in deep learning (DL). Specifically, DL\ntechniques are extensively utilized to decode the chaotic and nonlinear aspects\nof Earth systems, and to address climate challenges via understanding weather\nand climate data. Cutting-edge performance on specific tasks within narrower\nspatio-temporal scales has been achieved recently through DL. The rise of large\nmodels, specifically large language models (LLMs), has enabled fine-tuning\nprocesses that yield remarkable outcomes across various downstream tasks,\nthereby propelling the advancement of general AI. However, we are still\nnavigating the initial stages of crafting general AI for weather and climate.\nIn this survey, we offer an exhaustive, timely overview of state-of-the-art AI\nmethodologies specifically engineered for weather and climate data, with a\nspecial focus on time series and text data. Our primary coverage encompasses\nfour critical aspects: types of weather and climate data, principal model\narchitectures, model scopes and applications, and datasets for weather and\nclimate. Furthermore, in relation to the creation and application of foundation\nmodels for weather and climate data understanding, we delve into the field's\nprevailing challenges, offer crucial insights, and propose detailed avenues for\nfuture research. This comprehensive approach equips practitioners with the\nrequisite knowledge to make substantial progress in this domain. Our survey\nencapsulates the most recent breakthroughs in research on large, data-driven\nmodels for weather and climate data understanding, emphasizing robust\nfoundations, current advancements, practical applications, crucial resources,\nand prospective research opportunities.\n", "published": "05-12-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence"]}, {"title": "FreshLLMs: Refreshing Large Language Models with Search Engine\n  Augmentation", "url": "http://arxiv.org/abs/2310.03214v2", "authors": "Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, Thang Luong", "abstract": "  Most large language models (LLMs) are trained once and never updated; thus,\nthey lack the ability to dynamically adapt to our ever-changing world. In this\nwork, we perform a detailed study of the factuality of LLM-generated text in\nthe context of answering questions that test current world knowledge.\nSpecifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a\ndiverse range of question and answer types, including questions that require\nfast-changing world knowledge as well as questions with false premises that\nneed to be debunked. We benchmark a diverse array of both closed and\nopen-source LLMs under a two-mode evaluation procedure that allows us to\nmeasure both correctness and hallucination. Through human evaluations involving\nmore than 50K judgments, we shed light on limitations of these models and\ndemonstrate significant room for improvement: for instance, all models\n(regardless of model size) struggle on questions that involve fast-changing\nknowledge and false premises. Motivated by these results, we present\nFreshPrompt, a simple few-shot prompting method that substantially boosts the\nperformance of an LLM on FreshQA by incorporating relevant and up-to-date\ninformation retrieved from a search engine into the prompt. Our experiments\nshow that FreshPrompt outperforms both competing search engine-augmented\nprompting methods such as Self-Ask (Press et al., 2022) as well as commercial\nsystems such as Perplexity.AI. Further analysis of FreshPrompt reveals that\nboth the number of retrieved evidences and their order play a key role in\ninfluencing the correctness of LLM-generated answers. Additionally, instructing\nthe LLM to generate concise and direct answers helps reduce hallucination\ncompared to encouraging more verbose answers. To facilitate future work, we\nrelease FreshQA at github.com/freshllms/freshqa and commit to updating it at\nregular intervals.\n", "published": "05-10-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "From Classification to Clinical Insights: Towards Analyzing and\n  Reasoning About Mobile and Behavioral Health Data With Large Language Models", "url": "http://arxiv.org/abs/2311.13063v2", "authors": "Zachary Englhardt, Chengqian Ma, Margaret E. Morris, Xuhai \"Orson\" Xu, Chun-Cheng Chang, Lianhui Qin, Daniel McDuff, Xin Liu, Shwetak Patel, Vikram Iyer", "abstract": "  Passively collected behavioral health data from ubiquitous sensors holds\nsignificant promise to provide mental health professionals insights from\npatient's daily lives; however, developing analysis tools to use this data in\nclinical practice requires addressing challenges of generalization across\ndevices and weak or ambiguous correlations between the measured signals and an\nindividual's mental health. To address these challenges, we take a novel\napproach that leverages large language models (LLMs) to synthesize clinically\nuseful insights from multi-sensor data. We develop chain of thought prompting\nmethods that use LLMs to generate reasoning about how trends in data such as\nstep count and sleep relate to conditions like depression and anxiety. We first\ndemonstrate binary depression classification with LLMs achieving accuracies of\n61.1% which exceed the state of the art. While it is not robust for clinical\nuse, this leads us to our key finding: even more impactful and valued than\nclassification is a new human-AI collaboration approach in which clinician\nexperts interactively query these tools and combine their domain expertise and\ncontext about the patient with AI generated reasoning to support clinical\ndecision-making. We find models like GPT-4 correctly reference numerical data\n75% of the time, and clinician participants express strong interest in using\nthis approach to interpret self-tracking data.\n", "published": "21-11-2023", "year": "2023", "categories": ["Artificial Intelligence"]}, {"title": "From Human Days to Machine Seconds: Automatically Answering and\n  Generating Machine Learning Final Exams", "url": "http://arxiv.org/abs/2206.05442v7", "authors": "Iddo Drori, Sarah J. Zhang, Reece Shuttleworth, Sarah Zhang, Keith Tyser, Zad Chin, Pedro Lantigua, Saisamrit Surbehera, Gregory Hunter, Derek Austin, Leonard Tang, Yann Hicke, Sage Simhon, Sathwik Karnik, Darnell Granberry, Madeleine Udell", "abstract": "  A final exam in machine learning at a top institution such as MIT, Harvard,\nor Cornell typically takes faculty days to write, and students hours to solve.\nWe demonstrate that large language models pass machine learning finals at a\nhuman level, on finals available online after the models were trained, and\nautomatically generate new human-quality final exam questions in seconds.\nPrevious work has developed program synthesis and few-shot learning methods to\nsolve university-level problem set questions in mathematics and STEM courses.\nIn this work, we develop and compare methods that solve final exams, which\ndiffer from problem sets in several ways: the questions are longer, have\nmultiple parts, are more complicated, and span a broader set of topics. We\ncurate a dataset and benchmark of questions from machine learning final exams\navailable online and code for answering these questions and generating new\nquestions. We show how to generate new questions from other questions and\ncourse notes. For reproducibility and future research on this final exam\nbenchmark, we use automatic checkers for multiple-choice, numeric, and\nquestions with expression answers. We perform ablation studies comparing\nzero-shot learning with few-shot learning and chain-of-thought prompting using\nGPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that\nfew-shot learning methods perform best. We highlight the transformative\npotential of language models to streamline the writing and solution of\nlarge-scale assessments, significantly reducing the workload from human days to\nmere machine seconds. Our results suggest that rather than banning large\nlanguage models such as ChatGPT in class, instructors should teach students to\nharness them by asking students meta-questions about correctness, completeness,\nand originality of the responses generated, encouraging critical thinking in\nacademic studies.\n", "published": "11-06-2022", "year": "2022", "categories": ["Machine Learning"]}, {"title": "From LLM to Conversational Agent: A Memory Enhanced Architecture with\n  Fine-Tuning of Large Language Models", "url": "http://arxiv.org/abs/2401.02777v1", "authors": "Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, Ming Cui", "abstract": "  This paper introduces RAISE (Reasoning and Acting through Scratchpad and\nExamples), an advanced architecture enhancing the integration of Large Language\nModels (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of\nthe ReAct framework, incorporates a dual-component memory system, mirroring\nhuman short-term and long-term memory, to maintain context and continuity in\nconversations. It entails a comprehensive agent construction scenario,\nincluding phases like Conversation Selection, Scene Extraction, CoT Completion,\nand Scene Augmentation, leading to the LLMs Training phase. This approach\nappears to enhance agent controllability and adaptability in complex,\nmulti-turn dialogues. Our preliminary evaluations in a real estate sales\ncontext suggest that RAISE has some advantages over traditional agents,\nindicating its potential for broader applications. This work contributes to the\nAI field by providing a robust framework for developing more context-aware and\nversatile conversational agents.\n", "published": "05-01-2024", "year": "2024", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "From Sparse to Dense: GPT-4 Summarization with Chain of Density\n  Prompting", "url": "http://arxiv.org/abs/2309.04269v1", "authors": "Griffin Adams, Alexander Fabbri, Faisal Ladhak, Eric Lehman, No\u00e9mie Elhadad", "abstract": "  Selecting the ``right'' amount of information to include in a summary is a\ndifficult task. A good summary should be detailed and entity-centric without\nbeing overly dense and hard to follow. To better understand this tradeoff, we\nsolicit increasingly dense GPT-4 summaries with what we refer to as a ``Chain\nof Density'' (CoD) prompt. Specifically, GPT-4 generates an initial\nentity-sparse summary before iteratively incorporating missing salient entities\nwithout increasing the length. Summaries generated by CoD are more abstractive,\nexhibit more fusion, and have less of a lead bias than GPT-4 summaries\ngenerated by a vanilla prompt. We conduct a human preference study on 100 CNN\nDailyMail articles and find that that humans prefer GPT-4 summaries that are\nmore dense than those generated by a vanilla prompt and almost as dense as\nhuman written summaries. Qualitative analysis supports the notion that there\nexists a tradeoff between informativeness and readability. 500 annotated CoD\nsummaries, as well as an extra 5,000 unannotated summaries, are freely\navailable on HuggingFace\n(https://huggingface.co/datasets/griffin/chain_of_density).\n", "published": "08-09-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "GPT Understands, Too", "url": "http://arxiv.org/abs/2103.10385v2", "authors": "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang", "abstract": "  Prompting a pretrained language model with natural language patterns has been\nproved effective for natural language understanding (NLU). However, our\npreliminary study reveals that manual discrete prompts often lead to unstable\nperformance -- e.g., changing a single word in the prompt might result in\nsubstantial performance drop. We propose a novel method P-Tuning that employs\ntrainable continuous prompt embeddings in concatenation with discrete prompts.\nEmpirically, P-Tuning not only stabilizes training by minimizing the gap\nbetween various discrete prompts, but also improves performance by a sizeable\nmargin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is\ngenerally effective for both frozen and tuned language models, under both the\nfully-supervised and few-shot settings.\n", "published": "18-03-2021", "year": "2021", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "GPT-4 Technical Report", "url": "http://arxiv.org/abs/2303.08774v4", "authors": " OpenAI,  :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim\u00f3n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, \u0141ukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, \u0141ukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M\u00e9ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto,  Michael,  Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer\u00f3n Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, Barret Zoph", "abstract": "  We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.\n", "published": "15-03-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation", "url": "http://arxiv.org/abs/2104.08826v2", "authors": "Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, Woomyeong Park", "abstract": "  Large-scale language models such as GPT-3 are excellent few-shot learners,\nallowing them to be controlled via natural text prompts. Recent studies report\nthat prompt-based direct classification eliminates the need for fine-tuning but\nlacks data and inference scalability. This paper proposes a novel data\naugmentation technique that leverages large-scale language models to generate\nrealistic text samples from a mixture of real samples. We also propose\nutilizing soft-labels predicted by the language models, effectively distilling\nknowledge from the large-scale language models and creating textual\nperturbations simultaneously. We perform data augmentation experiments on\ndiverse classification tasks and show that our method hugely outperforms\nexisting text augmentation methods. Ablation studies and a qualitative analysis\nprovide more insights into our approach.\n", "published": "18-04-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Gemini: A Family of Highly Capable Multimodal Models", "url": "http://arxiv.org/abs/2312.11805v1", "authors": " Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Ana\u00efs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Martin Chadwick, Gaurav Singh Tomar, Xavier Garcia, Evan Senter, Emanuel Taropa, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adri\u00e0 Puigdom\u00e8nech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Yujing Zhang, Ravi Addanki, Antoine Miech, Annie Louis, Laurent El Shafey, Denis Teplyashin, Geoff Brown, Elliot Catt, Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, S\u00e9bastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozi\u0144ska, Vitaly Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Gim\u00e9nez, Legg Yeung, Hanzhao Lin, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo-yiin Chang, Paul Komarek, Ross McIlroy, Mario Lu\u010di\u0107, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yong Cheng, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Rapha\u00ebl Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sj\u00f6sund, S\u00e9bastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, L\u00e9onard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adri\u00e0 Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, V\u00edctor Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, \u00c7a\u011flar \u00dcnl\u00fc, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, YaGuang Li, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Raki\u0107evi\u0107, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Gamaleldin Elsayed, Ed Chi, Mahdis Mahdieh, Ian Tenney, Nan Hua, Ivan Petrychenko, Patrick Kane, Dylan Scandinaro, Rishub Jain, Jonathan Uesato, Romina Datta, Adam Sadovsky, Oskar Bunyan, Dominik Rabiej, Shimu Wu, John Zhang, Gautam Vasudevan, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Betty Chan, Pam G Rabinovitch, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Sahitya Potluri, Jane Park, Elnaz Davoodi, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Chris Gorgolewski, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Paul Suganthan, Evan Palmer, Geoffrey Irving, Edward Loper, Manaal Faruqui, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Michael Fink, Alfonso Casta\u00f1o, Irene Giannoumis, Wooyeol Kim, Miko\u0142aj Rybi\u0144ski, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marin Georgiev, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie Lui, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Lam Nguyen Thiet, Daniel Andor, Pedro Valenzuela, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Sarmishta Velury, Sebastian Krause, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Tejasi Latkar, Mingyang Zhang, Quoc Le, Elena Allica Abellan, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Sid Lall, Ken Franko, Egor Filonov, Anna Bulanova, R\u00e9mi Leblond, Vikas Yadav, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Hao Zhou, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Jeremiah Liu, Mark Omernick, Colton Bishop, Chintu Kumar, Rachel Sterneck, Ryan Foley, Rohan Jain, Swaroop Mishra, Jiawei Xia, Taylor Bos, Geoffrey Cideron, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Petru Gurita, Hila Noga, Premal Shah, Daniel J. Mankowitz, Alex Polozov, Nate Kushman, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Anhad Mohananey, Matthieu Geist, Sidharth Mudgal, Sertan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Quan Yuan, Sumit Bagri, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Aliaksei Severyn, Jonathan Lai, Kathy Wu, Heng-Tze Cheng, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Mark Geller, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Andrei Sozanschi, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Abhimanyu Goyal, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Sabaer Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Tao Zhu, Yeongil Ko, Laura Knight, Am\u00e9lie H\u00e9liou, Ning Niu, Shane Gu, Chenxi Pang, Dustin Tran, Yeqing Li, Nir Levine, Ariel Stolovich, Norbert Kalb, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Balaji Lakshminarayanan, Charlie Deck, Shyam Upadhyay, Hyo Lee, Mike Dusenberry, Zonglin Li, Xuezhi Wang, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Summer Yue, Sho Arora, Eric Malmi, Daniil Mirylenka, Qijun Tan, Christy Koh, Soheil Hassas Yeganeh, Siim P\u00f5der, Steven Zheng, Francesco Pongetti, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Ragha Kotikalapudi, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Chenkai Kuang, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Pei Sun, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Ishita Dasgupta, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivi\u00e8re, Alanna Walton, Cl\u00e9ment Crepy, Alicia Parrish, Yuan Liu, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Pluci\u0144ska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Ivo Penchev, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Adam Kurzrok, Lynette Webb, Sahil Dua, Dong Li, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Taylan Bilal, Evgenii Eltyshev, Daniel Balle, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Adams Yu, Christof Angermueller, Xiaowei Li, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Kevin Brooks, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, Ginger Perng, Blake Hechtman, Parker Schuh, Milad Nasr, Mia Chen, Kieran Milan, Vladimir Mikulik, Trevor Strohman, Juliana Franco, Tim Green, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, Oriol Vinyals", "abstract": "  This report introduces a new family of multimodal models, Gemini, that\nexhibit remarkable capabilities across image, audio, video, and text\nunderstanding. The Gemini family consists of Ultra, Pro, and Nano sizes,\nsuitable for applications ranging from complex reasoning tasks to on-device\nmemory-constrained use-cases. Evaluation on a broad range of benchmarks shows\nthat our most-capable Gemini Ultra model advances the state of the art in 30 of\n32 of these benchmarks - notably being the first model to achieve human-expert\nperformance on the well-studied exam benchmark MMLU, and improving the state of\nthe art in every one of the 20 multimodal benchmarks we examined. We believe\nthat the new capabilities of Gemini models in cross-modal reasoning and\nlanguage understanding will enable a wide variety of use cases and we discuss\nour approach toward deploying them responsibly to users.\n", "published": "19-12-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "General-Purpose Question-Answering with Macaw", "url": "http://arxiv.org/abs/2109.02593v1", "authors": "Oyvind Tafjord, Peter Clark", "abstract": "  Despite the successes of pretrained language models, there are still few\nhigh-quality, general-purpose QA systems that are freely available. In\nresponse, we present Macaw, a versatile, generative question-answering (QA)\nsystem that we are making available to the community. Macaw is built on\nUnifiedQA, itself built on T5, and exhibits strong performance, zero-shot, on a\nwide variety of topics, including outperforming GPT-3 by over 10% (absolute) on\nChallenge300, a suite of 300 challenge questions, despite being an order of\nmagnitude smaller (11 billion vs. 175 billion parameters). In addition, Macaw\nallows different permutations (\"angles\") of its inputs and outputs to be used,\nfor example Macaw can take a question and produce an answer; or take an answer\nand produce a question; or take an answer and question, and produce\nmultiple-choice options. We describe the system, and illustrate a variety of\nquestion types where it produces surprisingly good answers, well outside the\ntraining setup. We also identify question classes where it still appears to\nstruggle, offering insights into the limitations of pretrained language models.\nMacaw is freely available, and we hope that it proves useful to the community.\nMacaw is available at https://github.com/allenai/macaw\n", "published": "06-09-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "General-purpose foundation models for increased autonomy in\n  robot-assisted surgery", "url": "http://arxiv.org/abs/2401.00678v1", "authors": "Samuel Schmidgall, Ji Woong Kim, Alan Kuntz, Ahmed Ezzat Ghazi, Axel Krieger", "abstract": "  The dominant paradigm for end-to-end robot learning focuses on optimizing\ntask-specific objectives that solve a single robotic problem such as picking up\nan object or reaching a target position. However, recent work on high-capacity\nmodels in robotics has shown promise toward being trained on large collections\nof diverse and task-agnostic datasets of video demonstrations. These models\nhave shown impressive levels of generalization to unseen circumstances,\nespecially as the amount of data and the model complexity scale. Surgical robot\nsystems that learn from data have struggled to advance as quickly as other\nfields of robot learning for a few reasons: (1) there is a lack of existing\nlarge-scale open-source data to train models, (2) it is challenging to model\nthe soft-body deformations that these robots work with during surgery because\nsimulation cannot match the physical and visual complexity of biological\ntissue, and (3) surgical robots risk harming patients when tested in clinical\ntrials and require more extensive safety measures. This perspective article\naims to provide a path toward increasing robot autonomy in robot-assisted\nsurgery through the development of a multi-modal, multi-task,\nvision-language-action model for surgical robots. Ultimately, we argue that\nsurgical robots are uniquely positioned to benefit from general-purpose models\nand provide three guiding actions toward increased autonomy in robot-assisted\nsurgery.\n", "published": "01-01-2024", "year": "2024", "categories": ["Machine Learning"]}, {"title": "Generalized products and Lorentzian length spaces", "url": "http://arxiv.org/abs/2311.10691v1", "authors": "Elefterios Soultanis", "abstract": "  We construct a Lorentzian length space with an orthogonal splitting on a\nproduct $I\\times X$ of an interval and a metric space, and use this framework\nto consider the relationship between metric and causal geometry, as well as\nsynthetic time-like Ricci curvature bounds.\n  The generalized Lorentzian product naturally has a Lorentzian length\nstructure but can fail the push-up condition in general. We recover the push-up\nproperty under a log-Lipschitz condition on the time variable and establish\nsufficient conditions for global hyperbolicity. Moreover we formulate time-like\nRicci curvature bounds without push-up and regularity assumptions, and obtain a\npartial rigidity of the splitting under a strong energy condition.\n", "published": "17-11-2023", "year": "2023", "categories": []}, {"title": "Generated Knowledge Prompting for Commonsense Reasoning", "url": "http://arxiv.org/abs/2110.08387v3", "authors": "Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, Hannaneh Hajishirzi", "abstract": "  It remains an open question whether incorporating external knowledge benefits\ncommonsense reasoning while maintaining the flexibility of pretrained sequence\nmodels. To investigate this question, we develop generated knowledge prompting,\nwhich consists of generating knowledge from a language model, then providing\nthe knowledge as additional input when answering a question. Our method does\nnot require task-specific supervision for knowledge integration, or access to a\nstructured knowledge base, yet it improves performance of large-scale,\nstate-of-the-art models on four commonsense reasoning tasks, achieving\nstate-of-the-art results on numerical commonsense (NumerSense), general\ncommonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks.\nGenerated knowledge prompting highlights large-scale language models as\nflexible sources of external knowledge for improving commonsense reasoning. Our\ncode is available at https://github.com/liujch1998/GKP\n", "published": "15-10-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "Generating Datasets with Pretrained Language Models", "url": "http://arxiv.org/abs/2104.07540v3", "authors": "Timo Schick, Hinrich Sch\u00fctze", "abstract": "  To obtain high-quality sentence embeddings from pretrained language models\n(PLMs), they must either be augmented with additional pretraining objectives or\nfinetuned on a large set of labeled text pairs. While the latter approach\ntypically outperforms the former, it requires great human effort to generate\nsuitable datasets of sufficient size. In this paper, we show how PLMs can be\nleveraged to obtain high-quality sentence embeddings without the need for\nlabeled data, finetuning or modifications to the pretraining objective: We\nutilize the generative abilities of large and high-performing PLMs to generate\nentire datasets of labeled text pairs from scratch, which we then use for\nfinetuning much smaller and more efficient models. Our fully unsupervised\napproach outperforms strong baselines on several semantic textual similarity\ndatasets.\n", "published": "15-04-2021", "year": "2021", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Generating Long Sequences with Sparse Transformers", "url": "http://arxiv.org/abs/1904.10509v1", "authors": "Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever", "abstract": "  Transformers are powerful sequence models, but require time and memory that\ngrows quadratically with the sequence length. In this paper we introduce sparse\nfactorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We\nalso introduce a) a variation on architecture and initialization to train\ndeeper networks, b) the recomputation of attention matrices to save memory, and\nc) fast attention kernels for training. We call networks with these changes\nSparse Transformers, and show they can model sequences tens of thousands of\ntimesteps long using hundreds of layers. We use the same architecture to model\nimages, audio, and text from raw bytes, setting a new state of the art for\ndensity modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate\nunconditional samples that demonstrate global coherence and great diversity,\nand show it is possible in principle to use self-attention to model sequences\nof length one million or more.\n", "published": "23-04-2019", "year": "2019", "categories": ["Machine Learning"]}, {"title": "Generative Agents: Interactive Simulacra of Human Behavior", "url": "http://arxiv.org/abs/2304.03442v2", "authors": "Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, Michael S. Bernstein", "abstract": "  Believable proxies of human behavior can empower interactive applications\nranging from immersive environments to rehearsal spaces for interpersonal\ncommunication to prototyping tools. In this paper, we introduce generative\nagents--computational software agents that simulate believable human behavior.\nGenerative agents wake up, cook breakfast, and head to work; artists paint,\nwhile authors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan the next\nday. To enable generative agents, we describe an architecture that extends a\nlarge language model to store a complete record of the agent's experiences\nusing natural language, synthesize those memories over time into higher-level\nreflections, and retrieve them dynamically to plan behavior. We instantiate\ngenerative agents to populate an interactive sandbox environment inspired by\nThe Sims, where end users can interact with a small town of twenty five agents\nusing natural language. In an evaluation, these generative agents produce\nbelievable individual and emergent social behaviors: for example, starting with\nonly a single user-specified notion that one agent wants to throw a Valentine's\nDay party, the agents autonomously spread invitations to the party over the\nnext two days, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right time. We\ndemonstrate through ablation that the components of our agent\narchitecture--observation, planning, and reflection--each contribute critically\nto the believability of agent behavior. By fusing large language models with\ncomputational, interactive agents, this work introduces architectural and\ninteraction patterns for enabling believable simulations of human behavior.\n", "published": "07-04-2023", "year": "2023", "categories": ["Artificial Intelligence", "Machine Learning"]}, {"title": "Generative Multimodal Models are In-Context Learners", "url": "http://arxiv.org/abs/2312.13286v1", "authors": "Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, Xinlong Wang", "abstract": "  The human ability to easily solve multimodal tasks in context (i.e., with\nonly a few demonstrations or simple instructions), is what current multimodal\nsystems have largely struggled to imitate. In this work, we demonstrate that\nthe task-agnostic in-context learning capabilities of large multimodal models\ncan be significantly enhanced by effective scaling-up. We introduce Emu2, a\ngenerative multimodal model with 37 billion parameters, trained on large-scale\nmultimodal sequences with a unified autoregressive objective. Emu2 exhibits\nstrong multimodal in-context learning abilities, even emerging to solve tasks\nthat require on-the-fly reasoning, such as visual prompting and object-grounded\ngeneration. The model sets a new record on multiple multimodal understanding\ntasks in few-shot settings. When instruction-tuned to follow specific\ninstructions, Emu2 further achieves new state-of-the-art on challenging tasks\nsuch as question answering benchmarks for large multimodal models and\nopen-ended subject-driven generation. These achievements demonstrate that Emu2\ncan serve as a base model and general-purpose interface for a wide range of\nmultimodal tasks. Code and models are publicly available to facilitate future\nresearch.\n", "published": "20-12-2023", "year": "2023", "categories": []}, {"title": "GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural\n  Networks", "url": "http://arxiv.org/abs/2302.08043v3", "authors": "Zemin Liu, Xingtong Yu, Yuan Fang, Xinming Zhang", "abstract": "  Graphs can model complex relationships between objects, enabling a myriad of\nWeb applications such as online page/article classification and social\nrecommendation. While graph neural networks(GNNs) have emerged as a powerful\ntool for graph representation learning, in an end-to-end supervised setting,\ntheir performance heavily rely on a large amount of task-specific supervision.\nTo reduce labeling requirement, the \"pre-train, fine-tune\" and \"pre-train,\nprompt\" paradigms have become increasingly common. In particular, prompting is\na popular alternative to fine-tuning in natural language processing, which is\ndesigned to narrow the gap between pre-training and downstream objectives in a\ntask-specific manner. However, existing study of prompting on graphs is still\nlimited, lacking a universal treatment to appeal to different downstream tasks.\nIn this paper, we propose GraphPrompt, a novel pre-training and prompting\nframework on graphs. GraphPrompt not only unifies pre-training and downstream\ntasks into a common task template, but also employs a learnable prompt to\nassist a downstream task in locating the most relevant knowledge from the\npre-train model in a task-specific manner. Finally, we conduct extensive\nexperiments on five public datasets to evaluate and analyze GraphPrompt.\n", "published": "16-02-2023", "year": "2023", "categories": ["Machine Learning", "Computation and Language"]}, {"title": "Guiding Large Language Models via Directional Stimulus Prompting", "url": "http://arxiv.org/abs/2302.11520v4", "authors": "Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan", "abstract": "  We introduce Directional Stimulus Prompting, a novel framework for guiding\nblack-box large language models (LLMs) toward specific desired outputs. Instead\nof directly adjusting LLMs, our method employs a small tunable policy model\n(e.g., T5) to generate an auxiliary directional stimulus prompt for each input\ninstance. These directional stimulus prompts act as nuanced, instance-specific\nhints and clues to guide LLMs in generating desired outcomes, such as including\nspecific keywords in the generated summary. Our approach sidesteps the\nchallenges of direct LLM tuning by optimizing the policy model to explore\ndirectional stimulus prompts that align LLMs with desired behaviors. The policy\nmodel can be optimized through 1) supervised fine-tuning using labeled data and\n2) reinforcement learning from offline or online rewards based on the LLM's\noutput. We assess our method across summarization, dialogue response\ngeneration, and chain-of-thought reasoning tasks. Our experiments demonstrate\nthat the framework consistently improves LLMs' (e.g., ChatGPT, Codex,\nInstructGPT) performance on these supervised tasks using minimal labeled data.\nNotably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances\nChatGPT's performance by an impressive 41.4%, matching or surpassing some fully\nsupervised start-of-the-art models. Additionally, the instance-specific\nchain-of-thought prompt generated by our approach improves InstructGPT's\nreasoning accuracy compared to human-crafted or automatically generated\nprompts. The code and data are publicly available at\n\\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.\n", "published": "22-02-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Guiding Pretraining in Reinforcement Learning with Large Language Models", "url": "http://arxiv.org/abs/2302.06692v2", "authors": "Yuqing Du, Olivia Watkins, Zihan Wang, C\u00e9dric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, Jacob Andreas", "abstract": "  Reinforcement learning algorithms typically struggle in the absence of a\ndense, well-shaped reward function. Intrinsically motivated exploration methods\naddress this limitation by rewarding agents for visiting novel states or\ntransitions, but these methods offer limited benefits in large environments\nwhere most discovered novelty is irrelevant for downstream tasks. We describe a\nmethod that uses background knowledge from text corpora to shape exploration.\nThis method, called ELLM (Exploring with LLMs) rewards an agent for achieving\ngoals suggested by a language model prompted with a description of the agent's\ncurrent state. By leveraging large-scale language model pretraining, ELLM\nguides agents toward human-meaningful and plausibly useful behaviors without\nrequiring a human in the loop. We evaluate ELLM in the Crafter game environment\nand the Housekeep robotic simulator, showing that ELLM-trained agents have\nbetter coverage of common-sense behaviors during pretraining and usually match\nor improve performance on a range of downstream tasks. Code available at\nhttps://github.com/yuqingd/ellm.\n", "published": "13-02-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond", "url": "http://arxiv.org/abs/2304.13712v2", "authors": "Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu", "abstract": "  This paper presents a comprehensive and practical guide for practitioners and\nend-users working with Large Language Models (LLMs) in their downstream natural\nlanguage processing (NLP) tasks. We provide discussions and insights into the\nusage of LLMs from the perspectives of models, data, and downstream tasks.\nFirstly, we offer an introduction and brief summary of current GPT- and\nBERT-style LLMs. Then, we discuss the influence of pre-training data, training\ndata, and test data. Most importantly, we provide a detailed discussion about\nthe use and non-use cases of large language models for various natural language\nprocessing tasks, such as knowledge-intensive tasks, traditional natural\nlanguage understanding tasks, natural language generation tasks, emergent\nabilities, and considerations for specific tasks.We present various use cases\nand non-use cases to illustrate the practical applications and limitations of\nLLMs in real-world scenarios. We also try to understand the importance of data\nand the specific challenges associated with each NLP task. Furthermore, we\nexplore the impact of spurious biases on LLMs and delve into other essential\nconsiderations, such as efficiency, cost, and latency, to ensure a\ncomprehensive understanding of deploying LLMs in practice. This comprehensive\nguide aims to provide researchers and practitioners with valuable insights and\nbest practices for working with LLMs, thereby enabling the successful\nimplementation of these models in a wide range of NLP tasks. A curated list of\npractical guide resources of LLMs, regularly updated, can be found at\n\\url{https://github.com/Mooler0410/LLMsPracticalGuide}.\n", "published": "26-04-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "How Can We Know What Language Models Know?", "url": "http://arxiv.org/abs/1911.12543v2", "authors": "Zhengbao Jiang, Frank F. Xu, Jun Araki, Graham Neubig", "abstract": "  Recent work has presented intriguing results examining the knowledge\ncontained in language models (LM) by having the LM fill in the blanks of\nprompts such as \"Obama is a _ by profession\". These prompts are usually\nmanually created, and quite possibly sub-optimal; another prompt such as \"Obama\nworked as a _\" may result in more accurately predicting the correct profession.\nBecause of this, given an inappropriate prompt, we might fail to retrieve facts\nthat the LM does know, and thus any given prompt only provides a lower bound\nestimate of the knowledge contained in an LM. In this paper, we attempt to more\naccurately estimate the knowledge contained in LMs by automatically discovering\nbetter prompts to use in this querying process. Specifically, we propose\nmining-based and paraphrasing-based methods to automatically generate\nhigh-quality and diverse prompts, as well as ensemble methods to combine\nanswers from different prompts. Extensive experiments on the LAMA benchmark for\nextracting relational knowledge from LMs demonstrate that our methods can\nimprove accuracy from 31.1% to 39.6%, providing a tighter lower bound on what\nLMs know. We have released the code and the resulting LM Prompt And Query\nArchive (LPAQA) at https://github.com/jzbjyb/LPAQA.\n", "published": "28-11-2019", "year": "2019", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "How FaR Are Large Language Models From Agents with Theory-of-Mind?", "url": "http://arxiv.org/abs/2310.03051v1", "authors": "Pei Zhou, Aman Madaan, Srividya Pranavi Potharaju, Aditya Gupta, Kevin R. McKee, Ari Holtzman, Jay Pujara, Xiang Ren, Swaroop Mishra, Aida Nematzadeh, Shyam Upadhyay, Manaal Faruqui", "abstract": "  \"Thinking is for Doing.\" Humans can infer other people's mental states from\nobservations--an ability called Theory-of-Mind (ToM)--and subsequently act\npragmatically on those inferences. Existing question answering benchmarks such\nas ToMi ask models questions to make inferences about beliefs of characters in\na story, but do not test whether models can then use these inferences to guide\ntheir actions. We propose a new evaluation paradigm for large language models\n(LLMs): Thinking for Doing (T4D), which requires models to connect inferences\nabout others' mental states to actions in social scenarios. Experiments on T4D\ndemonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking\ncharacters' beliefs in stories, but they struggle to translate this capability\ninto strategic action. Our analysis reveals the core challenge for LLMs lies in\nidentifying the implicit inferences about mental states without being\nexplicitly asked about as in ToMi, that lead to choosing the correct action in\nT4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee\nand Reflect (FaR), which provides a reasoning structure that encourages LLMs to\nanticipate future challenges and reason about potential actions. FaR boosts\nGPT-4's performance from 50% to 71% on T4D, outperforming other prompting\nmethods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to\ndiverse out-of-distribution story structures and scenarios that also require\nToM inferences to choose an action, consistently outperforming other methods\nincluding few-shot in-context learning.\n", "published": "04-10-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to\n  Challenge AI Safety by Humanizing LLMs", "url": "http://arxiv.org/abs/2401.06373v1", "authors": "Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi", "abstract": "  Most traditional AI safety research has approached AI models as machines and\ncentered on algorithm-focused attacks developed by security experts. As large\nlanguage models (LLMs) become increasingly common and competent, non-expert\nusers can also impose risks during daily interactions. This paper introduces a\nnew perspective to jailbreak LLMs as human-like communicators, to explore this\noverlooked intersection between everyday language interaction and AI safety.\nSpecifically, we study how to persuade LLMs to jailbreak them. First, we\npropose a persuasion taxonomy derived from decades of social science research.\nThen, we apply the taxonomy to automatically generate interpretable persuasive\nadversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion\nsignificantly increases the jailbreak performance across all risk categories:\nPAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b\nChat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused\nattacks. On the defense side, we explore various mechanisms against PAP and,\nfound a significant gap in existing defenses, and advocate for more fundamental\nmitigation for highly interactive LLMs\n", "published": "12-01-2024", "year": "2024", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "How Many Data Points is a Prompt Worth?", "url": "http://arxiv.org/abs/2103.08493v2", "authors": "Teven Le Scao, Alexander M. Rush", "abstract": "  When fine-tuning pretrained models for classification, researchers either use\na generic model head or a task-specific prompt for prediction. Proponents of\nprompting have argued that prompts provide a method for injecting task-specific\nguidance, which is beneficial in low-data regimes. We aim to quantify this\nbenefit through rigorous testing of prompts in a fair setting: comparing\nprompted and head-based fine-tuning in equal conditions across many tasks and\ndata sizes. By controlling for many sources of advantage, we find that\nprompting does indeed provide a benefit, and that this benefit can be\nquantified per task. Results show that prompting is often worth 100s of data\npoints on average across classification tasks.\n", "published": "15-03-2021", "year": "2021", "categories": ["Machine Learning"]}, {"title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging\n  Face", "url": "http://arxiv.org/abs/2303.17580v4", "authors": "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang", "abstract": "  Solving complicated AI tasks with different domains and modalities is a key\nstep toward artificial general intelligence. While there are numerous AI models\navailable for various domains and modalities, they cannot handle complicated AI\ntasks autonomously. Considering large language models (LLMs) have exhibited\nexceptional abilities in language understanding, generation, interaction, and\nreasoning, we advocate that LLMs could act as a controller to manage existing\nAI models to solve complicated AI tasks, with language serving as a generic\ninterface to empower this. Based on this philosophy, we present HuggingGPT, an\nLLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI\nmodels in machine learning communities (e.g., Hugging Face) to solve AI tasks.\nSpecifically, we use ChatGPT to conduct task planning when receiving a user\nrequest, select models according to their function descriptions available in\nHugging Face, execute each subtask with the selected AI model, and summarize\nthe response according to the execution results. By leveraging the strong\nlanguage capability of ChatGPT and abundant AI models in Hugging Face,\nHuggingGPT can tackle a wide range of sophisticated AI tasks spanning different\nmodalities and domains and achieve impressive results in language, vision,\nspeech, and other challenging tasks, which paves a new way towards the\nrealization of artificial general intelligence.\n", "published": "30-03-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code\n  Empowers Large Language Models to Serve as Intelligent Agents", "url": "http://arxiv.org/abs/2401.00812v2", "authors": "Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, Chengxiang Zhai", "abstract": "  The prominent large language models (LLMs) of today differ from past language\nmodels not only in size, but also in the fact that they are trained on a\ncombination of natural language and formal language (code). As a medium between\nhumans and computers, code translates high-level goals into executable steps,\nfeaturing standard syntax, logical consistency, abstraction, and modularity. In\nthis survey, we present an overview of the various benefits of integrating code\ninto LLMs' training data. Specifically, beyond enhancing LLMs in code\ngeneration, we observe that these unique properties of code help (i) unlock the\nreasoning ability of LLMs, enabling their applications to a range of more\ncomplex natural language tasks; (ii) steer LLMs to produce structured and\nprecise intermediate steps, which can then be connected to external execution\nends through function calls; and (iii) take advantage of code compilation and\nexecution environment, which also provides diverse feedback for model\nimprovement. In addition, we trace how these profound capabilities of LLMs,\nbrought by code, have led to their emergence as intelligent agents (IAs) in\nsituations where the ability to understand instructions, decompose goals, plan\nand execute actions, and refine from feedback are crucial to their success on\ndownstream tasks. Finally, we present several key challenges and future\ndirections of empowering LLMs with code.\n", "published": "01-01-2024", "year": "2024", "categories": ["Computation and Language"]}, {"title": "Igniting Language Intelligence: The Hitchhiker's Guide From\n  Chain-of-Thought Reasoning to Language Agents", "url": "http://arxiv.org/abs/2311.11797v1", "authors": "Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma, Zhiwei He, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, Hai Zhao", "abstract": "  Large language models (LLMs) have dramatically enhanced the field of language\nintelligence, as demonstrably evidenced by their formidable empirical\nperformance across a spectrum of complex reasoning tasks. Additionally,\ntheoretical proofs have illuminated their emergent reasoning capabilities,\nproviding a compelling showcase of their advanced cognitive abilities in\nlinguistic contexts. Critical to their remarkable efficacy in handling complex\nreasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning\ntechniques, obliging them to formulate intermediate steps en route to deriving\nan answer. The CoT reasoning approach has not only exhibited proficiency in\namplifying reasoning performance but also in enhancing interpretability,\ncontrollability, and flexibility. In light of these merits, recent research\nendeavors have extended CoT reasoning methodologies to nurture the development\nof autonomous language agents, which adeptly adhere to language instructions\nand execute actions within varied environments. This survey paper orchestrates\na thorough discourse, penetrating vital research dimensions, encompassing: (i)\nthe foundational mechanics of CoT techniques, with a focus on elucidating the\ncircumstances and justification behind its efficacy; (ii) the paradigm shift in\nCoT; and (iii) the burgeoning of language agents fortified by CoT approaches.\nProspective research avenues envelop explorations into generalization,\nefficiency, customization, scaling, and safety. This paper caters to a wide\naudience, including beginners seeking comprehensive knowledge of CoT reasoning\nand language agents, as well as experienced researchers interested in\nfoundational mechanics and engaging in cutting-edge discussions on these\ntopics. A repository for the related papers is available at\nhttps://github.com/Zoeyyao27/CoT-Igniting-Agent.\n", "published": "20-11-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Ignore Previous Prompt: Attack Techniques For Language Models", "url": "http://arxiv.org/abs/2211.09527v1", "authors": "F\u00e1bio Perez, Ian Ribeiro", "abstract": "  Transformer-based large language models (LLMs) provide a powerful foundation\nfor natural language tasks in large-scale customer-facing applications.\nHowever, studies that explore their vulnerabilities emerging from malicious\nuser interaction are scarce. By proposing PromptInject, a prosaic alignment\nframework for mask-based iterative adversarial prompt composition, we examine\nhow GPT-3, the most widely deployed language model in production, can be easily\nmisaligned by simple handcrafted inputs. In particular, we investigate two\ntypes of attacks -- goal hijacking and prompt leaking -- and demonstrate that\neven low-aptitude, but sufficiently ill-intentioned agents, can easily exploit\nGPT-3's stochastic nature, creating long-tail risks. The code for PromptInject\nis available at https://github.com/agencyenterprise/PromptInject.\n", "published": "17-11-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Improving Text Embeddings with Large Language Models", "url": "http://arxiv.org/abs/2401.00368v1", "authors": "Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei", "abstract": "  In this paper, we introduce a novel and simple method for obtaining\nhigh-quality text embeddings using only synthetic data and less than 1k\ntraining steps. Unlike existing methods that often depend on multi-stage\nintermediate pre-training with billions of weakly-supervised text pairs,\nfollowed by fine-tuning with a few labeled datasets, our method does not\nrequire building complex training pipelines or relying on manually collected\ndatasets that are often constrained by task diversity and language coverage. We\nleverage proprietary LLMs to generate diverse synthetic data for hundreds of\nthousands of text embedding tasks across nearly 100 languages. We then\nfine-tune open-source decoder-only LLMs on the synthetic data using standard\ncontrastive loss. Experiments demonstrate that our method achieves strong\nperformance on highly competitive text embedding benchmarks without using any\nlabeled data. Furthermore, when fine-tuned with a mixture of synthetic and\nlabeled data, our model sets new state-of-the-art results on the BEIR and MTEB\nbenchmarks.\n", "published": "31-12-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Inferring Implicit Relations in Complex Questions with Language Models", "url": "http://arxiv.org/abs/2204.13778v2", "authors": "Uri Katz, Mor Geva, Jonathan Berant", "abstract": "  A prominent challenge for modern language understanding systems is the\nability to answer implicit reasoning questions, where the required reasoning\nsteps for answering the question are not mentioned in the text explicitly. In\nthis work, we investigate why current models struggle with implicit reasoning\nquestion answering (QA) tasks, by decoupling inference of reasoning steps from\ntheir execution. We define a new task of implicit relation inference and\nconstruct a benchmark, IMPLICITRELATIONS, where given a question, a model\nshould output a list of concept-relation pairs, where the relations describe\nthe implicit reasoning steps required for answering the question. Using\nIMPLICITRELATIONS, we evaluate models from the GPT-3 family and find that,\nwhile these models struggle on the implicit reasoning QA task, they often\nsucceed at inferring implicit relations. This suggests that the challenge in\nimplicit reasoning questions does not stem from the need to plan a reasoning\nstrategy alone, but to do it while also retrieving and reasoning over relevant\ninformation.\n", "published": "28-04-2022", "year": "2022", "categories": ["Computation and Language"]}, {"title": "Inner Monologue: Embodied Reasoning through Planning with Language\n  Models", "url": "http://arxiv.org/abs/2207.05608v1", "authors": "Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter", "abstract": "  Recent works have shown how the reasoning capabilities of Large Language\nModels (LLMs) can be applied to domains beyond natural language processing,\nsuch as planning and interaction for robots. These embodied problems require an\nagent to understand many semantic aspects of the world: the repertoire of\nskills available, how these skills influence the world, and how changes to the\nworld map back to the language. LLMs planning in embodied environments need to\nconsider not just what skills to do, but also how and when to do them - answers\nthat change over time in response to the agent's own choices. In this work, we\ninvestigate to what extent LLMs used in such embodied contexts can reason over\nsources of feedback provided through natural language, without any additional\ntraining. We propose that by leveraging environment feedback, LLMs are able to\nform an inner monologue that allows them to more richly process and plan in\nrobotic control scenarios. We investigate a variety of sources of feedback,\nsuch as success detection, scene description, and human interaction. We find\nthat closed-loop language feedback significantly improves high-level\ninstruction completion on three domains, including simulated and real table top\nrearrangement tasks and long-horizon mobile manipulation tasks in a kitchen\nenvironment in the real world.\n", "published": "12-07-2022", "year": "2022", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "Intention Analysis Prompting Makes Large Language Models A Good\n  Jailbreak Defender", "url": "http://arxiv.org/abs/2401.06561v1", "authors": "Yuqi Zhang, Liang Ding, Lefei Zhang, Dacheng Tao", "abstract": "  Aligning large language models (LLMs) with human values, particularly in the\nface of stealthy and complex jailbreaks, presents a formidable challenge. In\nthis study, we present a simple yet highly effective defense strategy, i.e.,\nIntention Analysis Prompting (IAPrompt). The principle behind is to trigger\nLLMs' inherent self-correct and improve ability through a two-stage process: 1)\nessential intention analysis, and 2) policy-aligned response. Notably, IAPrompt\nis an inference-only method, thus could enhance the safety of LLMs without\ncompromising their helpfulness. Extensive experiments on SAP200 and DAN\nbenchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that\nIAPrompt could consistently and significantly reduce the harmfulness in\nresponse (averagely -46.5% attack success rate) and maintain the general\nhelpfulness. Further analyses present some insights into how our method works.\nTo facilitate reproducibility, We release our code and scripts at:\nhttps://github.com/alphadl/SafeLLM_with_IntentionAnalysis\n", "published": "12-01-2024", "year": "2024", "categories": ["Computation and Language"]}, {"title": "Interactive Language: Talking to Robots in Real Time", "url": "http://arxiv.org/abs/2210.06407v1", "authors": "Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, Pete Florence", "abstract": "  We present a framework for building interactive, real-time, natural\nlanguage-instructable robots in the real world, and we open source related\nassets (dataset, environment, benchmark, and policies). Trained with behavioral\ncloning on a dataset of hundreds of thousands of language-annotated\ntrajectories, a produced policy can proficiently execute an order of magnitude\nmore commands than previous works: specifically we estimate a 93.5% success\nrate on a set of 87,000 unique natural language strings specifying raw\nend-to-end visuo-linguo-motor skills in the real world. We find that the same\npolicy is capable of being guided by a human via real-time language to address\na wide range of precise long-horizon rearrangement goals, e.g. \"make a smiley\nface out of blocks\". The dataset we release comprises nearly 600,000\nlanguage-labeled trajectories, an order of magnitude larger than prior\navailable datasets. We hope the demonstrated results and associated assets\nenable further advancement of helpful, capable, natural-language-interactable\nrobots. See videos at https://interactive-language.github.io.\n", "published": "12-10-2022", "year": "2022", "categories": ["Artificial Intelligence", "Machine Learning"]}, {"title": "Investigating Zero- and Few-shot Generalization in Fact Verification", "url": "http://arxiv.org/abs/2309.09444v1", "authors": "Liangming Pan, Yunxiang Zhang, Min-Yen Kan", "abstract": "  In this paper, we explore zero- and few-shot generalization for fact\nverification (FV), which aims to generalize the FV model trained on\nwell-resourced domains (e.g., Wikipedia) to low-resourced domains that lack\nhuman annotations. To this end, we first construct a benchmark dataset\ncollection which contains 11 FV datasets representing 6 domains. We conduct an\nempirical analysis of generalization across these FV datasets, finding that\ncurrent models generalize poorly. Our analysis reveals that several factors\naffect generalization, including dataset size, length of evidence, and the type\nof claims. Finally, we show that two directions of work improve generalization:\n1) incorporating domain knowledge via pretraining on specialized domains, and\n2) automatically generating training data via claim generation.\n", "published": "18-09-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot\n  Learners", "url": "http://arxiv.org/abs/2009.07118v2", "authors": "Timo Schick, Hinrich Sch\u00fctze", "abstract": "  When scaled to hundreds of billions of parameters, pretrained language models\nsuch as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance.\nHowever, enormous amounts of compute are required for training and applying\nsuch big models, resulting in a large carbon footprint and making it difficult\nfor researchers and practitioners to use them. We show that performance similar\nto GPT-3 can be obtained with language models that are much \"greener\" in that\ntheir parameter count is several orders of magnitude smaller. This is achieved\nby converting textual inputs into cloze questions that contain a task\ndescription, combined with gradient-based optimization; exploiting unlabeled\ndata gives further improvements. We identify key factors required for\nsuccessful natural language understanding with small language models.\n", "published": "15-09-2020", "year": "2020", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study", "url": "http://arxiv.org/abs/2305.13860v1", "authors": "Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, Yang Liu", "abstract": "  Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential\nbut also introduce challenges related to content constraints and potential\nmisuse. Our study investigates three key research questions: (1) the number of\ndifferent prompt types that can jailbreak LLMs, (2) the effectiveness of\njailbreak prompts in circumventing LLM constraints, and (3) the resilience of\nChatGPT against these jailbreak prompts. Initially, we develop a classification\nmodel to analyze the distribution of existing prompts, identifying ten distinct\npatterns and three categories of jailbreak prompts. Subsequently, we assess the\njailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a\ndataset of 3,120 jailbreak questions across eight prohibited scenarios.\nFinally, we evaluate the resistance of ChatGPT against jailbreak prompts,\nfinding that the prompts can consistently evade the restrictions in 40 use-case\nscenarios. The study underscores the importance of prompt structures in\njailbreaking LLMs and discusses the challenges of robust jailbreak prompt\ngeneration and prevention.\n", "published": "23-05-2023", "year": "2023", "categories": ["Artificial Intelligence", "Computation and Language"]}, {"title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges", "url": "http://arxiv.org/abs/2310.17631v1", "authors": "Lianghui Zhu, Xinggang Wang, Xinlong Wang", "abstract": "  Evaluating Large Language Models (LLMs) in open-ended scenarios is\nchallenging because existing benchmarks and metrics can not measure them\ncomprehensively. To address this problem, we propose to fine-tune LLMs as\nscalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in\nopen-ended benchmarks. We first propose a comprehensive, large-scale,\nhigh-quality dataset containing task seeds, LLMs-generated answers, and\nGPT-4-generated judgments for fine-tuning high-performance judges, as well as a\nnew benchmark for evaluating the judges. We train JudgeLM at different scales\nfrom 7B, 13B, to 33B parameters, and conduct a systematic analysis of its\ncapabilities and behaviors. We then analyze the key biases in fine-tuning LLM\nas a judge and consider them as position bias, knowledge bias, and format bias.\nTo address these issues, JudgeLM introduces a bag of techniques including swap\naugmentation, reference support, and reference drop, which clearly enhance the\njudge's performance. JudgeLM obtains the state-of-the-art judge performance on\nboth the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM\nis efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8\nA100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an\nagreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM\nalso demonstrates extended capabilities in being judges of the single answer,\nmultimodal models, multiple answers, and multi-turn chat.\n", "published": "26-10-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt\n  Verbalizer for Text Classification", "url": "http://arxiv.org/abs/2108.02035v2", "authors": "Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, Maosong Sun", "abstract": "  Tuning pre-trained language models (PLMs) with task-specific prompts has been\na promising approach for text classification. Particularly, previous studies\nsuggest that prompt-tuning has remarkable superiority in the low-data scenario\nover the generic fine-tuning methods with extra classifiers. The core idea of\nprompt-tuning is to insert text pieces, i.e., template, to the input and\ntransform a classification problem into a masked language modeling problem,\nwhere a crucial step is to construct a projection, i.e., verbalizer, between a\nlabel space and a label word space. A verbalizer is usually handcrafted or\nsearched by gradient descent, which may lack coverage and bring considerable\nbias and high variances to the results. In this work, we focus on incorporating\nexternal knowledge into the verbalizer, forming a knowledgeable prompt-tuning\n(KPT), to improve and stabilize prompt-tuning. Specifically, we expand the\nlabel word space of the verbalizer using external knowledge bases (KBs) and\nrefine the expanded label word space with the PLM itself before predicting with\nthe expanded label word space. Extensive experiments on zero and few-shot text\nclassification tasks demonstrate the effectiveness of knowledgeable\nprompt-tuning.\n", "published": "04-08-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "KronA: Parameter Efficient Tuning with Kronecker Adapter", "url": "http://arxiv.org/abs/2212.10650v1", "authors": "Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J. Clark, Mehdi Rezagholizadeh", "abstract": "  Fine-tuning a Pre-trained Language Model (PLM) on a specific downstream task\nhas been a well-known paradigm in Natural Language Processing. However, with\nthe ever-growing size of PLMs, training the entire model on several downstream\ntasks becomes very expensive and resource-hungry. Recently, different Parameter\nEfficient Tuning (PET) techniques are proposed to improve the efficiency of\nfine-tuning PLMs. One popular category of PET methods is the low-rank\nadaptation methods which insert learnable truncated SVD modules into the\noriginal model either sequentially or in parallel. However, low-rank\ndecomposition suffers from limited representation power. In this work, we\naddress this problem using the Kronecker product instead of the low-rank\nrepresentation. We introduce KronA, a Kronecker product-based adapter module\nfor efficient fine-tuning of Transformer-based PLMs. We apply the proposed\nmethods for fine-tuning T5 on the GLUE benchmark to show that incorporating the\nKronecker-based modules can outperform state-of-the-art PET methods.\n", "published": "20-12-2022", "year": "2022", "categories": ["Computation and Language"]}, {"title": "KwaiAgents: Generalized Information-seeking Agent System with Large\n  Language Models", "url": "http://arxiv.org/abs/2312.04889v3", "authors": "Haojie Pan, Zepeng Zhai, Hao Yuan, Yaojia Lv, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin", "abstract": "  Driven by curiosity, humans have continually sought to explore and understand\nthe world around them, leading to the invention of various tools to satiate\nthis inquisitiveness. Despite not having the capacity to process and memorize\nvast amounts of information in their brains, humans excel in critical thinking,\nplanning, reflection, and harnessing available tools to interact with and\ninterpret the world, enabling them to find answers efficiently. The recent\nadvancements in large language models (LLMs) suggest that machines might also\npossess the aforementioned human-like capabilities, allowing them to exhibit\npowerful abilities even with a constrained parameter count. In this paper, we\nintroduce KwaiAgents, a generalized information-seeking agent system based on\nLLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its\ncognitive core, which is capable of understanding a user's query, behavior\nguidelines, and referencing external documents. The agent can also update and\nretrieve information from its internal memory, plan and execute actions using a\ntime-aware search-browse toolkit, and ultimately provide a comprehensive\nresponse. We further investigate the system's performance when powered by LLMs\nless advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,\ndesigned to ensure even an open-sourced 7B or 13B model performs well among\nmany agent systems. We exploit both benchmark and human evaluations to\nsystematically validate these capabilities. Extensive experiments show the\nsuperiority of our agent system compared to other autonomous agents and\nhighlight the enhanced generalized agent-abilities of our fine-tuned LLMs.\n", "published": "08-12-2023", "year": "2023", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "LARP: Language-Agent Role Play for Open-World Games", "url": "http://arxiv.org/abs/2312.17653v1", "authors": "Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, Ji Yan", "abstract": "  Language agents have shown impressive problem-solving skills within defined\nsettings and brief timelines. Yet, with the ever-evolving complexities of\nopen-world simulations, there's a pressing need for agents that can flexibly\nadapt to complex environments and consistently maintain a long-term memory to\nensure coherent actions. To bridge the gap between language agents and\nopen-world games, we introduce Language Agent for Role-Playing (LARP), which\nincludes a cognitive architecture that encompasses memory processing and a\ndecision-making assistant, an environment interaction module with a\nfeedback-driven learnable action space, and a postprocessing method that\npromotes the alignment of various personalities. The LARP framework refines\ninteractions between users and agents, predefined with unique backgrounds and\npersonalities, ultimately enhancing the gaming experience in open-world\ncontexts. Furthermore, it highlights the diverse uses of language models in a\nrange of areas such as entertainment, education, and various simulation\nscenarios. The project page is released at https://miao-ai-lab.github.io/LARP/.\n", "published": "24-12-2023", "year": "2023", "categories": ["Artificial Intelligence"]}, {"title": "LIMA: Less Is More for Alignment", "url": "http://arxiv.org/abs/2305.11206v1", "authors": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy", "abstract": "  Large language models are trained in two stages: (1) unsupervised pretraining\nfrom raw text, to learn general-purpose representations, and (2) large scale\ninstruction tuning and reinforcement learning, to better align to end tasks and\nuser preferences. We measure the relative importance of these two stages by\ntraining LIMA, a 65B parameter LLaMa language model fine-tuned with the\nstandard supervised loss on only 1,000 carefully curated prompts and responses,\nwithout any reinforcement learning or human preference modeling. LIMA\ndemonstrates remarkably strong performance, learning to follow specific\nresponse formats from only a handful of examples in the training data,\nincluding complex queries that range from planning trip itineraries to\nspeculating about alternate history. Moreover, the model tends to generalize\nwell to unseen tasks that did not appear in the training data. In a controlled\nhuman study, responses from LIMA are either equivalent or strictly preferred to\nGPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard\nand 65% versus DaVinci003, which was trained with human feedback. Taken\ntogether, these results strongly suggest that almost all knowledge in large\nlanguage models is learned during pretraining, and only limited instruction\ntuning data is necessary to teach models to produce high quality output.\n", "published": "18-05-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "LLM Augmented LLMs: Expanding Capabilities through Composition", "url": "http://arxiv.org/abs/2401.02412v1", "authors": "Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar", "abstract": "  Foundational models with billions of parameters which have been trained on\nlarge corpora of data have demonstrated non-trivial skills in a variety of\ndomains. However, due to their monolithic structure, it is challenging and\nexpensive to augment them or impart new skills. On the other hand, due to their\nadaptation abilities, several new instances of these models are being trained\ntowards new domains and tasks. In this work, we study the problem of efficient\nand practical composition of existing foundation models with more specific\nmodels to enable newer capabilities. To this end, we propose CALM --\nComposition to Augment Language Models -- which introduces cross-attention\nbetween models to compose their representations and enable new capabilities.\nSalient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\nexisting LLMs along with a few additional parameters and data, (ii) Existing\nmodel weights are kept intact, and hence preserves existing capabilities, and\n(iii) Applies to diverse domains and settings. We illustrate that augmenting\nPaLM2-S with a smaller model trained on low-resource languages results in an\nabsolute improvement of up to 13\\% on tasks like translation into English and\narithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\naugmented with a code-specific model, we see a relative improvement of 40\\%\nover the base model for code generation and explanation tasks -- on-par with\nfully fine-tuned counterparts.\n", "published": "04-01-2024", "year": "2024", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning", "url": "http://arxiv.org/abs/2401.01325v1", "authors": "Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, Xia Hu", "abstract": "  This work elicits LLMs' inherent ability to handle long contexts without\nfine-tuning. The limited length of the training sequence during training may\nlimit the application of Large Language Models (LLMs) on long input sequences\nfor inference. In this work, we argue that existing LLMs themselves have\ninherent capabilities for handling long contexts. Based on this argument, we\nsuggest extending LLMs' context window by themselves to fully utilize the\ninherent ability.We propose Self-Extend to stimulate LLMs' long context\nhandling potential. The basic idea is to construct bi-level attention\ninformation: the group level and the neighbor level. The two levels are\ncomputed by the original model's self-attention, which means the proposed does\nnot require any training. With only four lines of code modification, the\nproposed method can effortlessly extend existing LLMs' context window without\nany fine-tuning. We conduct comprehensive experiments and the results show that\nthe proposed method can effectively extend existing LLMs' context window's\nlength.\n", "published": "02-01-2024", "year": "2024", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "LLM in a flash: Efficient Large Language Model Inference with Limited\n  Memory", "url": "http://arxiv.org/abs/2312.11514v2", "authors": "Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C Del Mundo, Mohammad Rastegari, Mehrdad Farajtabar", "abstract": "  Large language models (LLMs) are central to modern natural language\nprocessing, delivering exceptional performance in various tasks. However, their\nsubstantial computational and memory requirements present challenges,\nespecially for devices with limited DRAM capacity. This paper tackles the\nchallenge of efficiently running LLMs that exceed the available DRAM capacity\nby storing the model parameters in flash memory, but bringing them on demand to\nDRAM. Our method involves constructing an inference cost model that takes into\naccount the characteristics of flash memory, guiding us to optimize in two\ncritical areas: reducing the volume of data transferred from flash and reading\ndata in larger, more contiguous chunks. Within this hardware-informed\nframework, we introduce two principal techniques. First, \"windowing\"\nstrategically reduces data transfer by reusing previously activated neurons,\nand second, \"row-column bundling\", tailored to the sequential data access\nstrengths of flash memory, increases the size of data chunks read from flash\nmemory. These methods collectively enable running models up to twice the size\nof the available DRAM, with a 4-5x and 20-25x increase in inference speed\ncompared to naive loading approaches in CPU and GPU, respectively. Our\nintegration of sparsity awareness, context-adaptive loading, and a\nhardware-oriented design paves the way for effective inference of LLMs on\ndevices with limited memory.\n", "published": "12-12-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large\n  Language Models", "url": "http://arxiv.org/abs/2212.04088v3", "authors": "Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, Yu Su", "abstract": "  This study focuses on using large language models (LLMs) as a planner for\nembodied agents that can follow natural language instructions to complete\ncomplex tasks in a visually-perceived environment. The high data cost and poor\nsample efficiency of existing methods hinders the development of versatile\nagents that are capable of many tasks and can learn new tasks quickly. In this\nwork, we propose a novel method, LLM-Planner, that harnesses the power of large\nlanguage models to do few-shot planning for embodied agents. We further propose\na simple but effective way to enhance LLMs with physical grounding to generate\nand update plans that are grounded in the current environment. Experiments on\nthe ALFRED dataset show that our method can achieve very competitive few-shot\nperformance: Despite using less than 0.5% of paired training data, LLM-Planner\nachieves competitive performance with recent baselines that are trained using\nthe full training data. Existing methods can barely complete any task\nsuccessfully under the same few-shot setting. Our work opens the door for\ndeveloping versatile and sample-efficient embodied agents that can quickly\nlearn many tasks. Website: https://dki-lab.github.io/LLM-Planner\n", "published": "08-12-2022", "year": "2022", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "LLM360: Towards Fully Transparent Open-Source LLMs", "url": "http://arxiv.org/abs/2312.06550v1", "authors": "Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, Eric P. Xing", "abstract": "  The recent surge in open-source Large Language Models (LLMs), such as LLaMA,\nFalcon, and Mistral, provides diverse options for AI practitioners and\nresearchers. However, most LLMs have only released partial artifacts, such as\nthe final model weights or inference code, and technical reports increasingly\nlimit their scope to high-level design choices and surface statistics. These\nchoices hinder progress in the field by degrading transparency into the\ntraining of LLMs and forcing teams to rediscover many details in the training\nprocess. We present LLM360, an initiative to fully open-source LLMs, which\nadvocates for all training code and data, model checkpoints, and intermediate\nresults to be made available to the community. The goal of LLM360 is to support\nopen and collaborative AI research by making the end-to-end LLM training\nprocess transparent and reproducible by everyone. As a first step of LLM360, we\nrelease two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder,\nincluding their training code, data, intermediate checkpoints, and analyses (at\nhttps://www.llm360.ai). We are committed to continually pushing the boundaries\nof LLMs through this open-source effort. More large-scale and stronger models\nare underway and will be released in the future.\n", "published": "11-12-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "LLM4Jobs: Unsupervised occupation extraction and standardization\n  leveraging Large Language Models", "url": "http://arxiv.org/abs/2309.09708v2", "authors": "Nan Li, Bo Kang, Tijl De Bie", "abstract": "  Automated occupation extraction and standardization from free-text job\npostings and resumes are crucial for applications like job recommendation and\nlabor market policy formation. This paper introduces LLM4Jobs, a novel\nunsupervised methodology that taps into the capabilities of large language\nmodels (LLMs) for occupation coding. LLM4Jobs uniquely harnesses both the\nnatural language understanding and generation capacities of LLMs. Evaluated on\nrigorous experimentation on synthetic and real-world datasets, we demonstrate\nthat LLM4Jobs consistently surpasses unsupervised state-of-the-art benchmarks,\ndemonstrating its versatility across diverse datasets and granularities. As a\nside result of our work, we present both synthetic and real-world datasets,\nwhich may be instrumental for subsequent research in this domain. Overall, this\ninvestigation highlights the promise of contemporary LLMs for the intricate\ntask of occupation extraction and standardization, laying the foundation for a\nrobust and adaptable framework relevant to both research and industrial\ncontexts.\n", "published": "18-09-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "LLMEval: A Preliminary Study on How to Evaluate Large Language Models", "url": "http://arxiv.org/abs/2312.07398v2", "authors": "Yue Zhang, Ming Zhang, Haipeng Yuan, Shichun Liu, Yongyao Shi, Tao Gui, Qi Zhang, Xuanjing Huang", "abstract": "  Recently, the evaluation of Large Language Models has emerged as a popular\narea of research. The three crucial questions for LLM evaluation are ``what,\nwhere, and how to evaluate''. However, the existing research mainly focuses on\nthe first two questions, which are basically what tasks to give the LLM during\ntesting and what kind of knowledge it should deal with. As for the third\nquestion, which is about what standards to use, the types of evaluators, how to\nscore, and how to rank, there hasn't been much discussion. In this paper, we\nanalyze evaluation methods by comparing various criteria with both manual and\nautomatic evaluation, utilizing onsite, crowd-sourcing, public annotators and\nGPT-4, with different scoring methods and ranking systems. We propose a new\ndataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186\nindividuals participated, leading to the generation of 243,337 manual\nannotations and 57,511 automatic evaluation results. We perform comparisons and\nanalyses of different settings and conduct 10 conclusions that can provide some\ninsights for evaluating LLM in the future. The dataset and the results are\npublicly available at https://github.com/llmeval .\n", "published": "12-12-2023", "year": "2023", "categories": ["Artificial Intelligence", "Computation and Language"]}, {"title": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer", "url": "http://arxiv.org/abs/2401.01055v2", "authors": "Jun Zhao, Zhihao Zhang, Luhui Gao, Qi Zhang, Tao Gui, Xuanjing Huang", "abstract": "  In recent times, substantial advancements have been witnessed in large\nlanguage models (LLMs), exemplified by ChatGPT, showcasing remarkable\nproficiency across a range of complex tasks. However, many mainstream LLMs\n(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their\nperformance in other non-English languages. In this paper, we focus on how to\neffectively transfer the capabilities of language generation and following\ninstructions to a non-English language. To answer this question, we conduct an\nextensive empirical investigation based on LLaMA, accumulating over 1440 GPU\nhours. We analyze the impact of key factors such as vocabulary extension,\nfurther pretraining, and instruction tuning on transfer. To accurately assess\nthe model's level of knowledge, we employ four widely used standardized testing\nbenchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a\ncomprehensive evaluation of the model's response quality is conducted,\nconsidering aspects such as accuracy, fluency, informativeness, logical\ncoherence, and harmlessness, based on LLM-Eval, a benchmarks consisting\ninstruction tasks from 17 diverse categories. Our evaluation results\ndemonstrate that comparable performance to state-of-the-art transfer models can\nbe achieved with less than 1% of the pretraining data, both in terms of\nknowledge alignment and response quality. Furthermore, the experimental\noutcomes across the thirteen low-resource languages also exhibit similar\ntrends. We anticipate that the conclusions revealed by the experiments will aid\nthe community in developing non-English LLMs.\n", "published": "02-01-2024", "year": "2024", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "LLaMA Pro: Progressive LLaMA with Block Expansion", "url": "http://arxiv.org/abs/2401.02415v1", "authors": "Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, Ying Shan", "abstract": "  Humans generally acquire new skills without compromising the old; however,\nthe opposite holds for Large Language Models (LLMs), e.g., from LLaMA to\nCodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with\nan expansion of Transformer blocks. We tune the expanded blocks using only new\ncorpus, efficiently and effectively improving the model's knowledge without\ncatastrophic forgetting. In this paper, we experiment on the corpus of code and\nmath, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from\nLLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro\nand its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced\nperformance among various benchmarks, demonstrating superiority over existing\nopen models in the LLaMA family and the immense potential of reasoning and\naddressing diverse tasks as an intelligent agent. Our findings provide valuable\ninsights into integrating natural and programming languages, laying a solid\nfoundation for developing advanced language agents that operate effectively in\nvarious environments.\n", "published": "04-01-2024", "year": "2024", "categories": ["Computation and Language"]}, {"title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init\n  Attention", "url": "http://arxiv.org/abs/2303.16199v2", "authors": "Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Yu Qiao", "abstract": "  We present LLaMA-Adapter, a lightweight adaption method to efficiently\nfine-tune LLaMA into an instruction-following model. Using 52K self-instruct\ndemonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon\nthe frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8\nA100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and\nprepend them to the word tokens at higher transformer layers. Then, a\nzero-initialized attention mechanism with zero gating is proposed, which\nadaptively injects the new instructional cues into LLaMA, while effectively\npreserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter\ncan generate high-quality responses, comparable to Alpaca with fully fine-tuned\n7B parameters. Besides language commands, our approach can be simply extended\nto multi-modal instructions for learning image-conditioned LLaMA model, which\nachieves superior reasoning performance on ScienceQA and COCO Caption\nbenchmarks. Furthermore, we also evaluate the zero-initialized attention\nmechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on\ntraditional vision and language tasks, demonstrating the superior\ngeneralization capacity of our approach. Code is released at\nhttps://github.com/OpenGVLab/LLaMA-Adapter.\n", "published": "28-03-2023", "year": "2023", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "LLaMA: Open and Efficient Foundation Language Models", "url": "http://arxiv.org/abs/2302.13971v1", "authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample", "abstract": "  We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B. We release all our models to the research community.\n", "published": "27-02-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model", "url": "http://arxiv.org/abs/2401.02330v2", "authors": "Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, Jian Tang", "abstract": "  In this paper, we introduce LLaVA-$\\phi$ (LLaVA-Phi), an efficient\nmulti-modal assistant that harnesses the power of the recently advanced small\nlanguage model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a\nnotable advancement in the realm of compact multi-modal models. It demonstrates\nthat even smaller language models, with as few as 2.7B parameters, can\neffectively engage in intricate dialogues that integrate both textual and\nvisual elements, provided they are trained with high-quality corpora. Our model\ndelivers commendable performance on publicly available benchmarks that\nencompass visual comprehension, reasoning, and knowledge-based perception.\nBeyond its remarkable performance in multi-modal dialogue tasks, our model\nopens new avenues for applications in time-sensitive environments and systems\nthat require real-time interaction, such as embodied agents. It highlights the\npotential of smaller language models to achieve sophisticated levels of\nunderstanding and interaction, while maintaining greater resource\nefficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.\n", "published": "04-01-2024", "year": "2024", "categories": ["Computation and Language"]}, {"title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents", "url": "http://arxiv.org/abs/2311.05437v1", "authors": "Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, Chunyuan Li", "abstract": "  LLaVA-Plus is a general-purpose multimodal assistant that expands the\ncapabilities of large multimodal models. It maintains a skill repository of\npre-trained vision and vision-language models and can activate relevant tools\nbased on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained on\nmultimodal instruction-following data to acquire the ability to use tools,\ncovering visual understanding, generation, external knowledge retrieval, and\ncompositions. Empirical results show that LLaVA-Plus outperforms LLaVA in\nexisting capabilities and exhibits new ones. It is distinct in that the image\nquery is directly grounded and actively engaged throughout the entire human-AI\ninteraction sessions, significantly improving tool use performance and enabling\nnew scenarios.\n", "published": "09-11-2023", "year": "2023", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language,\n  Vision, and Action", "url": "http://arxiv.org/abs/2207.04429v2", "authors": "Dhruv Shah, Blazej Osinski, Brian Ichter, Sergey Levine", "abstract": "  Goal-conditioned policies for robotic navigation can be trained on large,\nunannotated datasets, providing for good generalization to real-world settings.\nHowever, particularly in vision-based settings where specifying goals requires\nan image, this makes for an unnatural interface. Language provides a more\nconvenient modality for communication with robots, but contemporary methods\ntypically require expensive supervision, in the form of trajectories annotated\nwith language descriptions. We present a system, LM-Nav, for robotic navigation\nthat enjoys the benefits of training on unannotated large datasets of\ntrajectories, while still providing a high-level interface to the user. Instead\nof utilizing a labeled instruction following dataset, we show that such a\nsystem can be constructed entirely out of pre-trained models for navigation\n(ViNG), image-language association (CLIP), and language modeling (GPT-3),\nwithout requiring any fine-tuning or language-annotated robot data. We\ninstantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon\nnavigation through complex, outdoor environments from natural language\ninstructions. For videos of our experiments, code release, and an interactive\nColab notebook that runs in your browser, please check out our project page\nhttps://sites.google.com/view/lmnav\n", "published": "10-07-2022", "year": "2022", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "Language Is Not All You Need: Aligning Perception with Language Models", "url": "http://arxiv.org/abs/2302.14045v2", "authors": "Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei", "abstract": "  A big convergence of language, multimodal perception, action, and world\nmodeling is a key step toward artificial general intelligence. In this work, we\nintroduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive\ngeneral modalities, learn in context (i.e., few-shot), and follow instructions\n(i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale\nmultimodal corpora, including arbitrarily interleaved text and images,\nimage-caption pairs, and text data. We evaluate various settings, including\nzero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range\nof tasks without any gradient updates or finetuning. Experimental results show\nthat Kosmos-1 achieves impressive performance on (i) language understanding,\ngeneration, and even OCR-free NLP (directly fed with document images), (ii)\nperception-language tasks, including multimodal dialogue, image captioning,\nvisual question answering, and (iii) vision tasks, such as image recognition\nwith descriptions (specifying classification via text instructions). We also\nshow that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge\nfrom language to multimodal, and from multimodal to language. In addition, we\nintroduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning\ncapability of MLLMs.\n", "published": "27-02-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Language Models are Few-shot Multilingual Learners", "url": "http://arxiv.org/abs/2109.07684v1", "authors": "Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosinski, Pascale Fung", "abstract": "  General-purpose language models have demonstrated impressive capabilities,\nperforming on par with state-of-the-art approaches on a range of downstream\nnatural language processing (NLP) tasks and benchmarks when inferring\ninstructions from very few examples. Here, we evaluate the multilingual skills\nof the GPT and T5 models in conducting multi-class classification on\nnon-English languages without any parameter updates. We show that, given a few\nEnglish examples as context, pre-trained language models can predict not only\nEnglish test samples but also non-English ones. Finally, we find the in-context\nfew-shot cross-lingual prediction results of language models are significantly\nbetter than random prediction, and they are competitive compared to the\nexisting state-of-the-art cross-lingual models.\n", "published": "16-09-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Language Models are Multilingual Chain-of-Thought Reasoners", "url": "http://arxiv.org/abs/2210.03057v1", "authors": "Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei", "abstract": "  We evaluate the reasoning abilities of large language models in multilingual\nsettings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by\nmanually translating 250 grade-school math problems from the GSM8K dataset\n(Cobbe et al., 2021) into ten typologically diverse languages. We find that the\nability to solve MGSM problems via chain-of-thought prompting emerges with\nincreasing model scale, and that models have strikingly strong multilingual\nreasoning abilities, even in underrepresented languages such as Bengali and\nSwahili. Finally, we show that the multilingual reasoning abilities of language\nmodels extend to other tasks such as commonsense reasoning and word-in-context\nsemantic judgment. The MGSM benchmark is publicly available at\nhttps://github.com/google-research/url-nlp.\n", "published": "06-10-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Language Models are Super Mario: Absorbing Abilities from Homologous\n  Models as a Free Lunch", "url": "http://arxiv.org/abs/2311.03099v1", "authors": "Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li", "abstract": "  In this paper, we uncover that Language Models (LMs), either encoder- or\ndecoder-based, can obtain new capabilities by assimilating the parameters of\nhomologous models without retraining or GPUs. Typically, new abilities of LMs\ncan be imparted by Supervised Fine-Tuning (SFT), reflected in the disparity\nbetween fine-tuned and pre-trained parameters (i.e., delta parameters). We\ninitially observe that by introducing a novel operation called DARE (Drop And\nREscale), most delta parameters can be directly set to zeros without affecting\nthe capabilities of SFT LMs and larger models can tolerate a higher proportion\nof discarded parameters. Based on this observation, we further sparsify delta\nparameters of multiple SFT homologous models with DARE and subsequently merge\nthem into a single model by parameter averaging. We conduct experiments on\neight datasets from the GLUE benchmark with BERT and RoBERTa. We also merge\nWizardLM, WizardMath, and Code Alpaca based on Llama 2. Experimental results\nshow that: (1) The delta parameter value ranges for SFT models are typically\nsmall, often within 0.005, and DARE can eliminate 99% of them effortlessly.\nHowever, once the models are continuously pre-trained, the value ranges can\ngrow to around 0.03, making DARE impractical. We have also tried to remove\nfine-tuned instead of delta parameters and find that a 10% reduction can lead\nto drastically decreased performance (even to 0). This highlights that SFT\nmerely stimulates the abilities via delta parameters rather than injecting new\nabilities into LMs; (2) DARE can merge multiple task-specific LMs into one LM\nwith diverse abilities. For instance, the merger of WizardLM and WizardMath\nimproves the GSM8K zero-shot accuracy of WizardLM from 2.2 to 66.3, retaining\nits instruction-following ability while surpassing WizardMath's original 64.2\nperformance. Codes are available at https://github.com/yule-BUAA/MergeLM.\n", "published": "06-11-2023", "year": "2023", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Language Models as Knowledge Bases?", "url": "http://arxiv.org/abs/1909.01066v2", "authors": "Fabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel", "abstract": "  Recent progress in pretraining language models on large textual corpora led\nto a surge of improvements for downstream NLP tasks. Whilst learning linguistic\nknowledge, these models may also be storing relational knowledge present in the\ntraining data, and may be able to answer queries structured as\n\"fill-in-the-blank\" cloze statements. Language models have many advantages over\nstructured knowledge bases: they require no schema engineering, allow\npractitioners to query about an open class of relations, are easy to extend to\nmore data, and require no human supervision to train. We present an in-depth\nanalysis of the relational knowledge already present (without fine-tuning) in a\nwide range of state-of-the-art pretrained language models. We find that (i)\nwithout fine-tuning, BERT contains relational knowledge competitive with\ntraditional NLP methods that have some access to oracle knowledge, (ii) BERT\nalso does remarkably well on open-domain question answering against a\nsupervised baseline, and (iii) certain types of factual knowledge are learned\nmuch more readily than others by standard language model pretraining\napproaches. The surprisingly strong ability of these models to recall factual\nknowledge without any fine-tuning demonstrates their potential as unsupervised\nopen-domain QA systems. The code to reproduce our analysis is available at\nhttps://github.com/facebookresearch/LAMA.\n", "published": "03-09-2019", "year": "2019", "categories": ["Computation and Language"]}, {"title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge\n  for Embodied Agents", "url": "http://arxiv.org/abs/2201.07207v2", "authors": "Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch", "abstract": "  Can world knowledge learned by large language models (LLMs) be used to act in\ninteractive environments? In this paper, we investigate the possibility of\ngrounding high-level tasks, expressed in natural language (e.g. \"make\nbreakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While\nprior work focused on learning from explicit step-by-step examples of how to\nact, we surprisingly find that if pre-trained LMs are large enough and prompted\nappropriately, they can effectively decompose high-level tasks into mid-level\nplans without any further training. However, the plans produced naively by LLMs\noften cannot map precisely to admissible actions. We propose a procedure that\nconditions on existing demonstrations and semantically translates the plans to\nadmissible actions. Our evaluation in the recent VirtualHome environment shows\nthat the resulting method substantially improves executability over the LLM\nbaseline. The conducted human evaluation reveals a trade-off between\nexecutability and correctness but shows a promising sign towards extracting\nactionable knowledge from language models. Website at\nhttps://huangwl18.github.io/language-planner\n", "published": "18-01-2022", "year": "2022", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "Language Models of Code are Few-Shot Commonsense Learners", "url": "http://arxiv.org/abs/2210.07128v3", "authors": "Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig", "abstract": "  We address the general task of structured commonsense reasoning: given a\nnatural language input, the goal is to generate a graph such as an event -- or\na reasoning-graph. To employ large language models (LMs) for this task,\nexisting approaches ``serialize'' the output graph as a flat list of nodes and\nedges. Although feasible, these serialized graphs strongly deviate from the\nnatural language corpora that LMs were pre-trained on, hindering LMs from\ngenerating them correctly. In this paper, we show that when we instead frame\nstructured commonsense reasoning tasks as code generation tasks, pre-trained\nLMs of code are better structured commonsense reasoners than LMs of natural\nlanguage, even when the downstream task does not involve source code at all. We\ndemonstrate our approach across three diverse structured commonsense reasoning\ntasks. In all these natural language tasks, we show that using our approach, a\ncode generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the\ntarget task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot\nsetting.\n", "published": "13-10-2022", "year": "2022", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Large Language Model Alignment: A Survey", "url": "http://arxiv.org/abs/2309.15025v1", "authors": "Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong", "abstract": "  Recent years have witnessed remarkable progress made in large language models\n(LLMs). Such advancements, while garnering significant attention, have\nconcurrently elicited various concerns. The potential of these models is\nundeniably vast; however, they may yield texts that are imprecise, misleading,\nor even detrimental. Consequently, it becomes paramount to employ alignment\ntechniques to ensure these models to exhibit behaviors consistent with human\nvalues.\n  This survey endeavors to furnish an extensive exploration of alignment\nmethodologies designed for LLMs, in conjunction with the extant capability\nresearch in this domain. Adopting the lens of AI alignment, we categorize the\nprevailing methods and emergent proposals for the alignment of LLMs into outer\nand inner alignment. We also probe into salient issues including the models'\ninterpretability, and potential vulnerabilities to adversarial attacks. To\nassess LLM alignment, we present a wide variety of benchmarks and evaluation\nmethodologies. After discussing the state of alignment research for LLMs, we\nfinally cast a vision toward the future, contemplating the promising avenues of\nresearch that lie ahead.\n  Our aspiration for this survey extends beyond merely spurring research\ninterests in this realm. We also envision bridging the gap between the AI\nalignment research community and the researchers engrossed in the capability\nexploration of LLMs for both capable and safe LLMs.\n", "published": "26-09-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Large Language Models Are Human-Level Prompt Engineers", "url": "http://arxiv.org/abs/2211.01910v2", "authors": "Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba", "abstract": "  By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the \"program,\" optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.\n", "published": "03-11-2022", "year": "2022", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "Large Language Models Are Reasoning Teachers", "url": "http://arxiv.org/abs/2212.10071v2", "authors": "Namgyu Ho, Laura Schmid, Se-Young Yun", "abstract": "  Recent works have shown that chain-of-thought (CoT) prompting can elicit\nlanguage models to solve complex reasoning tasks, step-by-step. However,\nprompt-based CoT methods are dependent on very large models such as GPT-3 175B\nwhich are prohibitive to deploy at scale. In this paper, we use these large\nmodels as reasoning teachers to enable complex reasoning in smaller models and\nreduce model size requirements by several orders of magnitude. We propose\nFine-tune-CoT, a method that generates reasoning samples from very large\nteacher models to fine-tune smaller models. We evaluate our method on a wide\nrange of public models and complex tasks. We find that Fine-tune-CoT enables\nsubstantial reasoning capability in small models, far outperforming\nprompt-based baselines and even the teacher model in many tasks. Additionally,\nwe extend our method by leveraging the teacher model's ability to generate\nmultiple distinct rationales for each original sample. Enriching the\nfine-tuning data with such diverse reasoning results in a substantial\nperformance boost across datasets, even for very small models. We conduct\nablations and sample studies to understand the emergence of reasoning\ncapabilities of student models. Our code implementation and data are available\nat https://github.com/itsnamgyu/reasoning-teacher.\n", "published": "20-12-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Large Language Models Are Zero-Shot Time Series Forecasters", "url": "http://arxiv.org/abs/2310.07820v1", "authors": "Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson", "abstract": "  By encoding time series as a string of numerical digits, we can frame time\nseries forecasting as next-token prediction in text. Developing this approach,\nwe find that large language models (LLMs) such as GPT-3 and LLaMA-2 can\nsurprisingly zero-shot extrapolate time series at a level comparable to or\nexceeding the performance of purpose-built time series models trained on the\ndownstream tasks. To facilitate this performance, we propose procedures for\neffectively tokenizing time series data and converting discrete distributions\nover tokens into highly flexible densities over continuous values. We argue the\nsuccess of LLMs for time series stems from their ability to naturally represent\nmultimodal distributions, in conjunction with biases for simplicity, and\nrepetition, which align with the salient features in many time series, such as\nrepeated seasonal trends. We also show how LLMs can naturally handle missing\ndata without imputation through non-numerical text, accommodate textual side\ninformation, and answer questions to help explain predictions. While we find\nthat increasing model size generally improves performance on time series, we\nshow GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers,\nand poor uncertainty calibration, which is likely the result of alignment\ninterventions such as RLHF.\n", "published": "11-10-2023", "year": "2023", "categories": ["Machine Learning"]}, {"title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "url": "http://arxiv.org/abs/2302.00093v3", "authors": "Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Sch\u00e4rli, Denny Zhou", "abstract": "  Large language models have achieved impressive performance on various natural\nlanguage processing tasks. However, so far they have been evaluated primarily\non benchmarks where all information in the input context is relevant for\nsolving the task. In this work, we investigate the distractibility of large\nlanguage models, i.e., how the model problem-solving accuracy can be influenced\nby irrelevant context. In particular, we introduce Grade-School Math with\nIrrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant\ninformation in the problem description. We use this benchmark to measure the\ndistractibility of cutting-edge prompting techniques for large language models,\nand find that the model performance is dramatically decreased when irrelevant\ninformation is included. We also identify several approaches for mitigating\nthis deficiency, such as decoding with self-consistency and adding to the\nprompt an instruction that tells the language model to ignore the irrelevant\ninformation.\n", "published": "31-01-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Large Language Models Cannot Self-Correct Reasoning Yet", "url": "http://arxiv.org/abs/2310.01798v1", "authors": "Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, Denny Zhou", "abstract": "  Large Language Models (LLMs) have emerged as a groundbreaking technology with\ntheir unparalleled text generation capabilities across various applications.\nNevertheless, concerns persist regarding the accuracy and appropriateness of\ntheir generated content. A contemporary methodology, self-correction, has been\nproposed as a remedy to these issues. Building upon this premise, this paper\ncritically examines the role and efficacy of self-correction within LLMs,\nshedding light on its true potential and limitations. Central to our\ninvestigation is the notion of intrinsic self-correction, whereby an LLM\nattempts to correct its initial responses based solely on its inherent\ncapabilities, without the crutch of external feedback. In the context of\nreasoning, our research indicates that LLMs struggle to self-correct their\nresponses without external feedback, and at times, their performance might even\ndegrade post self-correction. Drawing from these insights, we offer suggestions\nfor future research and practical applications in this field.\n", "published": "03-10-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Large Language Models Illuminate a Progressive Pathway to Artificial\n  Healthcare Assistant: A Review", "url": "http://arxiv.org/abs/2311.01918v1", "authors": "Mingze Yuan, Peng Bao, Jiajia Yuan, Yunhao Shen, Zifan Chen, Yi Xie, Jie Zhao, Yang Chen, Li Zhang, Lin Shen, Bin Dong", "abstract": "  With the rapid development of artificial intelligence, large language models\n(LLMs) have shown promising capabilities in mimicking human-level language\ncomprehension and reasoning. This has sparked significant interest in applying\nLLMs to enhance various aspects of healthcare, ranging from medical education\nto clinical decision support. However, medicine involves multifaceted data\nmodalities and nuanced reasoning skills, presenting challenges for integrating\nLLMs. This paper provides a comprehensive review on the applications and\nimplications of LLMs in medicine. It begins by examining the fundamental\napplications of general-purpose and specialized LLMs, demonstrating their\nutilities in knowledge retrieval, research support, clinical workflow\nautomation, and diagnostic assistance. Recognizing the inherent multimodality\nof medicine, the review then focuses on multimodal LLMs, investigating their\nability to process diverse data types like medical imaging and EHRs to augment\ndiagnostic accuracy. To address LLMs' limitations regarding personalization and\ncomplex clinical reasoning, the paper explores the emerging development of\nLLM-powered autonomous agents for healthcare. Furthermore, it summarizes the\nevaluation methodologies for assessing LLMs' reliability and safety in medical\ncontexts. Overall, this review offers an extensive analysis on the\ntransformative potential of LLMs in modern medicine. It also highlights the\npivotal need for continuous optimizations and ethical oversight before these\nmodels can be effectively integrated into clinical practice. Visit\nhttps://github.com/mingze-yuan/Awesome-LLM-Healthcare for an accompanying\nGitHub repository containing latest papers.\n", "published": "03-11-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Large Language Models Relearn Removed Concepts", "url": "http://arxiv.org/abs/2401.01814v1", "authors": "Michelle Lo, Shay B. Cohen, Fazl Barez", "abstract": "  Advances in model editing through neuron pruning hold promise for removing\nundesirable concepts from large language models. However, it remains unclear\nwhether models have the capacity to reacquire pruned concepts after editing. To\ninvestigate this, we evaluate concept relearning in models by tracking concept\nsaliency and similarity in pruned neurons during retraining. Our findings\nreveal that models can quickly regain performance post-pruning by relocating\nadvanced concepts to earlier layers and reallocating pruned concepts to primed\nneurons with similar semantics. This demonstrates that models exhibit\npolysemantic capacities and can blend old and new concepts in individual\nneurons. While neuron pruning provides interpretability into model concepts,\nour results highlight the challenges of permanent concept removal for improved\nmodel \\textit{safety}. Monitoring concept reemergence and developing techniques\nto mitigate relearning of unsafe concepts will be important directions for more\nrobust model editing. Overall, our work strongly demonstrates the resilience\nand fluidity of concept representations in LLMs post concept removal.\n", "published": "03-01-2024", "year": "2024", "categories": ["Artificial Intelligence"]}, {"title": "Large Language Models Understand and Can be Enhanced by Emotional\n  Stimuli", "url": "http://arxiv.org/abs/2307.11760v7", "authors": "Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, Xing Xie", "abstract": "  Emotional intelligence significantly impacts our daily behaviors and\ninteractions. Although Large Language Models (LLMs) are increasingly viewed as\na stride toward artificial general intelligence, exhibiting impressive\nperformance in numerous tasks, it is still uncertain if LLMs can genuinely\ngrasp psychological emotional stimuli. Understanding and responding to\nemotional cues gives humans a distinct advantage in problem-solving. In this\npaper, we take the first step towards exploring the ability of LLMs to\nunderstand emotional stimuli. To this end, we first conduct automatic\nexperiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna,\nLlama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative\napplications that represent comprehensive evaluation scenarios. Our automatic\nexperiments show that LLMs have a grasp of emotional intelligence, and their\nperformance can be improved with emotional prompts (which we call\n\"EmotionPrompt\" that combines the original prompt with emotional stimuli),\ne.g., 8.00% relative performance improvement in Instruction Induction and 115%\nin BIG-Bench. In addition to those deterministic tasks that can be\nautomatically evaluated using existing metrics, we conducted a human study with\n106 participants to assess the quality of generative tasks using both vanilla\nand emotional prompts. Our human study results demonstrate that EmotionPrompt\nsignificantly boosts the performance of generative tasks (10.9% average\nimprovement in terms of performance, truthfulness, and responsibility metrics).\nWe provide an in-depth discussion regarding why EmotionPrompt works for LLMs\nand the factors that may influence its performance. We posit that EmotionPrompt\nheralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs\ninteraction.\n", "published": "14-07-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Large Language Models are Versatile Decomposers: Decompose Evidence and\n  Questions for Table-based Reasoning", "url": "http://arxiv.org/abs/2301.13808v3", "authors": "Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li", "abstract": "  Table-based reasoning has shown remarkable progress in combining deep models\nwith discrete reasoning, which requires reasoning over both free-form natural\nlanguage (NL) questions and structured tabular data. However, previous\ntable-based reasoning solutions usually suffer from significant performance\ndegradation on huge evidence (tables). In addition, most existing methods\nstruggle to reason over complex questions since the required information is\nscattered in different places. To alleviate the above challenges, we exploit\nlarge language models (LLMs) as decomposers for effective table-based\nreasoning, which (i) decompose huge evidence (a huge table) into sub-evidence\n(a small table) to mitigate the interference of useless information for table\nreasoning; and (ii) decompose complex questions into simpler sub-questions for\ntext reasoning. Specifically, we first use the LLMs to break down the evidence\n(tables) involved in the current question, retaining the relevant evidence and\nexcluding the remaining irrelevant evidence from the huge table. In addition,\nwe propose a \"parsing-execution-filling\" strategy to alleviate the\nhallucination dilemma of the chain of thought by decoupling logic and numerical\ncomputation in each step. Extensive experiments show that our method can\neffectively leverage decomposed evidence and questions and outperforms the\nstrong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably,\nour model outperforms human performance for the first time on the TabFact\ndataset.\n", "published": "31-01-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Large Language Models as Analogical Reasoners", "url": "http://arxiv.org/abs/2310.01714v2", "authors": "Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, Denny Zhou", "abstract": "  Chain-of-thought (CoT) prompting for language models demonstrates impressive\nperformance across reasoning tasks, but typically needs labeled exemplars of\nthe reasoning process. In this work, we introduce a new prompting approach,\nAnalogical Prompting, designed to automatically guide the reasoning process of\nlarge language models. Inspired by analogical reasoning, a cognitive process in\nwhich humans draw from relevant past experiences to tackle new problems, our\napproach prompts language models to self-generate relevant exemplars or\nknowledge in the context, before proceeding to solve the given problem. This\nmethod presents several advantages: it obviates the need for labeling or\nretrieving exemplars, offering generality and convenience; it can also tailor\nthe generated exemplars and knowledge to each problem, offering adaptability.\nExperimental results show that our approach outperforms 0-shot CoT and manual\nfew-shot CoT in a variety of reasoning tasks, including math problem solving in\nGSM8K and MATH, code generation in Codeforces, and other reasoning tasks in\nBIG-Bench.\n", "published": "03-10-2023", "year": "2023", "categories": ["Machine Learning"]}, {"title": "Large Language Models as Evolutionary Optimizers", "url": "http://arxiv.org/abs/2310.19046v2", "authors": "Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, Yew-Soon Ong", "abstract": "  Evolutionary algorithms (EAs) have achieved remarkable success in tackling\ncomplex combinatorial optimization problems. However, EAs often demand\ncarefully-designed operators with the aid of domain expertise to achieve\nsatisfactory performance. In this work, we present the first study on large\nlanguage models (LLMs) as evolutionary combinatorial optimizers. The main\nadvantage is that it requires minimal domain knowledge and human efforts, as\nwell as no additional training of the model. This approach is referred to as\nLLM-driven EA (LMEA). Specifically, in each generation of the evolutionary\nsearch, LMEA instructs the LLM to select parent solutions from current\npopulation, and perform crossover and mutation to generate offspring solutions.\nThen, LMEA evaluates these new solutions and include them into the population\nfor the next generation. LMEA is equipped with a self-adaptation mechanism that\ncontrols the temperature of the LLM. This enables it to balance between\nexploration and exploitation and prevents the search from getting stuck in\nlocal optima. We investigate the power of LMEA on the classical traveling\nsalesman problems (TSPs) widely used in combinatorial optimization research.\nNotably, the results show that LMEA performs competitively to traditional\nheuristics in finding high-quality solutions on TSP instances with up to 20\nnodes. Additionally, we also study the effectiveness of LLM-driven\ncrossover/mutation and the self-adaptation mechanism in evolutionary search. In\nsummary, our results reveal the great potentials of LLMs as evolutionary\noptimizers for solving combinatorial problems. We hope our research shall\ninspire future explorations on LLM-driven EAs for complex optimization\nchallenges.\n", "published": "29-10-2023", "year": "2023", "categories": []}, {"title": "Large Language Models as Optimizers", "url": "http://arxiv.org/abs/2309.03409v2", "authors": "Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen", "abstract": "  Optimization is ubiquitous. While derivative-based algorithms have been\npowerful tools for various problems, the absence of gradient imposes challenges\non many real-world applications. In this work, we propose Optimization by\nPROmpting (OPRO), a simple and effective approach to leverage large language\nmodels (LLMs) as optimizers, where the optimization task is described in\nnatural language. In each optimization step, the LLM generates new solutions\nfrom the prompt that contains previously generated solutions with their values,\nthen the new solutions are evaluated and added to the prompt for the next\noptimization step. We first showcase OPRO on linear regression and traveling\nsalesman problems, then move on to prompt optimization where the goal is to\nfind instructions that maximize the task accuracy. With a variety of LLMs, we\ndemonstrate that the best prompts optimized by OPRO outperform human-designed\nprompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at\nhttps://github.com/google-deepmind/opro.\n", "published": "07-09-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "Large Language Models as Optimizers", "url": "http://arxiv.org/abs/2309.03409v2", "authors": "Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen", "abstract": "  Optimization is ubiquitous. While derivative-based algorithms have been\npowerful tools for various problems, the absence of gradient imposes challenges\non many real-world applications. In this work, we propose Optimization by\nPROmpting (OPRO), a simple and effective approach to leverage large language\nmodels (LLMs) as optimizers, where the optimization task is described in\nnatural language. In each optimization step, the LLM generates new solutions\nfrom the prompt that contains previously generated solutions with their values,\nthen the new solutions are evaluated and added to the prompt for the next\noptimization step. We first showcase OPRO on linear regression and traveling\nsalesman problems, then move on to prompt optimization where the goal is to\nfind instructions that maximize the task accuracy. With a variety of LLMs, we\ndemonstrate that the best prompts optimized by OPRO outperform human-designed\nprompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at\nhttps://github.com/google-deepmind/opro.\n", "published": "07-09-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "Large Language Models for Generative Information Extraction: A Survey", "url": "http://arxiv.org/abs/2312.17617v1", "authors": "Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Enhong Chen", "abstract": "  Information extraction (IE) aims to extract structural knowledge (such as\nentities, relations, and events) from plain natural language texts. Recently,\ngenerative Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in text understanding and generation, allowing for generalization\nacross various domains and tasks. As a result, numerous works have been\nproposed to harness abilities of LLMs and offer viable solutions for IE tasks\nbased on a generative paradigm. To conduct a comprehensive systematic review\nand exploration of LLM efforts for IE tasks, in this study, we survey the most\nrecent advancements in this field. We first present an extensive overview by\ncategorizing these works in terms of various IE subtasks and learning\nparadigms, then we empirically analyze the most advanced methods and discover\nthe emerging trend of IE tasks with LLMs. Based on thorough review conducted,\nwe identify several insights in technique and promising research directions\nthat deserve further exploration in future studies. We maintain a public\nrepository and consistently update related resources at:\n\\url{https://github.com/quqxui/Awesome-LLM4IE-Papers}.\n", "published": "29-12-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Large Language Models for Software Engineering: Survey and Open Problems", "url": "http://arxiv.org/abs/2310.03533v4", "authors": "Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, Jie M. Zhang", "abstract": "  This paper provides a survey of the emerging area of Large Language Models\n(LLMs) for Software Engineering (SE). It also sets out open research challenges\nfor the application of LLMs to technical problems faced by software engineers.\nLLMs' emergent properties bring novelty and creativity with applications right\nacross the spectrum of Software Engineering activities including coding,\ndesign, requirements, repair, refactoring, performance improvement,\ndocumentation and analytics. However, these very same emergent properties also\npose significant technical challenges; we need techniques that can reliably\nweed out incorrect solutions, such as hallucinations. Our survey reveals the\npivotal role that hybrid techniques (traditional SE plus LLMs) have to play in\nthe development and deployment of reliable, efficient and effective LLM-based\nSE.\n", "published": "05-10-2023", "year": "2023", "categories": []}, {"title": "Large Language Models in the Workplace: A Case Study on Prompt\n  Engineering for Job Type Classification", "url": "http://arxiv.org/abs/2303.07142v3", "authors": "Benjamin Clavi\u00e9, Alexandru Ciceu, Frederick Naylor, Guillaume Souli\u00e9, Thomas Brightwell", "abstract": "  This case study investigates the task of job classification in a real-world\nsetting, where the goal is to determine whether an English-language job posting\nis appropriate for a graduate or entry-level position. We explore multiple\napproaches to text classification, including supervised approaches such as\ntraditional models like Support Vector Machines (SVMs) and state-of-the-art\ndeep learning methods such as DeBERTa. We compare them with Large Language\nModels (LLMs) used in both few-shot and zero-shot classification settings. To\naccomplish this task, we employ prompt engineering, a technique that involves\ndesigning prompts to guide the LLMs towards the desired output. Specifically,\nwe evaluate the performance of two commercially available state-of-the-art\nGPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also\nconduct a detailed analysis of the impact of different aspects of prompt\nengineering on the model's performance. Our results show that, with a\nwell-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all\nother models, achieving a 6% increase in Precision@95% Recall compared to the\nbest supervised approach. Furthermore, we observe that the wording of the\nprompt is a critical factor in eliciting the appropriate \"reasoning\" in the\nmodel, and that seemingly minor aspects of the prompt significantly affect the\nmodel's performance.\n", "published": "13-03-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Large-scale Training of Foundation Models for Wearable Biosignals", "url": "http://arxiv.org/abs/2312.05409v1", "authors": "Salar Abbaspourazad, Oussama Elachqar, Andrew C. Miller, Saba Emrani, Udhyakumar Nallasamy, Ian Shapiro", "abstract": "  Tracking biosignals is crucial for monitoring wellness and preempting the\ndevelopment of severe medical conditions. Today, wearable devices can\nconveniently record various biosignals, creating the opportunity to monitor\nhealth status without disruption to one's daily routine. Despite widespread use\nof wearable devices and existing digital biomarkers, the absence of curated\ndata with annotated medical labels hinders the development of new biomarkers to\nmeasure common health conditions. In fact, medical datasets are usually small\nin comparison to other domains, which is an obstacle for developing neural\nnetwork models for biosignals. To address this challenge, we have employed\nself-supervised learning using the unlabeled sensor data collected under\ninformed consent from the large longitudinal Apple Heart and Movement Study\n(AHMS) to train foundation models for two common biosignals:\nphotoplethysmography (PPG) and electrocardiogram (ECG) recorded on Apple Watch.\nWe curated PPG and ECG datasets from AHMS that include data from ~141K\nparticipants spanning ~3 years. Our self-supervised learning framework includes\nparticipant level positive pair selection, stochastic augmentation module and a\nregularized contrastive loss optimized with momentum training, and generalizes\nwell to both PPG and ECG modalities. We show that the pre-trained foundation\nmodels readily encode information regarding participants' demographics and\nhealth conditions. To the best of our knowledge, this is the first study that\nbuilds foundation models using large-scale PPG and ECG data collected via\nwearable consumer devices $\\unicode{x2013}$ prior works have commonly used\nsmaller-size datasets collected in clinical and experimental settings. We\nbelieve PPG and ECG foundation models can enhance future wearable devices by\nreducing the reliance on labeled data and hold the potential to help the users\nimprove their health.\n", "published": "08-12-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence"]}, {"title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science\n  Question Answering", "url": "http://arxiv.org/abs/2209.09513v2", "authors": "Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan", "abstract": "  When answering a question, humans utilize the information available across\ndifferent modalities to synthesize a consistent and complete chain of thought\n(CoT). This process is normally a black box in the case of deep learning models\nlike large-scale language models. Recently, science question benchmarks have\nbeen used to diagnose the multi-hop reasoning ability and interpretability of\nan AI system. However, existing datasets fail to provide annotations for the\nanswers, or are restricted to the textual-only modality, small scales, and\nlimited domain diversity. To this end, we present Science Question Answering\n(ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice\nquestions with a diverse set of science topics and annotations of their answers\nwith corresponding lectures and explanations. We further design language models\nto learn to generate lectures and explanations as the chain of thought (CoT) to\nmimic the multi-hop reasoning process when answering ScienceQA questions.\nScienceQA demonstrates the utility of CoT in language models, as CoT improves\nthe question answering performance by 1.20% in few-shot GPT-3 and 3.99% in\nfine-tuned UnifiedQA. We also explore the upper bound for models to leverage\nexplanations by feeding those in the input; we observe that it improves the\nfew-shot performance of GPT-3 by 18.96%. Our analysis further shows that\nlanguage models, similar to humans, benefit from explanations to learn from\nfewer data and achieve the same performance with just 40% of the data. The data\nand code are available at https://scienceqa.github.io.\n", "published": "20-09-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Learning From Mistakes Makes LLM Better Reasoner", "url": "http://arxiv.org/abs/2310.20689v2", "authors": "Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, Weizhu Chen", "abstract": "  Large language models (LLMs) recently exhibited remarkable reasoning\ncapabilities on solving math problems. To further improve this capability, this\nwork proposes Learning from Mistakes (LeMa), akin to human learning processes.\nConsider a human student who failed to solve a math problem, he will learn from\nwhat mistake he has made and how to correct it. Mimicking this error-driven\nlearning process, LeMa fine-tunes LLMs on mistake-correction data pairs\ngenerated by GPT-4. Specifically, we first collect inaccurate reasoning paths\nfrom various LLMs and then employ GPT-4 as a \"corrector\" to (1) identify the\nmistake step, (2) explain the reason for the mistake, and (3) correct the\nmistake and generate the final answer. Experimental results demonstrate the\neffectiveness of LeMa: across five backbone LLMs and two mathematical reasoning\ntasks, LeMa consistently improves the performance compared with fine-tuning on\nCoT data alone. Impressively, LeMa can also benefit specialized LLMs such as\nWizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on\nMATH. This surpasses the SOTA performance achieved by non-execution open-source\nmodels on these challenging tasks. Our code, data and models will be publicly\navailable at https://github.com/microsoft/LEMA.\n", "published": "31-10-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts", "url": "http://arxiv.org/abs/2104.06599v1", "authors": "Guanghui Qin, Jason Eisner", "abstract": "  Natural-language prompts have recently been used to coax pretrained language\nmodels into performing other AI tasks, using a fill-in-the-blank paradigm\n(Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al.,\n2020). For example, language models retain factual knowledge from their\ntraining corpora that can be extracted by asking them to \"fill in the blank\" in\na sentential prompt. However, where does this prompt come from? We explore the\nidea of learning prompts by gradient descent -- either fine-tuning prompts\ntaken from previous work, or starting from random initialization. Our prompts\nconsist of \"soft words,\" i.e., continuous vectors that are not necessarily word\ntype embeddings from the language model. Furthermore, for each task, we\noptimize a mixture of prompts, learning which prompts are most effective and\nhow to ensemble them. Across multiple English LMs and tasks, our approach\nhugely outperforms previous methods, showing that the implicit factual\nknowledge in language models was previously underestimated. Moreover, this\nknowledge is cheap to elicit: random initialization is nearly as good as\ninformed initialization.\n", "published": "14-04-2021", "year": "2021", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Learning Transferable Visual Models From Natural Language Supervision", "url": "http://arxiv.org/abs/2103.00020v1", "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever", "abstract": "  State-of-the-art computer vision systems are trained to predict a fixed set\nof predetermined object categories. This restricted form of supervision limits\ntheir generality and usability since additional labeled data is needed to\nspecify any other visual concept. Learning directly from raw text about images\nis a promising alternative which leverages a much broader source of\nsupervision. We demonstrate that the simple pre-training task of predicting\nwhich caption goes with which image is an efficient and scalable way to learn\nSOTA image representations from scratch on a dataset of 400 million (image,\ntext) pairs collected from the internet. After pre-training, natural language\nis used to reference learned visual concepts (or describe new ones) enabling\nzero-shot transfer of the model to downstream tasks. We study the performance\nof this approach by benchmarking on over 30 different existing computer vision\ndatasets, spanning tasks such as OCR, action recognition in videos,\ngeo-localization, and many types of fine-grained object classification. The\nmodel transfers non-trivially to most tasks and is often competitive with a\nfully supervised baseline without the need for any dataset specific training.\nFor instance, we match the accuracy of the original ResNet-50 on ImageNet\nzero-shot without needing to use any of the 1.28 million training examples it\nwas trained on. We release our code and pre-trained model weights at\nhttps://github.com/OpenAI/CLIP.\n", "published": "26-02-2021", "year": "2021", "categories": ["Machine Learning"]}, {"title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language\n  Models", "url": "http://arxiv.org/abs/2205.10625v3", "authors": "Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, Ed Chi", "abstract": "  Chain-of-thought prompting has demonstrated remarkable performance on various\nnatural language reasoning tasks. However, it tends to perform poorly on tasks\nwhich requires solving problems harder than the exemplars shown in the prompts.\nTo overcome this challenge of easy-to-hard generalization, we propose a novel\nprompting strategy, least-to-most prompting. The key idea in this strategy is\nto break down a complex problem into a series of simpler subproblems and then\nsolve them in sequence. Solving each subproblem is facilitated by the answers\nto previously solved subproblems. Our experimental results on tasks related to\nsymbolic manipulation, compositional generalization, and math reasoning reveal\nthat least-to-most prompting is capable of generalizing to more difficult\nproblems than those seen in the prompts. A notable finding is that when the\nGPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve\nthe compositional generalization benchmark SCAN in any split (including length\nsplit) with an accuracy of at least 99% using just 14 exemplars, compared to\nonly 16% accuracy with chain-of-thought prompting. This is particularly\nnoteworthy because neural-symbolic models in the literature that specialize in\nsolving SCAN are trained on the entire training set containing over 15,000\nexamples. We have included prompts for all the tasks in the Appendix.\n", "published": "21-05-2022", "year": "2022", "categories": ["Artificial Intelligence", "Computation and Language"]}, {"title": "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses", "url": "http://arxiv.org/abs/2305.19339v1", "authors": "Liyan Tang, Yifan Peng, Yanshan Wang, Ying Ding, Greg Durrett, Justin F. Rousseau", "abstract": "  A human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.\n", "published": "30-05-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "url": "http://arxiv.org/abs/2307.09288v2", "authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom", "abstract": "  In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.\n", "published": "18-07-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "LoRA: Low-Rank Adaptation of Large Language Models", "url": "http://arxiv.org/abs/2106.09685v2", "authors": "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen", "abstract": "  An important paradigm of natural language processing consists of large-scale\npre-training on general domain data and adaptation to particular tasks or\ndomains. As we pre-train larger models, full fine-tuning, which retrains all\nmodel parameters, becomes less feasible. Using GPT-3 175B as an example --\ndeploying independent instances of fine-tuned models, each with 175B\nparameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\nLoRA, which freezes the pre-trained model weights and injects trainable rank\ndecomposition matrices into each layer of the Transformer architecture, greatly\nreducing the number of trainable parameters for downstream tasks. Compared to\nGPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\nparameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\nperforms on-par or better than fine-tuning in model quality on RoBERTa,\nDeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\ntraining throughput, and, unlike adapters, no additional inference latency. We\nalso provide an empirical investigation into rank-deficiency in language model\nadaptation, which sheds light on the efficacy of LoRA. We release a package\nthat facilitates the integration of LoRA with PyTorch models and provide our\nimplementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\nhttps://github.com/microsoft/LoRA.\n", "published": "17-06-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models", "url": "http://arxiv.org/abs/2310.08659v4", "authors": "Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, Tuo Zhao", "abstract": "  Quantization is an indispensable technique for serving Large Language Models\n(LLMs) and has recently found its way into LoRA fine-tuning. In this work we\nfocus on the scenario where quantization and LoRA fine-tuning are applied\ntogether on a pre-trained model. In such cases it is common to observe a\nconsistent gap in the performance on downstream tasks between full fine-tuning\nand quantization plus LoRA fine-tuning approach. In response, we propose LoftQ\n(LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that\nsimultaneously quantizes an LLM and finds a proper low-rank initialization for\nLoRA fine-tuning. Such an initialization alleviates the discrepancy between the\nquantized and full-precision model and significantly improves generalization in\ndownstream tasks. We evaluate our method on natural language understanding,\nquestion answering, summarization, and natural language generation tasks.\nExperiments show that our method is highly effective and outperforms existing\nquantization methods, especially in the challenging 2-bit and 2/4-bit mixed\nprecision regimes. The code is available on https://github.com/yxli2123/LoftQ.\n", "published": "12-10-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Longformer: The Long-Document Transformer", "url": "http://arxiv.org/abs/2004.05150v2", "authors": "Iz Beltagy, Matthew E. Peters, Arman Cohan", "abstract": "  Transformer-based models are unable to process long sequences due to their\nself-attention operation, which scales quadratically with the sequence length.\nTo address this limitation, we introduce the Longformer with an attention\nmechanism that scales linearly with sequence length, making it easy to process\ndocuments of thousands of tokens or longer. Longformer's attention mechanism is\na drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Following prior work\non long-sequence transformers, we evaluate Longformer on character-level\nlanguage modeling and achieve state-of-the-art results on text8 and enwik8. In\ncontrast to most prior work, we also pretrain Longformer and finetune it on a\nvariety of downstream tasks. Our pretrained Longformer consistently outperforms\nRoBERTa on long document tasks and sets new state-of-the-art results on WikiHop\nand TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a\nLongformer variant for supporting long document generative sequence-to-sequence\ntasks, and demonstrate its effectiveness on the arXiv summarization dataset.\n", "published": "10-04-2020", "year": "2020", "categories": ["Computation and Language"]}, {"title": "MEGAVERSE: Benchmarking Large Language Models Across Languages,\n  Modalities, Models and Tasks", "url": "http://arxiv.org/abs/2311.07463v1", "authors": "Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika Bali, Sunayana Sitaram", "abstract": "  Recently, there has been a rapid advancement in research on Large Language\nModels (LLMs), resulting in significant progress in several Natural Language\nProcessing (NLP) tasks. Consequently, there has been a surge in LLM evaluation\nresearch to comprehend the models' capabilities and limitations. However, much\nof this research has been confined to the English language, leaving LLM\nbuilding and evaluation for non-English languages relatively unexplored. There\nhas been an introduction of several new LLMs, necessitating their evaluation on\nnon-English languages. This study aims to expand our MEGA benchmarking suite by\nincluding six new datasets to form the MEGAVERSE benchmark. The benchmark\ncomprises 22 datasets covering 81 languages, including low-resource African\nlanguages. We evaluate several state-of-the-art LLMs like GPT-3.5-Turbo, GPT4,\nPaLM2, and Llama2 on the MEGAVERSE datasets. Additionally, we include two\nmultimodal datasets in the benchmark and assess the performance of the\nLLaVa-v1.5 model. Our experiments suggest that GPT4 and PaLM2 outperform the\nLlama models on various tasks, notably on low-resource languages, with GPT4\noutperforming PaLM2 on more datasets than vice versa. However, issues such as\ndata contamination must be addressed to obtain an accurate assessment of LLM\nperformance on non-English languages.\n", "published": "13-11-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "MLCopilot: Unleashing the Power of Large Language Models in Solving\n  Machine Learning Tasks", "url": "http://arxiv.org/abs/2304.14979v1", "authors": "Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, Yuqing Yang", "abstract": "  The field of machine learning (ML) has gained widespread adoption, leading to\na significant demand for adapting ML to specific scenarios, which is yet\nexpensive and non-trivial. The predominant approaches towards the automation of\nsolving ML tasks (e.g., AutoML) are often time consuming and hard to understand\nfor human developers. In contrast, though human engineers have the incredible\nability to understand tasks and reason about solutions, their experience and\nknowledge are often sparse and difficult to utilize by quantitative approaches.\nIn this paper, we aim to bridge the gap between machine intelligence and human\nknowledge by introducing a novel framework MLCopilot, which leverages the\nstate-of-the-art LLMs to develop ML solutions for novel tasks. We showcase the\npossibility of extending the capability of LLMs to comprehend structured inputs\nand perform thorough reasoning for solving novel ML tasks. And we find that,\nafter some dedicated design, the LLM can (i) observe from the existing\nexperiences of ML tasks and (ii) reason effectively to deliver promising\nresults for new tasks. The solution generated can be used directly to achieve\nhigh levels of competitiveness.\n", "published": "28-04-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence"]}, {"title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action", "url": "http://arxiv.org/abs/2303.11381v1", "authors": "Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang", "abstract": "  We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of\nvision experts to achieve multimodal reasoning and action. In this paper, we\ndefine and explore a comprehensive list of advanced vision tasks that are\nintriguing to solve, but may exceed the capabilities of existing vision and\nvision-language models. To achieve such advanced visual intelligence, MM-REACT\nintroduces a textual prompt design that can represent text descriptions,\ntextualized spatial coordinates, and aligned file names for dense visual\nsignals such as images and videos. MM-REACT's prompt design allows language\nmodels to accept, associate, and process multimodal information, thereby\nfacilitating the synergetic combination of ChatGPT and various vision experts.\nZero-shot experiments demonstrate MM-REACT's effectiveness in addressing the\nspecified capabilities of interests and its wide application in different\nscenarios that require advanced visual understanding. Furthermore, we discuss\nand compare MM-REACT's system paradigm with an alternative approach that\nextends language models for multimodal scenarios through joint finetuning.\nCode, demo, video, and visualization are available at\nhttps://multimodal-react.github.io/\n", "published": "20-03-2023", "year": "2023", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Making Large Language Models Better Reasoners with Step-Aware Verifier", "url": "http://arxiv.org/abs/2206.02336v3", "authors": "Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen", "abstract": "  Few-shot learning is a challenging task that requires language models to\ngeneralize from limited examples. Large language models like GPT-3 and PaLM\nhave made impressive progress in this area, but they still face difficulties in\nreasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve\ntheir reasoning skills, previous work has proposed to guide the language model\nwith prompts that elicit a series of reasoning steps before giving the final\nanswer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in\nproblem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on\nReasoning Step), a novel approach that further enhances the reasoning\ncapability of language models. DIVERSE has three main components: first, it\ngenerates diverse prompts to explore different reasoning paths for the same\nquestion; second, it uses a verifier to filter out incorrect answers based on a\nweighted voting scheme; and third, it verifies each reasoning step individually\ninstead of the whole chain. We evaluate DIVERSE on the latest language model\ncode-davinci-002 and show that it achieves new state-of-the-art results on six\nof eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).\n", "published": "06-06-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Making Pre-trained Language Models Better Few-shot Learners", "url": "http://arxiv.org/abs/2012.15723v2", "authors": "Tianyu Gao, Adam Fisch, Danqi Chen", "abstract": "  The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot\nperformance solely by leveraging a natural-language prompt and a few task\ndemonstrations as input context. Inspired by their findings, we study few-shot\nlearning in a more practical scenario, where we use smaller language models for\nwhich fine-tuning is computationally efficient. We present LM-BFF--better\nfew-shot fine-tuning of language models--a suite of simple and complementary\ntechniques for fine-tuning language models on a small number of annotated\nexamples. Our approach includes (1) prompt-based fine-tuning together with a\nnovel pipeline for automating prompt generation; and (2) a refined strategy for\ndynamically and selectively incorporating demonstrations into each context.\nFinally, we present a systematic evaluation for analyzing few-shot performance\non a range of NLP tasks, including classification and regression. Our\nexperiments demonstrate that our methods combine to dramatically outperform\nstandard fine-tuning procedures in this low resource setting, achieving up to\n30% absolute improvement, and 11% on average across all tasks. Our approach\nmakes minimal assumptions on task resources and domain expertise, and hence\nconstitutes a strong task-agnostic method for few-shot learning.\n", "published": "31-12-2020", "year": "2020", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical\n  Reasoning", "url": "http://arxiv.org/abs/2311.10537v1", "authors": "Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, Mark Gerstein", "abstract": "  Large Language Models (LLMs), despite their remarkable progress across\nvarious general domains, encounter significant barriers in medicine and\nhealthcare. This field faces unique challenges such as domain-specific\nterminologies and the reasoning over specialized knowledge. To address these\nobstinate issues, we propose a novel Multi-disciplinary Collaboration (MC)\nframework for the medical domain that leverages role-playing LLM-based agents\nwho participate in a collaborative multi-round discussion, thereby enhancing\nLLM proficiency and reasoning capabilities. This training-free and\ninterpretable framework encompasses five critical steps: gathering domain\nexperts, proposing individual analyses, summarising these analyses into a\nreport, iterating over discussions until a consensus is reached, and ultimately\nmaking a decision. Our work particularly focuses on the zero-shot scenario, our\nresults on nine data sets (MedQA, MedMCQA, PubMedQA, and six subtasks from\nMMLU) establish that our proposed MC framework excels at mining and harnessing\nthe medical expertise in LLMs, as well as extending its reasoning abilities.\nBased on these outcomes, we further conduct a human evaluation to pinpoint and\ncategorize common errors within our method, as well as ablation studies aimed\nat understanding the impact of various factors on overall performance. Our code\ncan be found at \\url{https://github.com/gersteinlab/MedAgents}.\n", "published": "16-11-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using\n  Model Parallelism", "url": "http://arxiv.org/abs/1909.08053v4", "authors": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro", "abstract": "  Recent work in language modeling demonstrates that training large transformer\nmodels advances the state of the art in Natural Language Processing\napplications. However, very large models can be quite difficult to train due to\nmemory constraints. In this work, we present our techniques for training very\nlarge transformer models and implement a simple, efficient intra-layer model\nparallel approach that enables training transformer models with billions of\nparameters. Our approach does not require a new compiler or library changes, is\northogonal and complimentary to pipeline model parallelism, and can be fully\nimplemented with the insertion of a few communication operations in native\nPyTorch. We illustrate this approach by converging transformer based models up\nto 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the\nentire application with 76% scaling efficiency when compared to a strong single\nGPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To\ndemonstrate that large language models can further advance the state of the art\n(SOTA), we train an 8.3 billion parameter transformer language model similar to\nGPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful\nattention to the placement of layer normalization in BERT-like models is\ncritical to achieving increased performance as the model size grows. Using the\nGPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA\nperplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%)\ndatasets. Our BERT model achieves SOTA results on the RACE dataset (90.9%\ncompared to SOTA accuracy of 89.4%).\n", "published": "17-09-2019", "year": "2019", "categories": ["Computation and Language"]}, {"title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework", "url": "http://arxiv.org/abs/2308.00352v5", "authors": "Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, J\u00fcrgen Schmidhuber", "abstract": "  Remarkable progress has been made on automated problem solving through\nsocieties of agents based on large language models (LLMs). Existing LLM-based\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\ncomplex tasks, however, are complicated through logic inconsistencies due to\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\nMetaGPT, an innovative meta-programming framework incorporating efficient human\nworkflows into LLM-based multi-agent collaborations. MetaGPT encodes\nStandardized Operating Procedures (SOPs) into prompt sequences for more\nstreamlined workflows, thus allowing agents with human-like domain expertise to\nverify intermediate results and reduce errors. MetaGPT utilizes an assembly\nline paradigm to assign diverse roles to various agents, efficiently breaking\ndown complex tasks into subtasks involving many agents working together. On\ncollaborative software engineering benchmarks, MetaGPT generates more coherent\nsolutions than previous chat-based multi-agent systems. Our project can be\nfound at https://github.com/geekan/MetaGPT\n", "published": "01-08-2023", "year": "2023", "categories": ["Artificial Intelligence"]}, {"title": "Mind2Web: Towards a Generalist Agent for the Web", "url": "http://arxiv.org/abs/2306.06070v3", "authors": "Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su", "abstract": "  We introduce Mind2Web, the first dataset for developing and evaluating\ngeneralist agents for the web that can follow language instructions to complete\ncomplex tasks on any website. Existing datasets for web agents either use\nsimulated websites or only cover a limited set of websites and tasks, thus not\nsuitable for generalist web agents. With over 2,000 open-ended tasks collected\nfrom 137 websites spanning 31 domains and crowdsourced action sequences for the\ntasks, Mind2Web provides three necessary ingredients for building generalist\nweb agents: 1) diverse domains, websites, and tasks, 2) use of real-world\nwebsites instead of simulated and simplified ones, and 3) a broad spectrum of\nuser interaction patterns. Based on Mind2Web, we conduct an initial exploration\nof using large language models (LLMs) for building generalist web agents. While\nthe raw HTML of real-world websites are often too large to be fed to LLMs, we\nshow that first filtering it with a small LM significantly improves the\neffectiveness and efficiency of LLMs. Our solution demonstrates a decent level\nof performance, even on websites or entire domains the model has never seen\nbefore, but there is still a substantial room to improve towards truly\ngeneralizable agents. We open-source our dataset, model implementation, and\ntrained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further\nresearch on building a generalist agent for the web.\n", "published": "09-06-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "MindAgent: Emergent Gaming Interaction", "url": "http://arxiv.org/abs/2309.09971v2", "authors": "Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao", "abstract": "  Large Language Models (LLMs) have the capacity of performing complex\nscheduling in a multi-agent system and can coordinate these agents into\ncompleting sophisticated tasks that require extensive collaboration. However,\ndespite the introduction of numerous gaming frameworks, the community has\ninsufficient benchmarks towards building general multi-agents collaboration\ninfrastructure that encompass both LLM and human-NPCs collaborations. In this\nwork, we propose a novel infrastructure - MindAgent - to evaluate planning and\ncoordination emergent capabilities for gaming interaction. In particular, our\ninfrastructure leverages existing gaming framework, to i) require understanding\nof the coordinator for a multi-agent system, ii) collaborate with human players\nvia un-finetuned proper instructions, and iii) establish an in-context learning\non few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new\ngaming scenario and related benchmark that dispatch a multi-agent collaboration\nefficiency and supervise multiple agents playing the game simultaneously. We\nconduct comprehensive evaluations with new auto-metric CoS for calculating the\ncollaboration efficiency. Finally, our infrastructure can be deployed into\nreal-world gaming scenarios in a customized VR version of CUISINEWORLD and\nadapted in existing broader Minecraft gaming domain. We hope our findings on\nLLMs and the new infrastructure for general-purpose scheduling and coordination\ncan help shed light on how such skills can be obtained by learning from large\nlanguage corpora.\n", "published": "18-09-2023", "year": "2023", "categories": ["Artificial Intelligence"]}, {"title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale\n  Knowledge", "url": "http://arxiv.org/abs/2206.08853v2", "authors": "Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, Anima Anandkumar", "abstract": "  Autonomous agents have made great strides in specialist domains like Atari\ngames and Go. However, they typically learn tabula rasa in isolated\nenvironments with limited and manually conceived objectives, thus failing to\ngeneralize across a wide spectrum of tasks and capabilities. Inspired by how\nhumans continually learn and adapt in the open world, we advocate a trinity of\ningredients for building generalist agents: 1) an environment that supports a\nmultitude of tasks and goals, 2) a large-scale database of multimodal\nknowledge, and 3) a flexible and scalable agent architecture. We introduce\nMineDojo, a new framework built on the popular Minecraft game that features a\nsimulation suite with thousands of diverse open-ended tasks and an\ninternet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. Using MineDojo's data, we propose a novel agent learning\nalgorithm that leverages large pre-trained video-language models as a learned\nreward function. Our agent is able to solve a variety of open-ended tasks\nspecified in free-form language without any manually designed dense shaping\nreward. We open-source the simulation suite, knowledge bases, algorithm\nimplementation, and pretrained models (https://minedojo.org) to promote\nresearch towards the goal of generally capable embodied agents.\n", "published": "17-06-2022", "year": "2022", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "Mini-GPTs: Efficient Large Language Models through Contextual Pruning", "url": "http://arxiv.org/abs/2312.12682v1", "authors": "Tim Valicenti, Justice Vidal, Ritik Patnaik", "abstract": "  In AI research, the optimization of Large Language Models (LLMs) remains a\nsignificant challenge, crucial for advancing the field's practical applications\nand sustainability. Building upon the foundational work of Professor Song Han's\nlab at MIT, this paper introduces a novel approach in developing Mini-GPTs via\ncontextual pruning. Our methodology strategically prunes the computational\narchitecture of traditional LLMs, like Phi-1.5, focusing on retaining core\nfunctionalities while drastically reducing model sizes. We employ the technique\nacross diverse and complex datasets, including US law, Medical Q&A, Skyrim\ndialogue, English-Taiwanese translation, and Economics articles. The results\nunderscore the efficiency and effectiveness of contextual pruning, not merely\nas a theoretical concept but as a practical tool in developing domain-specific,\nresource-efficient LLMs. Contextual pruning is a promising method for building\ndomain-specific LLMs, and this research is a building block towards future\ndevelopment with more hardware compute, refined fine-tuning, and quantization.\n", "published": "20-12-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "MiniGPT-v2: large language model as a unified interface for\n  vision-language multi-task learning", "url": "http://arxiv.org/abs/2310.09478v3", "authors": "Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny", "abstract": "  Large language models have shown their remarkable capabilities as a general\ninterface for various language-related applications. Motivated by this, we\ntarget to build a unified interface for completing many vision-language tasks\nincluding image description, visual question answering, and visual grounding,\namong others. The challenge is to use a single model for performing diverse\nvision-language tasks effectively with simple multi-modal instructions. Towards\nthis objective, we introduce MiniGPT-v2, a model that can be treated as a\nunified interface for better handling various vision-language tasks. We propose\nusing unique identifiers for different tasks when training the model. These\nidentifiers enable our model to better distinguish each task instruction\neffortlessly and also improve the model learning efficiency for each task.\nAfter the three-stage training, the experimental results show that MiniGPT-v2\nachieves strong performance on many visual question-answering and visual\ngrounding benchmarks compared to other vision-language generalist models. Our\nmodel and codes are available at https://minigpt-v2.github.io/\n", "published": "14-10-2023", "year": "2023", "categories": []}, {"title": "Mixtral of Experts", "url": "http://arxiv.org/abs/2401.04088v1", "authors": "Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, William El Sayed", "abstract": "  We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.\n", "published": "08-01-2024", "year": "2024", "categories": ["Machine Learning", "Computation and Language"]}, {"title": "MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile\n  Devices", "url": "http://arxiv.org/abs/2312.16886v2", "authors": "Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, Chunhua Shen", "abstract": "  We present MobileVLM, a competent multimodal vision language model (MMVLM)\ntargeted to run on mobile devices. It is an amalgamation of a myriad of\narchitectural designs and techniques that are mobile-oriented, which comprises\na set of language models at the scale of 1.4B and 2.7B parameters, trained from\nscratch, a multimodal vision model that is pre-trained in the CLIP fashion,\ncross-modality interaction via an efficient projector. We evaluate MobileVLM on\nseveral typical VLM benchmarks. Our models demonstrate on par performance\ncompared with a few much larger models. More importantly, we measure the\ninference speed on both a Qualcomm Snapdragon 888 CPU and an NVIDIA Jeston Orin\nGPU, and we obtain state-of-the-art performance of 21.5 tokens and 65.3 tokens\nper second, respectively. Our code will be made available at:\nhttps://github.com/Meituan-AutoML/MobileVLM.\n", "published": "28-12-2023", "year": "2023", "categories": []}, {"title": "MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of\n  Thought Prompting", "url": "http://arxiv.org/abs/2305.16896v1", "authors": "Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, Sadao Kurohashi", "abstract": "  Large language models (LLMs) have achieved impressive performance on various\nreasoning tasks. To further improve the performance, we propose MultiTool-CoT,\na novel framework that leverages chain-of-thought (CoT) prompting to\nincorporate multiple external tools, such as a calculator and a knowledge\nretriever, during the reasoning process. We apply MultiTool-CoT to the Task 2\ndataset of NumGLUE, which requires both numerical reasoning and domain-specific\nknowledge. The experiments show that our method significantly outperforms\nstrong baselines and achieves state-of-the-art performance.\n", "published": "26-05-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning", "url": "http://arxiv.org/abs/2303.02861v1", "authors": "Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, Yoon Kim", "abstract": "  Prompt tuning, in which a base pretrained model is adapted to each task via\nconditioning on learned prompt vectors, has emerged as a promising approach for\nefficiently adapting large language models to multiple downstream tasks.\nHowever, existing methods typically learn soft prompt vectors from scratch, and\nit has not been clear how to exploit the rich cross-task knowledge with prompt\nvectors in a multitask learning setting. We propose multitask prompt tuning\n(MPT), which first learns a single transferable prompt by distilling knowledge\nfrom multiple task-specific source prompts. We then learn multiplicative low\nrank updates to this shared prompt to efficiently adapt it to each downstream\ntarget task. Extensive experiments on 23 NLP datasets demonstrate that our\nproposed approach outperforms the state-of-the-art methods, including the full\nfinetuning baseline in some cases, despite only tuning 0.035% as many\ntask-specific parameters.\n", "published": "06-03-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "url": "http://arxiv.org/abs/2110.08207v3", "authors": "Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M. Rush", "abstract": "  Large language models have recently been shown to attain reasonable zero-shot\ngeneralization on a diverse set of tasks (Brown et al., 2020). It has been\nhypothesized that this is a consequence of implicit multitask learning in\nlanguage models' pretraining (Radford et al., 2019). Can zero-shot\ngeneralization instead be directly induced by explicit multitask learning? To\ntest this question at scale, we develop a system for easily mapping any natural\nlanguage tasks into a human-readable prompted form. We convert a large set of\nsupervised datasets, each with multiple prompts with diverse wording. These\nprompted datasets allow for benchmarking the ability of a model to perform\ncompletely held-out tasks. We fine-tune a pretrained encoder-decoder model\n(Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a\nwide variety of tasks. The model attains strong zero-shot performance on\nseveral standard datasets, often outperforming models up to 16x its size.\nFurther, our approach attains strong performance on a subset of tasks from the\nBIG-bench benchmark, outperforming models up to 6x its size. All trained models\nare available at https://github.com/bigscience-workshop/t-zero and all prompts\nare available at https://github.com/bigscience-workshop/promptsource.\n", "published": "15-10-2021", "year": "2021", "categories": ["Machine Learning", "Computation and Language"]}, {"title": "Natural Language Reasoning, A Survey", "url": "http://arxiv.org/abs/2303.14725v2", "authors": "Fei Yu, Hongbo Zhang, Prayag Tiwari, Benyou Wang", "abstract": "  This survey paper proposes a clearer view of natural language reasoning in\nthe field of Natural Language Processing (NLP), both conceptually and\npractically. Conceptually, we provide a distinct definition for natural\nlanguage reasoning in NLP, based on both philosophy and NLP scenarios, discuss\nwhat types of tasks require reasoning, and introduce a taxonomy of reasoning.\nPractically, we conduct a comprehensive literature review on natural language\nreasoning in NLP, mainly covering classical logical reasoning, natural language\ninference, multi-hop question answering, and commonsense reasoning. The paper\nalso identifies and views backward reasoning, a powerful paradigm for\nmulti-step reasoning, and introduces defeasible reasoning as one of the most\nimportant future directions in natural language reasoning research. We focus on\nsingle-modality unstructured natural language text, excluding neuro-symbolic\ntechniques and mathematical reasoning.\n", "published": "26-03-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Noisy Channel Language Model Prompting for Few-Shot Text Classification", "url": "http://arxiv.org/abs/2108.04106v3", "authors": "Sewon Min, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer", "abstract": "  We introduce a noisy channel approach for language model prompting in\nfew-shot text classification. Instead of computing the likelihood of the label\ngiven the input (referred as direct models), channel models compute the\nconditional probability of the input given the label, and are thereby required\nto explain every word in the input. We use channel models for recently proposed\nfew-shot learning methods with no or very limited updates to the language model\nparameters, via either in-context demonstration or prompt tuning. Our\nexperiments show that, for both methods, channel models significantly\noutperform their direct counterparts, which we attribute to their stability,\ni.e., lower variance and higher worst-case accuracy. We also present extensive\nablations that provide recommendations for when to use channel prompt tuning\ninstead of other competitive methods (e.g., direct head tuning): channel prompt\ntuning is preferred when the number of training examples is small, labels in\nthe training data are imbalanced, or generalization to unseen labels is\nrequired.\n", "published": "09-08-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "NoteChat: A Dataset of Synthetic Doctor-Patient Conversations\n  Conditioned on Clinical Notes", "url": "http://arxiv.org/abs/2310.15959v2", "authors": "Junda Wang, Zonghai Yao, Zhichao Yang, Huixue Zhou, Rumeng Li, Xun Wang, Yucheng Xu, Hong Yu", "abstract": "  We introduce NoteChat, a novel cooperative multi-agent framework leveraging\nLarge Language Models (LLMs) to generate patient-physician dialogues. NoteChat\nembodies the principle that an ensemble of role-specific LLMs, through\nstructured role-play and strategic prompting, can perform their assigned roles\nmore effectively. The synergy among these role-playing LLMs results in a\ncohesive and efficient dialogue generation. Evaluation on MTS-dialogue, a\nbenchmark dataset for patient-physician dialogues-note pairs, shows that models\ntrained with the augmented synthetic patient-physician dialogues by NoteChat\noutperforms other state-of-the-art models for generating clinical notes. Our\ncomprehensive automatic and human evaluation demonstrates that NoteChat\nsubstantially surpasses state-of-the-art models like ChatGPT and GPT-4 up to\n22.78% by domain experts in generating superior synthetic patient-physician\ndialogues based on clinical notes. NoteChat has the potential to engage\npatients directly and help clinical documentation, a leading cause of physician\nburnout.\n", "published": "24-10-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "OWL: A Large Language Model for IT Operations", "url": "http://arxiv.org/abs/2309.09298v1", "authors": "Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang, Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu, Chao Chen, Dongfeng Zhang, Xu Shi, Tieqiao Zheng, Liangfan Zheng, Bo Zhang, Ke Xu, Zhoujun Li", "abstract": "  With the rapid development of IT operations, it has become increasingly\ncrucial to efficiently manage and analyze large volumes of data for practical\napplications. The techniques of Natural Language Processing (NLP) have shown\nremarkable capabilities for various tasks, including named entity recognition,\nmachine translation and dialogue systems. Recently, Large Language Models\n(LLMs) have achieved significant improvements across various NLP downstream\ntasks. However, there is a lack of specialized LLMs for IT operations. In this\npaper, we introduce the OWL, a large language model trained on our collected\nOWL-Instruct dataset with a wide range of IT-related information, where the\nmixture-of-adapter strategy is proposed to improve the parameter-efficient\ntuning across different domains or tasks. Furthermore, we evaluate the\nperformance of our OWL on the OWL-Bench established by us and open IT-related\nbenchmarks. OWL demonstrates superior performance results on IT tasks, which\noutperforms existing models by significant margins. Moreover, we hope that the\nfindings of our work will provide more insights to revolutionize the techniques\nof IT operations with specialized LLMs.\n", "published": "17-09-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "One Small Step for Generative AI, One Giant Leap for AGI: A Complete\n  Survey on ChatGPT in AIGC Era", "url": "http://arxiv.org/abs/2304.06488v1", "authors": "Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang, Jung Uk Kim, Seong Tae Kim, Jinwoo Choi, Gyeong-Moon Park, Sung-Ho Bae, Lik-Hang Lee, Pan Hui, In So Kweon, Choong Seon Hong", "abstract": "  OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is\ndemonstrated to be one small step for generative AI (GAI), but one giant leap\nfor artificial general intelligence (AGI). Since its official release in\nNovember 2022, ChatGPT has quickly attracted numerous users with extensive\nmedia coverage. Such unprecedented attention has also motivated numerous\nresearchers to investigate ChatGPT from various aspects. According to Google\nscholar, there are more than 500 articles with ChatGPT in their titles or\nmentioning it in their abstracts. Considering this, a review is urgently\nneeded, and our work fills this gap. Overall, this work is the first to survey\nChatGPT with a comprehensive review of its underlying technology, applications,\nand challenges. Moreover, we present an outlook on how ChatGPT might evolve to\nrealize general-purpose AIGC (a.k.a. AI-generated content), which will be a\nsignificant milestone for the development of AGI.\n", "published": "04-04-2023", "year": "2023", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "Open Aspect Target Sentiment Classification with Natural Language\n  Prompts", "url": "http://arxiv.org/abs/2109.03685v1", "authors": "Ronald Seoh, Ian Birle, Mrinal Tak, Haw-Shiuan Chang, Brian Pinette, Alfred Hough", "abstract": "  For many business applications, we often seek to analyze sentiments\nassociated with any arbitrary aspects of commercial products, despite having a\nvery limited amount of labels or even without any labels at all. However,\nexisting aspect target sentiment classification (ATSC) models are not trainable\nif annotated datasets are not available. Even with labeled data, they fall\nshort of reaching satisfactory performance. To address this, we propose simple\napproaches that better solve ATSC with natural language prompts, enabling the\ntask under zero-shot cases and enhancing supervised settings, especially for\nfew-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop\ndomain, our method of reformulating ATSC as an NLI task outperforms supervised\nSOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points.\nMoreover, we demonstrate that our prompts could handle implicitly stated\naspects as well: our models reach about 77% accuracy on detecting sentiments\nfor aspect categories (e.g., food), which do not necessarily appear within the\ntext, even though we trained the models only with explicitly mentioned aspect\nterms (e.g., fajitas) from just 16 reviews - while the accuracy of the\nno-prompt baseline is only around 65%.\n", "published": "08-09-2021", "year": "2021", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "OpenAssistant Conversations -- Democratizing Large Language Model\n  Alignment", "url": "http://arxiv.org/abs/2304.07327v2", "authors": "Andreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, Alexander Mattick", "abstract": "  Aligning large language models (LLMs) with human preferences has proven to\ndrastically improve usability and has driven rapid adoption as demonstrated by\nChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and\nreinforcement learning from human feedback (RLHF) greatly reduce the required\nskill and domain knowledge to effectively harness the capabilities of LLMs,\nincreasing their accessibility and utility across various domains. However,\nstate-of-the-art alignment techniques like RLHF rely on high-quality human\nfeedback data, which is expensive to create and often remains proprietary. In\nan effort to democratize research on large-scale alignment, we release\nOpenAssistant Conversations, a human-generated, human-annotated assistant-style\nconversation corpus consisting of 161,443 messages in 35 different languages,\nannotated with 461,292 quality ratings, resulting in over 10,000 complete and\nfully annotated conversation trees. The corpus is a product of a worldwide\ncrowd-sourcing effort involving over 13,500 volunteers. Models trained on\nOpenAssistant Conversations show consistent improvements on standard benchmarks\nover respective base models. We release our code and data under a fully\npermissive licence.\n", "published": "14-04-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "url": "http://arxiv.org/abs/2309.11235v1", "authors": "Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu", "abstract": "  Nowadays, open-source large language models like LLaMA have emerged. Recent\ndevelopments have incorporated supervised fine-tuning (SFT) and reinforcement\nlearning fine-tuning (RLFT) to align these models with human goals. However,\nSFT methods treat all training data with mixed quality equally, while RLFT\nmethods require high-quality pairwise or ranking-based preference data. In this\nstudy, we present a novel framework, named OpenChat, to advance open-source\nlanguage models with mixed-quality data. Specifically, we consider the general\nSFT training data, consisting of a small amount of expert data mixed with a\nlarge proportion of sub-optimal data, without any preference labels. We propose\nthe C(onditioned)-RLFT, which regards different data sources as coarse-grained\nreward labels and learns a class-conditioned policy to leverage complementary\ndata quality information. Interestingly, the optimal policy in C-RLFT can be\neasily solved through single-stage, RL-free supervised learning, which is\nlightweight and avoids costly human preference labeling. Through extensive\nexperiments on three standard benchmarks, our openchat-13b fine-tuned with\nC-RLFT achieves the highest average performance among all 13b open-source\nlanguage models. Moreover, we use AGIEval to validate the model generalization\nperformance, in which only openchat-13b surpasses the base model. Finally, we\nconduct a series of analyses to shed light on the effectiveness and robustness\nof OpenChat. Our code, data, and models are publicly available at\nhttps://github.com/imoneoi/openchat.\n", "published": "20-09-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Opening A Pandora's Box: Things You Should Know in the Era of Custom\n  GPTs", "url": "http://arxiv.org/abs/2401.00905v1", "authors": "Guanhong Tao, Siyuan Cheng, Zhuo Zhang, Junmin Zhu, Guangyu Shen, Xiangyu Zhang", "abstract": "  The emergence of large language models (LLMs) has significantly accelerated\nthe development of a wide range of applications across various fields. There is\na growing trend in the construction of specialized platforms based on LLMs,\nsuch as the newly introduced custom GPTs by OpenAI. While custom GPTs provide\nvarious functionalities like web browsing and code execution, they also\nintroduce significant security threats. In this paper, we conduct a\ncomprehensive analysis of the security and privacy issues arising from the\ncustom GPT platform. Our systematic examination categorizes potential attack\nscenarios into three threat models based on the role of the malicious actor,\nand identifies critical data exchange channels in custom GPTs. Utilizing the\nSTRIDE threat modeling framework, we identify 26 potential attack vectors, with\n19 being partially or fully validated in real-world settings. Our findings\nemphasize the urgent need for robust security and privacy measures in the\ncustom GPT ecosystem, especially in light of the forthcoming launch of the\nofficial GPT store by OpenAI.\n", "published": "31-12-2023", "year": "2023", "categories": []}, {"title": "Orca 2: Teaching Small Language Models How to Reason", "url": "http://arxiv.org/abs/2311.11045v2", "authors": "Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, Ahmed Awadallah", "abstract": "  Orca 1 learns from rich signals, such as explanation traces, allowing it to\noutperform conventional instruction-tuned models on benchmarks like BigBench\nHard and AGIEval. In Orca 2, we continue exploring how improved training\nsignals can enhance smaller LMs' reasoning abilities. Research on training\nsmall LMs has often relied on imitation learning to replicate the output of\nmore capable models. We contend that excessive emphasis on imitation may\nrestrict the potential of smaller models. We seek to teach small LMs to employ\ndifferent solution strategies for different tasks, potentially different from\nthe one used by the larger model. For example, while larger models might\nprovide a direct answer to a complex task, smaller models may not have the same\ncapacity. In Orca 2, we teach the model various reasoning techniques\n(step-by-step, recall then generate, recall-reason-generate, direct answer,\netc.). More crucially, we aim to help the model learn to determine the most\neffective solution strategy for each task. We evaluate Orca 2 using a\ncomprehensive set of 15 diverse benchmarks (corresponding to approximately 100\ntasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of\nsimilar size and attains performance levels similar or better to those of\nmodels 5-10x larger, as assessed on complex tasks that test advanced reasoning\nabilities in zero-shot settings. make Orca 2 weights publicly available at\naka.ms/orca-lm to support research on the development, evaluation, and\nalignment of smaller LMs\n", "published": "18-11-2023", "year": "2023", "categories": ["Artificial Intelligence"]}, {"title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally\n  Across Scales and Tasks", "url": "http://arxiv.org/abs/2110.07602v3", "authors": "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang", "abstract": "  Prompt tuning, which only tunes continuous prompts with a frozen language\nmodel, substantially reduces per-task storage and memory usage at training.\nHowever, in the context of NLU, prior work reveals that prompt tuning does not\nperform well for normal-sized pretrained models. We also find that existing\nmethods of prompt tuning cannot handle hard sequence labeling tasks, indicating\na lack of universality. We present a novel empirical finding that properly\noptimized prompt tuning can be universally effective across a wide range of\nmodel scales and NLU tasks. It matches the performance of finetuning while\nhaving only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an\nimplementation of Deep Prompt Tuning \\cite{li2021prefix,qin2021learning}\noptimized and adapted for NLU. Given the universality and simplicity of\nP-Tuning v2, we believe it can serve as an alternative to finetuning and a\nstrong baseline for future research.Our code and data are released at\nhttps://github.com/THUDM/P-tuning-v2.\n", "published": "14-10-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen\n  Domains", "url": "http://arxiv.org/abs/2102.12206v4", "authors": "Eyal Ben-David, Nadav Oved, Roi Reichart", "abstract": "  Natural Language Processing algorithms have made incredible progress, but\nthey still struggle when applied to out-of-distribution examples. We address a\nchallenging and underexplored version of this domain adaptation problem, where\nan algorithm is trained on several source domains, and then applied to examples\nfrom unseen domains that are unknown at training time. Particularly, no\nexamples, labeled or unlabeled, or any other knowledge about the target domain\nare available to the algorithm at training time. We present PADA: An\nexample-based autoregressive Prompt learning algorithm for on-the-fly\nAny-Domain Adaptation, based on the T5 language model. Given a test example,\nPADA first generates a unique prompt for it and then, conditioned on this\nprompt, labels the example with respect to the NLP prediction task. PADA is\ntrained to generate a prompt which is a token sequence of unrestricted length,\nconsisting of Domain Related Features (DRFs) that characterize each of the\nsource domains. Intuitively, the generated prompt is a unique signature that\nmaps the test example to a semantic space spanned by the source domains. In\nexperiments with 3 tasks (text classification and sequence tagging), for a\ntotal of 14 multi-source adaptation scenarios, PADA substantially outperforms\nstrong baselines.\n", "published": "24-02-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "PAL: Program-aided Language Models", "url": "http://arxiv.org/abs/2211.10435v2", "authors": "Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig", "abstract": "  Large language models (LLMs) have recently demonstrated an impressive ability\nto perform arithmetic and symbolic reasoning tasks, when provided with a few\nexamples at test time (\"few-shot prompting\"). Much of this success can be\nattributed to prompting methods such as \"chain-of-thought'', which employ LLMs\nfor both understanding the problem description by decomposing it into steps, as\nwell as solving each step of the problem. While LLMs seem to be adept at this\nsort of step-by-step decomposition, LLMs often make logical and arithmetic\nmistakes in the solution part, even when the problem is decomposed correctly.\nIn this paper, we present Program-Aided Language models (PAL): a novel approach\nthat uses the LLM to read natural language problems and generate programs as\nthe intermediate reasoning steps, but offloads the solution step to a runtime\nsuch as a Python interpreter. With PAL, decomposing the natural language\nproblem into runnable steps remains the only learning task for the LLM, while\nsolving is delegated to the interpreter. We demonstrate this synergy between a\nneural LLM and a symbolic interpreter across 13 mathematical, symbolic, and\nalgorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all\nthese natural language reasoning tasks, generating code using an LLM and\nreasoning using a Python interpreter leads to more accurate results than much\nlarger models. For example, PAL using Codex achieves state-of-the-art few-shot\naccuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B\nwhich uses chain-of-thought by absolute 15% top-1. Our code and data are\npublicly available at http://reasonwithpal.com/ .\n", "published": "18-11-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "PDFTriage: Question Answering over Long, Structured Documents", "url": "http://arxiv.org/abs/2309.08872v2", "authors": "Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, David Seunghyun Yoon, Ryan A. Rossi, Franck Dernoncourt", "abstract": "  Large Language Models (LLMs) have issues with document question answering\n(QA) in situations where the document is unable to fit in the small context\nlength of an LLM. To overcome this issue, most existing works focus on\nretrieving the relevant context from the document, representing them as plain\ntext. However, documents such as PDFs, web pages, and presentations are\nnaturally structured with different pages, tables, sections, and so on.\nRepresenting such structured documents as plain text is incongruous with the\nuser's mental model of these documents with rich structure. When a system has\nto query the document for context, this incongruity is brought to the fore, and\nseemingly trivial questions can trip up the QA system. To bridge this\nfundamental gap in handling structured documents, we propose an approach called\nPDFTriage that enables models to retrieve the context based on either structure\nor content. Our experiments demonstrate the effectiveness of the proposed\nPDFTriage-augmented models across several classes of questions where existing\nretrieval-augmented LLMs fail. To facilitate further research on this\nfundamental problem, we release our benchmark dataset consisting of 900+\nhuman-generated questions over 80 structured documents from 10 different\ncategories of question types for document QA. Our code and datasets will be\nreleased soon on Github.\n", "published": "16-09-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning", "url": "http://arxiv.org/abs/2109.04332v3", "authors": "Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang", "abstract": "  Prompts for pre-trained language models (PLMs) have shown remarkable\nperformance by bridging the gap between pre-training tasks and various\ndownstream tasks. Among these methods, prompt tuning, which freezes PLMs and\nonly tunes soft prompts, provides an efficient and effective solution for\nadapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to\nbe fully explored. In our pilot experiments, we find that prompt tuning\nperforms comparably with conventional full-model fine-tuning when downstream\ndata are sufficient, whereas it performs much worse under few-shot learning\nsettings, which may hinder the application of prompt tuning in practice. We\nattribute this low performance to the manner of initializing soft prompts.\nTherefore, in this work, we propose to pre-train prompts by adding soft prompts\ninto the pre-training stage to obtain a better initialization. We name this\nPre-trained Prompt Tuning framework \"PPT\". To ensure the generalization of PPT,\nwe formulate similar classification tasks into a unified task form and\npre-train soft prompts for this unified task. Extensive experiments show that\ntuning pre-trained prompts for downstream tasks can reach or even outperform\nfull-model fine-tuning under both full-data and few-shot settings. Our approach\nis effective and efficient for using large-scale PLMs in practice.\n", "published": "09-09-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "PTR: Prompt Tuning with Rules for Text Classification", "url": "http://arxiv.org/abs/2105.11259v3", "authors": "Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, Maosong Sun", "abstract": "  Fine-tuned pre-trained language models (PLMs) have achieved awesome\nperformance on almost all NLP tasks. By using additional prompts to fine-tune\nPLMs, we can further stimulate the rich knowledge distributed in PLMs to better\nserve downstream tasks. Prompt tuning has achieved promising results on some\nfew-class classification tasks such as sentiment classification and natural\nlanguage inference. However, manually designing lots of language prompts is\ncumbersome and fallible. For those auto-generated prompts, it is also expensive\nand time-consuming to verify their effectiveness in non-few-shot scenarios.\nHence, it is still challenging for prompt tuning to address many-class\nclassification tasks. To this end, we propose prompt tuning with rules (PTR)\nfor many-class text classification and apply logic rules to construct prompts\nwith several sub-prompts. In this way, PTR is able to encode prior knowledge of\neach class into prompt tuning. We conduct experiments on relation\nclassification, a typical and complicated many-class classification task, and\nthe results show that PTR can significantly and consistently outperform\nexisting state-of-the-art baselines. This indicates that PTR is a promising\napproach to take advantage of both human prior knowledge and PLMs for those\ncomplicated classification tasks.\n", "published": "24-05-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "PaLM 2 Technical Report", "url": "http://arxiv.org/abs/2305.10403v3", "authors": "Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, Yonghui Wu", "abstract": "  We introduce PaLM 2, a new state-of-the-art language model that has better\nmultilingual and reasoning capabilities and is more compute-efficient than its\npredecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture\nof objectives. Through extensive evaluations on English and multilingual\nlanguage, and reasoning tasks, we demonstrate that PaLM 2 has significantly\nimproved quality on downstream tasks across different model sizes, while\nsimultaneously exhibiting faster and more efficient inference compared to PaLM.\nThis improved efficiency enables broader deployment while also allowing the\nmodel to respond faster, for a more natural pace of interaction. PaLM 2\ndemonstrates robust reasoning capabilities exemplified by large improvements\nover PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable\nperformance on a suite of responsible AI evaluations, and enables\ninference-time control over toxicity without additional overhead or impact on\nother capabilities. Overall, PaLM 2 achieves state-of-the-art performance\nacross a diverse set of tasks and capabilities.\n  When discussing the PaLM 2 family, it is important to distinguish between\npre-trained models (of various sizes), fine-tuned variants of these models, and\nthe user-facing products that use these models. In particular, user-facing\nproducts typically include additional pre- and post-processing steps.\nAdditionally, the underlying models may evolve over time. Therefore, one should\nnot expect the performance of user-facing products to exactly match the results\nreported in this report.\n", "published": "17-05-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models:\n  A Critical Review and Assessment", "url": "http://arxiv.org/abs/2312.12148v1", "authors": "Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, Fu Lee Wang", "abstract": "  With the continuous growth in the number of parameters of transformer-based\npretrained language models (PLMs), particularly the emergence of large language\nmodels (LLMs) with billions of parameters, many natural language processing\n(NLP) tasks have demonstrated remarkable success. However, the enormous size\nand computational demands of these models pose significant challenges for\nadapting them to specific downstream tasks, especially in environments with\nlimited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers\nan effective solution by reducing the number of fine-tuning parameters and\nmemory usage while achieving comparable performance to full fine-tuning. The\ndemands for fine-tuning PLMs, especially LLMs, have led to a surge in the\ndevelopment of PEFT methods, as depicted in Fig. 1. In this paper, we present a\ncomprehensive and systematic review of PEFT methods for PLMs. We summarize\nthese PEFT methods, discuss their applications, and outline future directions.\nFurthermore, we conduct experiments using several representative PEFT methods\nto better understand their effectiveness in parameter efficiency and memory\nefficiency. By offering insights into the latest advancements and practical\napplications, this survey serves as an invaluable resource for researchers and\npractitioners seeking to navigate the challenges and opportunities presented by\nPEFT in the context of PLMs.\n", "published": "19-12-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Plan, Eliminate, and Track -- Language Models are Good Teachers for\n  Embodied Agents", "url": "http://arxiv.org/abs/2305.02412v2", "authors": "Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom Mitchell, Shrimai Prabhumoye", "abstract": "  Pre-trained large language models (LLMs) capture procedural knowledge about\nthe world. Recent work has leveraged LLM's ability to generate abstract plans\nto simplify challenging control tasks, either by action scoring, or action\nmodeling (fine-tuning). However, the transformer architecture inherits several\nconstraints that make it difficult for the LLM to directly serve as the agent:\ne.g. limited input lengths, fine-tuning inefficiency, bias from pre-training,\nand incompatibility with non-text environments. To maintain compatibility with\na low-level trainable actor, we propose to instead use the knowledge in LLMs to\nsimplify the control problem, rather than solving it. We propose the Plan,\nEliminate, and Track (PET) framework. The Plan module translates a task\ndescription into a list of high-level sub-tasks. The Eliminate module masks out\nirrelevant objects and receptacles from the observation for the current\nsub-task. Finally, the Track module determines whether the agent has\naccomplished each sub-task. On the AlfWorld instruction following benchmark,\nthe PET framework leads to a significant 15% improvement over SOTA for\ngeneralization to human goal specifications.\n", "published": "03-05-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning\n  by Large Language Models", "url": "http://arxiv.org/abs/2305.04091v3", "authors": "Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, Ee-Peng Lim", "abstract": "  Large language models (LLMs) have recently been shown to deliver impressive\nperformance in various NLP tasks. To tackle multi-step reasoning tasks,\nfew-shot chain-of-thought (CoT) prompting includes a few manually crafted\nstep-by-step reasoning demonstrations which enable LLMs to explicitly generate\nreasoning steps and improve their reasoning task accuracy. To eliminate the\nmanual effort, Zero-shot-CoT concatenates the target problem statement with\n\"Let's think step by step\" as an input prompt to LLMs. Despite the success of\nZero-shot-CoT, it still suffers from three pitfalls: calculation errors,\nmissing-step errors, and semantic misunderstanding errors. To address the\nmissing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of\ntwo components: first, devising a plan to divide the entire task into smaller\nsubtasks, and then carrying out the subtasks according to the plan. To address\nthe calculation errors and improve the quality of generated reasoning steps, we\nextend PS prompting with more detailed instructions and derive PS+ prompting.\nWe evaluate our proposed prompting strategy on ten datasets across three\nreasoning problems. The experimental results over GPT-3 show that our proposed\nzero-shot prompting consistently outperforms Zero-shot-CoT across all datasets\nby a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought\nPrompting, and has comparable performance with 8-shot CoT prompting on the math\nreasoning problem. The code can be found at\nhttps://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.\n", "published": "06-05-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "PoKE: A Prompt-based Knowledge Eliciting Approach for Event Argument\n  Extraction", "url": "http://arxiv.org/abs/2109.05190v3", "authors": "Jiaju Lin, Qin Chen", "abstract": "  Eliciting knowledge from pre-trained language models via prompt-based\nlearning has shown great potential in many natural language processing tasks.\nWhereas, the applications for more complex tasks such as event extraction are\nless studied since the design of prompt is not straightforward for the\nstructured event containing various triggers and arguments. % Meanwhile,\ncurrent conditional generation methods employ large encoder-decoder models,\nwhich are costly to train and serve. In this paper, we present a novel\nprompt-based approach, which elicits both the independent and joint knowledge\nabout different events for event argument extraction. The experimental results\non the benchmark ACE2005 dataset show the great advantages of our proposed\napproach. In particular, our approach is superior to the recent advanced\nmethods in both fully-supervised and low-resource scenarios.\n", "published": "11-09-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "PolicyGPT: Automated Analysis of Privacy Policies with Large Language\n  Models", "url": "http://arxiv.org/abs/2309.10238v1", "authors": "Chenhao Tang, Zhengliang Liu, Chong Ma, Zihao Wu, Yiwei Li, Wei Liu, Dajiang Zhu, Quanzheng Li, Xiang Li, Tianming Liu, Lei Fan", "abstract": "  Privacy policies serve as the primary conduit through which online service\nproviders inform users about their data collection and usage procedures.\nHowever, in a bid to be comprehensive and mitigate legal risks, these policy\ndocuments are often quite verbose. In practical use, users tend to click the\nAgree button directly rather than reading them carefully. This practice exposes\nusers to risks of privacy leakage and legal issues. Recently, the advent of\nLarge Language Models (LLM) such as ChatGPT and GPT-4 has opened new\npossibilities for text analysis, especially for lengthy documents like privacy\npolicies. In this study, we investigate a privacy policy text analysis\nframework PolicyGPT based on the LLM. This framework was tested using two\ndatasets. The first dataset comprises of privacy policies from 115 websites,\nwhich were meticulously annotated by legal experts, categorizing each segment\ninto one of 10 classes. The second dataset consists of privacy policies from\n304 popular mobile applications, with each sentence manually annotated and\nclassified into one of another 10 categories. Under zero-shot learning\nconditions, PolicyGPT demonstrated robust performance. For the first dataset,\nit achieved an accuracy rate of 97%, while for the second dataset, it attained\nan 87% accuracy rate, surpassing that of the baseline machine learning and\nneural network models.\n", "published": "19-09-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU", "url": "http://arxiv.org/abs/2312.12456v1", "authors": "Yixin Song, Zeyu Mi, Haotong Xie, Haibo Chen", "abstract": "  This paper introduces PowerInfer, a high-speed Large Language Model (LLM)\ninference engine on a personal computer (PC) equipped with a single\nconsumer-grade GPU. The key underlying the design of PowerInfer is exploiting\nthe high locality inherent in LLM inference, characterized by a power-law\ndistribution in neuron activation. This distribution indicates that a small\nsubset of neurons, termed hot neurons, are consistently activated across\ninputs, while the majority, cold neurons, vary based on specific inputs.\nPowerInfer exploits such an insight to design a GPU-CPU hybrid inference\nengine: hot-activated neurons are preloaded onto the GPU for fast access, while\ncold-activated neurons are computed on the CPU, thus significantly reducing GPU\nmemory demands and CPU-GPU data transfers. PowerInfer further integrates\nadaptive predictors and neuron-aware sparse operators, optimizing the\nefficiency of neuron activation and computational sparsity. Evaluation shows\nthat PowerInfer attains an average token generation rate of 13.20 tokens/s,\nwith a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a\nsingle NVIDIA RTX 4090 GPU, only 18% lower than that achieved by a top-tier\nserver-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69x\nwhile retaining model accuracy.\n", "published": "16-12-2023", "year": "2023", "categories": ["Machine Learning"]}, {"title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods\n  in Natural Language Processing", "url": "http://arxiv.org/abs/2107.13586v1", "authors": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig", "abstract": "  This paper surveys and organizes research works in a new paradigm in natural\nlanguage processing, which we dub \"prompt-based learning\". Unlike traditional\nsupervised learning, which trains a model to take in an input x and predict an\noutput y as P(y|x), prompt-based learning is based on language models that\nmodel the probability of text directly. To use these models to perform\nprediction tasks, the original input x is modified using a template into a\ntextual string prompt x' that has some unfilled slots, and then the language\nmodel is used to probabilistically fill the unfilled information to obtain a\nfinal string x, from which the final output y can be derived. This framework is\npowerful and attractive for a number of reasons: it allows the language model\nto be pre-trained on massive amounts of raw text, and by defining a new\nprompting function the model is able to perform few-shot or even zero-shot\nlearning, adapting to new scenarios with few or no labeled data. In this paper\nwe introduce the basics of this promising paradigm, describe a unified set of\nmathematical notations that can cover a wide variety of existing work, and\norganize existing work along several dimensions, e.g.the choice of pre-trained\nmodels, prompts, and tuning strategies. To make the field more accessible to\ninterested beginners, we not only make a systematic review of existing works\nand a highly structured typology of prompt-based concepts, but also release\nother resources, e.g., a website http://pretrain.nlpedia.ai/ including\nconstantly-updated survey, and paperlist.\n", "published": "28-07-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "url": "http://arxiv.org/abs/2101.00190v1", "authors": "Xiang Lisa Li, Percy Liang", "abstract": "  Fine-tuning is the de facto way to leverage large pretrained language models\nto perform downstream tasks. However, it modifies all the language model\nparameters and therefore necessitates storing a full copy for each task. In\nthis paper, we propose prefix-tuning, a lightweight alternative to fine-tuning\nfor natural language generation tasks, which keeps language model parameters\nfrozen, but optimizes a small continuous task-specific vector (called the\nprefix). Prefix-tuning draws inspiration from prompting, allowing subsequent\ntokens to attend to this prefix as if it were \"virtual tokens\". We apply\nprefix-tuning to GPT-2 for table-to-text generation and to BART for\nsummarization. We find that by learning only 0.1\\% of the parameters,\nprefix-tuning obtains comparable performance in the full data setting,\noutperforms fine-tuning in low-data settings, and extrapolates better to\nexamples with topics unseen during training.\n", "published": "01-01-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "Principled Instructions Are All You Need for Questioning LLaMA-1/2,\n  GPT-3.5/4", "url": "http://arxiv.org/abs/2312.16171v1", "authors": "Sondos Mahmoud Bsharat, Aidar Myrzakhan, Zhiqiang Shen", "abstract": "  This paper introduces 26 guiding principles designed to streamline the\nprocess of querying and prompting large language models. Our goal is to\nsimplify the underlying concepts of formulating questions for various scales of\nlarge language models, examining their abilities, and enhancing user\ncomprehension on the behaviors of different scales of large language models\nwhen feeding into different prompts. Extensive experiments are conducted on\nLLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the\nproposed principles on instructions and prompts design. We hope that this work\nprovides a better guide for researchers working on the prompting of large\nlanguage models. Project page is available at\nhttps://github.com/VILA-Lab/ATLAS.\n", "published": "26-12-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language\n  Models", "url": "http://arxiv.org/abs/2209.11302v1", "authors": "Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg", "abstract": "  Task planning can require defining myriad domain knowledge about the world in\nwhich a robot needs to act. To ameliorate that effort, large language models\n(LLMs) can be used to score potential next actions during task planning, and\neven generate action sequences directly, given an instruction in natural\nlanguage with no additional domain information. However, such methods either\nrequire enumerating all possible next steps for scoring, or generate free-form\ntext that may contain actions not possible on a given robot in its current\ncontext. We present a programmatic LLM prompt structure that enables plan\ngeneration functional across situated environments, robot capabilities, and\ntasks. Our key insight is to prompt the LLM with program-like specifications of\nthe available actions and objects in an environment, as well as with example\nprograms that can be executed. We make concrete recommendations about prompt\nstructure and generation constraints through ablation experiments, demonstrate\nstate of the art success rates in VirtualHome household tasks, and deploy our\nmethod on a physical robot arm for tabletop tasks. Website at\nprogprompt.github.io\n", "published": "22-09-2022", "year": "2022", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning\n  for Numerical Reasoning Tasks", "url": "http://arxiv.org/abs/2211.12588v4", "authors": "Wenhu Chen, Xueguang Ma, Xinyi Wang, William W. Cohen", "abstract": "  Recently, there has been significant progress in teaching language models to\nperform step-by-step reasoning to solve complex numerical reasoning tasks.\nChain-of-thoughts prompting (CoT) is by far the state-of-art method for these\ntasks. CoT uses language models to perform both reasoning and computation in\nthe multi-step `thought' process. To disentangle computation from reasoning, we\npropose `Program of Thoughts' (PoT), which uses language models (mainly Codex)\nto express the reasoning process as a program. The computation is relegated to\nan external computer, which executes the generated programs to derive the\nanswer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,\nTabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)\nfor both few-shot and zero-shot setups. Under both few-shot and zero-shot\nsettings, PoT can show an average performance gain over CoT by around 12\\%\nacross all the evaluated datasets. By combining PoT with self-consistency\ndecoding, we can achieve SoTA performance on all math problem datasets and\nnear-SoTA performance on financial datasets. All of our data and code are\nreleased in Github https://github.com/wenhuchen/Program-of-Thoughts\n", "published": "22-11-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Prompt Programming for Large Language Models: Beyond the Few-Shot\n  Paradigm", "url": "http://arxiv.org/abs/2102.07350v1", "authors": "Laria Reynolds, Kyle McDonell", "abstract": "  Prevailing methods for mapping large generative language models to supervised\ntasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as\na case study, we show that 0-shot prompts can significantly outperform few-shot\nprompts. We suggest that the function of few-shot examples in these cases is\nbetter described as locating an already learned task rather than meta-learning.\nThis analysis motivates rethinking the role of prompts in controlling and\nevaluating powerful language models. In this work, we discuss methods of prompt\nprogramming, emphasizing the usefulness of considering prompts through the lens\nof natural language. We explore techniques for exploiting the capacity of\nnarratives and cultural anchors to encode nuanced intentions and techniques for\nencouraging deconstruction of a problem into components before producing a\nverdict. Informed by this more encompassing theory of prompt programming, we\nalso introduce the idea of a metaprompt that seeds the model to generate its\nown natural language prompts for a range of tasks. Finally, we discuss how\nthese more general methods of interacting with language models can be\nincorporated into existing and future benchmarks and practical applications.\n", "published": "15-02-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Prompt, Condition, and Generate: Classification of Unsupported Claims\n  with In-Context Learning", "url": "http://arxiv.org/abs/2309.10359v1", "authors": "Peter Ebert Christensen, Srishti Yadav, Serge Belongie", "abstract": "  Unsupported and unfalsifiable claims we encounter in our daily lives can\ninfluence our view of the world. Characterizing, summarizing, and -- more\ngenerally -- making sense of such claims, however, can be challenging. In this\nwork, we focus on fine-grained debate topics and formulate a new task of\ndistilling, from such claims, a countable set of narratives. We present a\ncrowdsourced dataset of 12 controversial topics, comprising more than 120k\narguments, claims, and comments from heterogeneous sources, each annotated with\na narrative label. We further investigate how large language models (LLMs) can\nbe used to synthesise claims using In-Context Learning. We find that generated\nclaims with supported evidence can be used to improve the performance of\nnarrative classification models and, additionally, that the same model can\ninfer the stance and aspect using a few training examples. Such a model can be\nuseful in applications which rely on narratives , e.g. fact-checking.\n", "published": "19-09-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "PromptBench: A Unified Library for Evaluation of Large Language Models", "url": "http://arxiv.org/abs/2312.07910v2", "authors": "Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie", "abstract": "  The evaluation of large language models (LLMs) is crucial to assess their\nperformance and mitigate potential security risks. In this paper, we introduce\nPromptBench, a unified library to evaluate LLMs. It consists of several key\ncomponents that are easily used and extended by researchers: prompt\nconstruction, prompt engineering, dataset and model loading, adversarial prompt\nattack, dynamic evaluation protocols, and analysis tools. PromptBench is\ndesigned to be an open, general, and flexible codebase for research purposes\nthat can facilitate original study in creating new benchmarks, deploying\ndownstream applications, and designing new evaluation protocols. The code is\navailable at: https://github.com/microsoft/promptbench and will be continuously\nsupported.\n", "published": "13-12-2023", "year": "2023", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "Promptagator: Few-shot Dense Retrieval From 8 Examples", "url": "http://arxiv.org/abs/2209.11755v1", "authors": "Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, Ming-Wei Chang", "abstract": "  Much recent research on information retrieval has focused on how to transfer\nfrom one task (typically with abundant supervised data) to various other tasks\nwhere supervision is limited, with the implicit assumption that it is possible\nto generalize from one task to all the rest. However, this overlooks the fact\nthat there are many diverse and unique retrieval tasks, each targeting\ndifferent search intents, queries, and search domains. In this paper, we\nsuggest to work on Few-shot Dense Retrieval, a setting where each task comes\nwith a short description and a few examples. To amplify the power of a few\nexamples, we propose Prompt-base Query Generation for Retriever (Promptagator),\nwhich leverages large language models (LLM) as a few-shot query generator, and\ncreates task-specific retrievers based on the generated data. Powered by LLM's\ngeneralization ability, Promptagator makes it possible to create task-specific\nend-to-end retrievers solely based on a few examples {without} using Natural\nQuestions or MS MARCO to train %question generators or dual encoders.\nSurprisingly, LLM prompting with no more than 8 examples allows dual encoders\nto outperform heavily engineered models trained on MS MARCO like ColBERT v2 by\nmore than 1.2 nDCG on average on 11 retrieval sets. Further training\nstandard-size re-rankers using the same generated data yields another 5.0 point\nnDCG improvement. Our studies determine that query generation can be far more\neffective than previously observed, especially when a small amount of\ntask-specific knowledge is given.\n", "published": "23-09-2022", "year": "2022", "categories": ["Computation and Language"]}, {"title": "Pushing Boundaries: Exploring Zero Shot Object Classification with Large\n  Multimodal Models", "url": "http://arxiv.org/abs/2401.00127v1", "authors": "Ashhadul Islam, Md. Rafiul Biswas, Wajdi Zaghouani, Samir Brahim Belhaouari, Zubair Shah", "abstract": "  $ $The synergy of language and vision models has given rise to Large Language\nand Vision Assistant models (LLVAs), designed to engage users in rich\nconversational experiences intertwined with image-based queries. These\ncomprehensive multimodal models seamlessly integrate vision encoders with Large\nLanguage Models (LLMs), expanding their applications in general-purpose\nlanguage and visual comprehension. The advent of Large Multimodal Models (LMMs)\nheralds a new era in Artificial Intelligence (AI) assistance, extending the\nhorizons of AI utilization. This paper takes a unique perspective on LMMs,\nexploring their efficacy in performing image classification tasks using\ntailored prompts designed for specific datasets. We also investigate the LLVAs\nzero-shot learning capabilities. Our study includes a benchmarking analysis\nacross four diverse datasets: MNIST, Cats Vs. Dogs, Hymnoptera (Ants Vs. Bees),\nand an unconventional dataset comprising Pox Vs. Non-Pox skin images. The\nresults of our experiments demonstrate the model's remarkable performance,\nachieving classification accuracies of 85\\%, 100\\%, 77\\%, and 79\\% for the\nrespective datasets without any fine-tuning. To bolster our analysis, we assess\nthe model's performance post fine-tuning for specific tasks. In one instance,\nfine-tuning is conducted over a dataset comprising images of faces of children\nwith and without autism. Prior to fine-tuning, the model demonstrated a test\naccuracy of 55\\%, which significantly improved to 83\\% post fine-tuning. These\nresults, coupled with our prior findings, underscore the transformative\npotential of LLVAs and their versatile applications in real-world scenarios.\n", "published": "30-12-2023", "year": "2023", "categories": []}, {"title": "Pythia: A Suite for Analyzing Large Language Models Across Training and\n  Scaling", "url": "http://arxiv.org/abs/2304.01373v2", "authors": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal", "abstract": "  How do large language models (LLMs) develop and evolve over the course of\ntraining? How do these patterns change as models scale? To answer these\nquestions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on\npublic data seen in the exact same order and ranging in size from 70M to 12B\nparameters. We provide public access to 154 checkpoints for each one of the 16\nmodels, alongside tools to download and reconstruct their exact training\ndataloaders for further study. We intend \\textit{Pythia} to facilitate research\nin many areas, and we present several case studies including novel results in\nmemorization, term frequency effects on few-shot performance, and reducing\ngender bias. We demonstrate that this highly controlled setup can be used to\nyield novel insights toward LLMs and their training dynamics. Trained models,\nanalysis code, training code, and training data can be found at\n\\url{https://github.com/EleutherAI/pythia}.\n", "published": "03-04-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "QLoRA: Efficient Finetuning of Quantized LLMs", "url": "http://arxiv.org/abs/2305.14314v1", "authors": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer", "abstract": "  We present QLoRA, an efficient finetuning approach that reduces memory usage\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\na frozen, 4-bit quantized pretrained language model into Low Rank\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\nsingle GPU. QLoRA introduces a number of innovations to save memory without\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\ninformation theoretically optimal for normally distributed weights (b) double\nquantization to reduce the average memory footprint by quantizing the\nquantization constants, and (c) paged optimziers to manage memory spikes. We\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\ninstruction following and chatbot performance across 8 instruction datasets,\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\nshow that QLoRA finetuning on a small high-quality dataset leads to\nstate-of-the-art results, even when using smaller models than the previous\nSoTA. We provide a detailed analysis of chatbot performance based on both human\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\nalternative to human evaluation. Furthermore, we find that current chatbot\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\nChatGPT. We release all of our models and code, including CUDA kernels for\n4-bit training.\n", "published": "23-05-2023", "year": "2023", "categories": ["Machine Learning"]}, {"title": "Query-Dependent Prompt Evaluation and Optimization with Offline Inverse\n  RL", "url": "http://arxiv.org/abs/2309.06553v3", "authors": "Hao Sun, Alihan H\u00fcy\u00fck, Mihaela van der Schaar", "abstract": "  In this study, we aim to enhance the arithmetic reasoning ability of Large\nLanguage Models (LLMs) through zero-shot prompt optimization. We identify a\npreviously overlooked objective of query dependency in such optimization and\nelucidate two ensuing challenges that impede the successful and economical\ndesign of prompt optimization techniques. One primary issue is the absence of\nan effective method to evaluate prompts during inference when the golden answer\nis unavailable. Concurrently, learning via interactions with the LLMs to\nnavigate the expansive natural language prompting space proves to be\nresource-intensive. To address this, we introduce Prompt-OIRL, which harnesses\noffline inverse reinforcement learning to draw insights from offline prompting\ndemonstration data. Such data exists as by-products when diverse prompts are\nbenchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent\nprompt optimization objective is achieved by first learning an offline reward\nmodel. This model can evaluate any query-prompt pairs without accessing LLMs.\nSubsequently, a best-of-N strategy is deployed to recommend the optimal prompt.\nOur experimental evaluations across various LLM scales and arithmetic reasoning\ndatasets underscore both the efficacy and economic viability of the proposed\napproach.\n", "published": "13-09-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "R-Tuning: Teaching Large Language Models to Refuse Unknown Questions", "url": "http://arxiv.org/abs/2311.09677v1", "authors": "Hanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, Tong Zhang", "abstract": "  Large language models (LLMs) have revolutionized numerous domains with their\nimpressive performance but still face their challenges. A predominant issue is\nthe propensity for these models to generate non-existent facts, a concern\ntermed hallucination. Our research is motivated by the observation that\nprevious instruction tuning methods force the model to complete a sentence no\nmatter whether the model knows the knowledge or not. When the question is out\nof the parametric knowledge, it will try to make up something and fail to\nindicate when it lacks knowledge. In this paper, we present a new approach\ncalled Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized\nby first identifying the knowledge gap between parametric knowledge and the\ninstruction tuning data. Then, we construct the refusal-aware data based on the\nknowledge intersection, to tune LLMs to refrain from responding to questions\nbeyond its parametric knowledge. Experimental results demonstrate this new\ninstruction tuning approach effectively improves a model's ability to answer\nknown questions and refrain from answering unknown questions. Furthermore, when\ntested on out-of-domain datasets, the refusal ability was found to be a\nmeta-skill that could be generalized to other tasks. Further analysis\nsurprisingly finds that learning the uncertainty during training displays a\nbetter ability to estimate uncertainty than uncertainty-based testing. Our code\nwill be released at https://github.com/shizhediao/R-Tuning.\n", "published": "16-11-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "ReAct: Synergizing Reasoning and Acting in Language Models", "url": "http://arxiv.org/abs/2210.03629v3", "authors": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao", "abstract": "  While large language models (LLMs) have demonstrated impressive capabilities\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\naction plan generation) have primarily been studied as separate topics. In this\npaper, we explore the use of LLMs to generate both reasoning traces and\ntask-specific actions in an interleaved manner, allowing for greater synergy\nbetween the two: reasoning traces help the model induce, track, and update\naction plans as well as handle exceptions, while actions allow it to interface\nwith external sources, such as knowledge bases or environments, to gather\nadditional information. We apply our approach, named ReAct, to a diverse set of\nlanguage and decision making tasks and demonstrate its effectiveness over\nstate-of-the-art baselines, as well as improved human interpretability and\ntrustworthiness over methods without reasoning or acting components.\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\nReAct overcomes issues of hallucination and error propagation prevalent in\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\ngenerates human-like task-solving trajectories that are more interpretable than\nbaselines without reasoning traces. On two interactive decision making\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\nreinforcement learning methods by an absolute success rate of 34% and 10%\nrespectively, while being prompted with only one or two in-context examples.\nProject site with code: https://react-lm.github.io\n", "published": "06-10-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Read and Reap the Rewards: Learning to Play Atari with the Help of\n  Instruction Manuals", "url": "http://arxiv.org/abs/2302.04449v3", "authors": "Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, Tom M. Mitchell", "abstract": "  High sample complexity has long been a challenge for RL. On the other hand,\nhumans learn to perform tasks not only from interaction or demonstrations, but\nalso by reading unstructured text documents, e.g., instruction manuals.\nInstruction manuals and wiki pages are among the most abundant data that could\ninform agents of valuable features and policies or task-specific environmental\ndynamics and reward structures. Therefore, we hypothesize that the ability to\nutilize human-written instruction manuals to assist learning policies for\nspecific tasks should lead to a more efficient and better-performing agent. We\npropose the Read and Reward framework. Read and Reward speeds up RL algorithms\non Atari games by reading manuals released by the Atari game developers. Our\nframework consists of a QA Extraction module that extracts and summarizes\nrelevant information from the manual and a Reasoning module that evaluates\nobject-agent interactions based on information from the manual. An auxiliary\nreward is then provided to a standard A2C RL agent, when interaction is\ndetected. Experimentally, various RL algorithms obtain significant improvement\nin performance and training speed when assisted by our design.\n", "published": "09-02-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "Reasoning Implicit Sentiment with Chain-of-Thought Prompting", "url": "http://arxiv.org/abs/2305.11255v4", "authors": "Hao Fei, Bobo Li, Qian Liu, Lidong Bing, Fei Li, Tat-Seng Chua", "abstract": "  While sentiment analysis systems try to determine the sentiment polarities of\ngiven targets based on the key opinion expressions in input texts, in implicit\nsentiment analysis (ISA) the opinion cues come in an implicit and obscure\nmanner. Thus detecting implicit sentiment requires the common-sense and\nmulti-hop reasoning ability to infer the latent intent of opinion. Inspired by\nthe recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop\nReasoning (THOR) CoT framework to mimic the human-like reasoning process for\nISA. We design a three-step prompting principle for THOR to step-by-step induce\nthe implicit aspect, opinion, and finally the sentiment polarity. Our\nTHOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on\nsupervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%\nF1 on zero-shot setting. Our code is open at\nhttps://github.com/scofield7419/THOR-ISA.\n", "published": "18-05-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Reasoning with Language Model Prompting: A Survey", "url": "http://arxiv.org/abs/2212.09597v8", "authors": "Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen", "abstract": "  Reasoning, as an essential ability for complex problem-solving, can provide\nback-end support for various real-world applications, such as medical\ndiagnosis, negotiation, etc. This paper provides a comprehensive survey of\ncutting-edge research on reasoning with language model prompting. We introduce\nresearch works with comparisons and summaries and provide systematic resources\nto help beginners. We also discuss the potential reasons for emerging such\nreasoning abilities and highlight future research directions. Resources are\navailable at https://github.com/zjunlp/Prompt4ReasoningPapers (updated\nperiodically).\n", "published": "19-12-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Reasoning with Language Model is Planning with World Model", "url": "http://arxiv.org/abs/2305.14992v2", "authors": "Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu", "abstract": "  Large language models (LLMs) have shown remarkable reasoning capabilities,\nespecially when prompted to generate intermediate reasoning steps (e.g.,\nChain-of-Thought, CoT). However, LLMs can still struggle with problems that are\neasy for humans, such as generating action plans for executing tasks in a given\nenvironment, or performing complex math, logical, and commonsense reasoning.\nThe deficiency stems from the key fact that LLMs lack an internal\n$\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment\nstatus, intermediate variable values) and simulate long-term outcomes of\nactions. This prevents LLMs from performing deliberate planning akin to human\nbrains, which involves exploring alternative reasoning paths, anticipating\nfuture states and rewards, and iteratively refining existing reasoning steps.\nTo overcome the limitations, we propose a new LLM reasoning framework,\n$\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning\n$\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning\nagent, and incorporates a principled planning algorithm (based on Monto Carlo\nTree Search) for strategic exploration in the vast reasoning space. During\nreasoning, the LLM (as agent) incrementally builds a reasoning tree under the\nguidance of the LLM (as world model) and task-specific rewards, and obtains a\nhigh-reward reasoning path efficiently with a proper balance between\nexploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of\nchallenging reasoning problems including plan generation, math reasoning, and\nlogical inference. Empirical results on these tasks demonstrate the superiority\nof RAP over various strong baselines, including CoT and least-to-most prompting\nwith self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%\nrelative improvement in a plan generation setting.\n", "published": "24-05-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context\n  Reasoning with Language Models", "url": "http://arxiv.org/abs/2306.06891v1", "authors": "Soochan Lee, Gunhee Kim", "abstract": "  Generating intermediate steps, or Chain of Thought (CoT), is an effective way\nto significantly improve language models' (LM) multi-step reasoning capability.\nHowever, the CoT lengths can grow rapidly with the problem complexity, easily\nexceeding the maximum context size. Instead of increasing the context limit,\nwhich has already been heavily investigated, we explore an orthogonal\ndirection: making LMs divide a problem into multiple contexts. We propose a new\ninference framework, called Recursion of Thought (RoT), which introduces\nseveral special tokens that the models can output to trigger context-related\noperations. Extensive experiments with multiple architectures including GPT-3\nshow that RoT dramatically improves LMs' inference capability to solve\nproblems, whose solution consists of hundreds of thousands of tokens.\n", "published": "12-06-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Reframing Instructional Prompts to GPTk's Language", "url": "http://arxiv.org/abs/2109.07830v3", "authors": "Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, Hannaneh Hajishirzi", "abstract": "  What kinds of instructional prompts are easier to follow for Language Models\n(LMs)? We study this question by conducting extensive empirical analysis that\nshed light on important features of successful instructional prompts.\nSpecifically, we study several classes of reframing techniques for manual\nreformulation of prompts into more effective ones. Some examples include\ndecomposing a complex task instruction into multiple simpler tasks or itemizing\ninstructions into sequential steps. Our experiments compare the zero-shot and\nfew-shot performance of LMs prompted with reframed instructions on 12 NLP tasks\nacross 6 categories. Compared with original instructions, our reframed\ninstructions lead to significant improvements across LMs with different sizes.\nFor example, the same reframed prompts boost few-shot performance of\nGPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all\ntasks. Furthermore, reframed instructions reduce the number of examples\nrequired to prompt LMs in the few-shot setting. We hope these\nempirically-driven techniques will pave the way towards more effective future\nprompting algorithms.\n", "published": "16-09-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Rephrase and Respond: Let Large Language Models Ask Better Questions for\n  Themselves", "url": "http://arxiv.org/abs/2311.04205v1", "authors": "Yihe Deng, Weitong Zhang, Zixiang Chen, Quanquan Gu", "abstract": "  Misunderstandings arise not only in interpersonal communication but also\nbetween humans and Large Language Models (LLMs). Such discrepancies can make\nLLMs interpret seemingly unambiguous questions in unexpected ways, yielding\nincorrect responses. While it is widely acknowledged that the quality of a\nprompt, such as a question, significantly impacts the quality of the response\nprovided by LLMs, a systematic method for crafting questions that LLMs can\nbetter comprehend is still underdeveloped. In this paper, we present a method\nnamed `Rephrase and Respond' (RaR), which allows LLMs to rephrase and expand\nquestions posed by humans and provide responses in a single prompt. This\napproach serves as a simple yet effective prompting method for improving\nperformance. We also introduce a two-step variant of RaR, where a rephrasing\nLLM first rephrases the question and then passes the original and rephrased\nquestions together to a different responding LLM. This facilitates the\neffective utilization of rephrased questions generated by one LLM with another.\nOur experiments demonstrate that our methods significantly improve the\nperformance of different models across a wide range to tasks. We further\nprovide a comprehensive comparison between RaR and the popular Chain-of-Thought\n(CoT) methods, both theoretically and empirically. We show that RaR is\ncomplementary to CoT and can be combined with CoT to achieve even better\nperformance. Our work not only contributes to enhancing LLM performance\nefficiently and effectively but also sheds light on a fair evaluation of LLM\ncapabilities. Data and codes are available at\nhttps://github.com/uclaml/Rephrase-and-Respond.\n", "published": "07-11-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for\n  Paper Reviewing", "url": "http://arxiv.org/abs/2306.00622v1", "authors": "Ryan Liu, Nihar B. Shah", "abstract": "  Given the rapid ascent of large language models (LLMs), we study the\nquestion: (How) can large language models help in reviewing of scientific\npapers or proposals? We first conduct some pilot studies where we find that (i)\nGPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly,\nOpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to\nidentify errors) outperforms prompting to simply write a review. With these\ninsights, we study the use of LLMs (specifically, GPT-4) for three tasks:\n  1. Identifying errors: We construct 13 short computer science papers each\nwith a deliberately inserted error, and ask the LLM to check for the\ncorrectness of these papers. We observe that the LLM finds errors in 7 of them,\nspanning both mathematical and conceptual errors.\n  2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist\nquestions in the respective sections of 15 NeurIPS 2022 papers. We find that\nacross 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy.\n  3. Choosing the \"better\" paper: We generate 10 pairs of abstracts,\ndeliberately designing each pair in such a way that one abstract was clearly\nsuperior than the other. The LLM, however, struggled to discern these\nrelatively straightforward distinctions accurately, committing errors in its\nevaluations for 6 out of the 10 pairs.\n  Based on these experiments, we think that LLMs have a promising use as\nreviewing assistants for specific reviewing tasks, but not (yet) for complete\nevaluations of papers or proposals.\n", "published": "01-06-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language\n  Model Systems", "url": "http://arxiv.org/abs/2401.05778v1", "authors": "Tianyu Cui, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng, Yunpeng Liu, Qinglin Zhang, Ziyi Qiu, Peiyang Li, Zhixing Tan, Junwu Xiong, Xinyu Kong, Zujie Wen, Ke Xu, Qi Li", "abstract": "  Large language models (LLMs) have strong capabilities in solving diverse\nnatural language processing tasks. However, the safety and security issues of\nLLM systems have become the major obstacle to their widespread application.\nMany studies have extensively investigated risks in LLM systems and developed\nthe corresponding mitigation strategies. Leading-edge enterprises such as\nOpenAI, Google, Meta, and Anthropic have also made lots of efforts on\nresponsible LLMs. Therefore, there is a growing need to organize the existing\nstudies and establish comprehensive taxonomies for the community. In this\npaper, we delve into four essential modules of an LLM system, including an\ninput module for receiving prompts, a language model trained on extensive\ncorpora, a toolchain module for development and deployment, and an output\nmodule for exporting LLM-generated content. Based on this, we propose a\ncomprehensive taxonomy, which systematically analyzes potential risks\nassociated with each module of an LLM system and discusses the corresponding\nmitigation strategies. Furthermore, we review prevalent benchmarks, aiming to\nfacilitate the risk assessment of LLM systems. We hope that this paper can help\nLLM participants embrace a systematic perspective to build their responsible\nLLM systems.\n", "published": "11-01-2024", "year": "2024", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "S-LoRA: Serving Thousands of Concurrent LoRA Adapters", "url": "http://arxiv.org/abs/2311.03285v2", "authors": "Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, Ion Stoica", "abstract": "  The \"pretrain-then-finetune\" paradigm is commonly adopted in the deployment\nof large language models. Low-Rank Adaptation (LoRA), a parameter-efficient\nfine-tuning method, is often employed to adapt a base model to a multitude of\ntasks, resulting in a substantial collection of LoRA adapters derived from one\nbase model. We observe that this paradigm presents significant opportunities\nfor batched inference during serving. To capitalize on these opportunities, we\npresent S-LoRA, a system designed for the scalable serving of many LoRA\nadapters. S-LoRA stores all adapters in the main memory and fetches the\nadapters used by the currently running queries to the GPU memory. To\nefficiently use the GPU memory and reduce fragmentation, S-LoRA proposes\nUnified Paging. Unified Paging uses a unified memory pool to manage dynamic\nadapter weights with different ranks and KV cache tensors with varying sequence\nlengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and\nhighly optimized custom CUDA kernels for heterogeneous batching of LoRA\ncomputation. Collectively, these features enable S-LoRA to serve thousands of\nLoRA adapters on a single GPU or across multiple GPUs with a small overhead.\nCompared to state-of-the-art libraries such as HuggingFace PEFT and vLLM (with\nnaive support of LoRA serving), S-LoRA can improve the throughput by up to 4\ntimes and increase the number of served adapters by several orders of\nmagnitude. As a result, S-LoRA enables scalable serving of many task-specific\nfine-tuned models and offers the potential for large-scale customized\nfine-tuning services. The code is available at https://github.com/S-LoRA/S-LoRA\n", "published": "06-11-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence"]}, {"title": "SM70: A Large Language Model for Medical Devices", "url": "http://arxiv.org/abs/2312.06974v1", "authors": "Anubhav Bhatti, Surajsinh Parmar, San Lee", "abstract": "  We are introducing SM70, a 70 billion-parameter Large Language Model that is\nspecifically designed for SpassMed's medical devices under the brand name\n'JEE1' (pronounced as G1 and means 'Life'). This large language model provides\nmore accurate and safe responses to medical-domain questions. To fine-tune\nSM70, we used around 800K data entries from the publicly available dataset\nMedAlpaca. The Llama2 70B open-sourced model served as the foundation for SM70,\nand we employed the QLoRA technique for fine-tuning. The evaluation is\nconducted across three benchmark datasets - MEDQA - USMLE, PUBMEDQA, and USMLE\n- each representing a unique aspect of medical knowledge and reasoning. The\nperformance of SM70 is contrasted with other notable LLMs, including Llama2\n70B, Clinical Camel 70 (CC70), GPT 3.5, GPT 4, and Med-Palm, to provide a\ncomparative understanding of its capabilities within the medical domain. Our\nresults indicate that SM70 outperforms several established models in these\ndatasets, showcasing its proficiency in handling a range of medical queries,\nfrom fact-based questions derived from PubMed abstracts to complex clinical\ndecision-making scenarios. The robust performance of SM70, particularly in the\nUSMLE and PUBMEDQA datasets, suggests its potential as an effective tool in\nclinical decision support and medical information retrieval. Despite its\npromising results, the paper also acknowledges the areas where SM70 lags behind\nthe most advanced model, GPT 4, thereby highlighting the need for further\ndevelopment, especially in tasks demanding extensive medical knowledge and\nintricate reasoning.\n", "published": "12-12-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "SPRING: Studying the Paper and Reasoning to Play Games", "url": "http://arxiv.org/abs/2305.15486v3", "authors": "Yue Wu, Shrimai Prabhumoye, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Tom Mitchell, Yuanzhi Li", "abstract": "  Open-world survival games pose significant challenges for AI algorithms due\nto their multi-tasking, deep exploration, and goal prioritization requirements.\nDespite reinforcement learning (RL) being popular for solving games, its high\nsample complexity limits its effectiveness in complex open-world games like\nCrafter or Minecraft. We propose a novel approach, SPRING, to read the game's\noriginal academic paper and use the knowledge learned to reason and play the\ngame through a large language model (LLM). Prompted with the LaTeX source as\ngame context and a description of the agent's current observation, our SPRING\nframework employs a directed acyclic graph (DAG) with game-related questions as\nnodes and dependencies as edges. We identify the optimal action to take in the\nenvironment by traversing the DAG and calculating LLM responses for each node\nin topological order, with the LLM's answer to final node directly translating\nto environment actions. In our experiments, we study the quality of in-context\n\"reasoning\" induced by different forms of prompts under the setting of the\nCrafter open-world environment. Our experiments suggest that LLMs, when\nprompted with consistent chain-of-thought, have great potential in completing\nsophisticated high-level trajectories. Quantitatively, SPRING with GPT-4\noutperforms all state-of-the-art RL baselines, trained for 1M steps, without\nany training. Finally, we show the potential of games as a test bed for LLMs.\n", "published": "24-05-2023", "year": "2023", "categories": ["Artificial Intelligence", "Machine Learning"]}, {"title": "Selection-Inference: Exploiting Large Language Models for Interpretable\n  Logical Reasoning", "url": "http://arxiv.org/abs/2205.09712v1", "authors": "Antonia Creswell, Murray Shanahan, Irina Higgins", "abstract": "  Large language models (LLMs) have been shown to be capable of impressive\nfew-shot generalisation to new tasks. However, they still tend to perform\npoorly on multi-step logical reasoning problems. Here we carry out a\ncomprehensive evaluation of LLMs on 50 tasks that probe different aspects of\nlogical reasoning. We show that language models tend to perform fairly well at\nsingle step inference or entailment tasks, but struggle to chain together\nmultiple reasoning steps to solve more complex problems. In light of this, we\npropose a Selection-Inference (SI) framework that exploits pre-trained LLMs as\ngeneral processing modules, and alternates between selection and inference to\ngenerate a series of interpretable, casual reasoning steps leading to the final\nanswer. We show that a 7B parameter LLM used within the SI framework in a\n5-shot generalisation setting, with no fine-tuning, yields a performance\nimprovement of over 100% compared to an equivalent vanilla baseline on a suite\nof 10 logical reasoning tasks. The same model in the same setting even\noutperforms a significantly larger 280B parameter baseline on the same suite of\ntasks. Moreover, answers produced by the SI framework are accompanied by a\ncausal natural-language-based reasoning trace, which has important implications\nfor the safety and trustworthiness of the system.\n", "published": "19-05-2022", "year": "2022", "categories": ["Artificial Intelligence", "Computation and Language"]}, {"title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "url": "http://arxiv.org/abs/2203.11171v4", "authors": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou", "abstract": "  Chain-of-thought prompting combined with pre-trained large language models\nhas achieved encouraging results on complex reasoning tasks. In this paper, we\npropose a new decoding strategy, self-consistency, to replace the naive greedy\ndecoding used in chain-of-thought prompting. It first samples a diverse set of\nreasoning paths instead of only taking the greedy one, and then selects the\nmost consistent answer by marginalizing out the sampled reasoning paths.\nSelf-consistency leverages the intuition that a complex reasoning problem\ntypically admits multiple different ways of thinking leading to its unique\ncorrect answer. Our extensive empirical evaluation shows that self-consistency\nboosts the performance of chain-of-thought prompting with a striking margin on\na range of popular arithmetic and commonsense reasoning benchmarks, including\nGSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and\nARC-challenge (+3.9%).\n", "published": "21-03-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "url": "http://arxiv.org/abs/2212.10560v2", "authors": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi", "abstract": "  Large \"instruction-tuned\" language models (i.e., finetuned to respond to\ninstructions) have demonstrated a remarkable ability to generalize zero-shot to\nnew tasks. Nevertheless, they depend heavily on human-written instruction data\nthat is often limited in quantity, diversity, and creativity, therefore\nhindering the generality of the tuned model. We introduce Self-Instruct, a\nframework for improving the instruction-following capabilities of pretrained\nlanguage models by bootstrapping off their own generations. Our pipeline\ngenerates instructions, input, and output samples from a language model, then\nfilters invalid or similar ones before using them to finetune the original\nmodel. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute\nimprovement over the original model on Super-NaturalInstructions, on par with\nthe performance of InstructGPT-001, which was trained with private user data\nand human annotations. For further evaluation, we curate a set of\nexpert-written instructions for novel tasks, and show through human evaluation\nthat tuning GPT3 with Self-Instruct outperforms using existing public\ninstruction datasets by a large margin, leaving only a 5% absolute gap behind\nInstructGPT-001. Self-Instruct provides an almost annotation-free method for\naligning pre-trained language models with instructions, and we release our\nlarge synthetic dataset to facilitate future studies on instruction tuning. Our\ncode and data are available at https://github.com/yizhongw/self-instruct.\n", "published": "20-12-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Self-RAG: Learning to Retrieve, Generate, and Critique through\n  Self-Reflection", "url": "http://arxiv.org/abs/2310.11511v1", "authors": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi", "abstract": "  Despite their remarkable capabilities, large language models (LLMs) often\nproduce responses containing factual inaccuracies due to their sole reliance on\nthe parametric knowledge they encapsulate. Retrieval-Augmented Generation\n(RAG), an ad hoc approach that augments LMs with retrieval of relevant\nknowledge, decreases such issues. However, indiscriminately retrieving and\nincorporating a fixed number of retrieved passages, regardless of whether\nretrieval is necessary, or passages are relevant, diminishes LM versatility or\ncan lead to unhelpful response generation. We introduce a new framework called\nSelf-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's\nquality and factuality through retrieval and self-reflection. Our framework\ntrains a single arbitrary LM that adaptively retrieves passages on-demand, and\ngenerates and reflects on retrieved passages and its own generations using\nspecial tokens, called reflection tokens. Generating reflection tokens makes\nthe LM controllable during the inference phase, enabling it to tailor its\nbehavior to diverse task requirements. Experiments show that Self-RAG (7B and\n13B parameters) significantly outperforms state-of-the-art LLMs and\nretrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\nreasoning and fact verification tasks, and it shows significant gains in\nimproving factuality and citation accuracy for long-form generations relative\nto these models.\n", "published": "17-10-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based\n  Sentiment Analysis", "url": "http://arxiv.org/abs/2109.08306v1", "authors": "Chengxi Li, Feiyu Gao, Jiajun Bu, Lu Xu, Xiang Chen, Yu Gu, Zirui Shao, Qi Zheng, Ningyu Zhang, Yongpan Wang, Zhi Yu", "abstract": "  Aspect-based sentiment analysis (ABSA) is an emerging fine-grained sentiment\nanalysis task that aims to extract aspects, classify corresponding sentiment\npolarities and find opinions as the causes of sentiment. The latest research\ntends to solve the ABSA task in a unified way with end-to-end frameworks. Yet,\nthese frameworks get fine-tuned from downstream tasks without any task-adaptive\nmodification. Specifically, they do not use task-related knowledge well or\nexplicitly model relations between aspect and opinion terms, hindering them\nfrom better performance. In this paper, we propose SentiPrompt to use sentiment\nknowledge enhanced prompts to tune the language model in the unified framework.\nWe inject sentiment knowledge regarding aspects, opinions, and polarities into\nprompt and explicitly model term relations via constructing consistency and\npolarity judgment templates from the ground truth triplets. Experimental\nresults demonstrate that our approach can outperform strong baselines on\nTriplet Extraction, Pair Extraction, and Aspect Term Extraction with Sentiment\nClassification by a notable margin.\n", "published": "17-09-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Seven Failure Points When Engineering a Retrieval Augmented Generation\n  System", "url": "http://arxiv.org/abs/2401.05856v1", "authors": "Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek", "abstract": "  Software engineers are increasingly adding semantic search capabilities to\napplications using a strategy known as Retrieval Augmented Generation (RAG). A\nRAG system involves finding documents that semantically match a query and then\npassing the documents to a large language model (LLM) such as ChatGPT to\nextract the right answer using an LLM. RAG systems aim to: a) reduce the\nproblem of hallucinated responses from LLMs, b) link sources/references to\ngenerated responses, and c) remove the need for annotating documents with\nmeta-data. However, RAG systems suffer from limitations inherent to information\nretrieval systems and from reliance on LLMs. In this paper, we present an\nexperience report on the failure points of RAG systems from three case studies\nfrom separate domains: research, education, and biomedical. We share the\nlessons learned and present 7 failure points to consider when designing a RAG\nsystem. The two key takeaways arising from our work are: 1) validation of a RAG\nsystem is only feasible during operation, and 2) the robustness of a RAG system\nevolves rather than designed in at the start. We conclude with a list of\npotential research directions on RAG systems for the software engineering\ncommunity.\n", "published": "11-01-2024", "year": "2024", "categories": []}, {"title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety\n  Training", "url": "http://arxiv.org/abs/2401.05566v2", "authors": "Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, S\u00f6ren Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, Ethan Perez", "abstract": "  Humans are capable of strategically deceptive behavior: behaving helpfully in\nmost situations, but then behaving very differently in order to pursue\nalternative objectives when given the opportunity. If an AI system learned such\na deceptive strategy, could we detect it and remove it using current\nstate-of-the-art safety training techniques? To study this question, we\nconstruct proof-of-concept examples of deceptive behavior in large language\nmodels (LLMs). For example, we train models that write secure code when the\nprompt states that the year is 2023, but insert exploitable code when the\nstated year is 2024. We find that such backdoor behavior can be made\npersistent, so that it is not removed by standard safety training techniques,\nincluding supervised fine-tuning, reinforcement learning, and adversarial\ntraining (eliciting unsafe behavior and then training to remove it). The\nbackdoor behavior is most persistent in the largest models and in models\ntrained to produce chain-of-thought reasoning about deceiving the training\nprocess, with the persistence remaining even when the chain-of-thought is\ndistilled away. Furthermore, rather than removing backdoors, we find that\nadversarial training can teach models to better recognize their backdoor\ntriggers, effectively hiding the unsafe behavior. Our results suggest that,\nonce a model exhibits deceptive behavior, standard techniques could fail to\nremove such deception and create a false impression of safety.\n", "published": "10-01-2024", "year": "2024", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "SmartPlay: A Benchmark for LLMs as Intelligent Agents", "url": "http://arxiv.org/abs/2310.01557v3", "authors": "Yue Wu, Xuan Tang, Tom M. Mitchell, Yuanzhi Li", "abstract": "  Recent large language models (LLMs) have demonstrated great potential toward\nintelligent agents and next-gen automation, but there currently lacks a\nsystematic benchmark for evaluating LLMs' abilities as agents. We introduce\nSmartPlay: both a challenging benchmark and a methodology for evaluating LLMs\nas agents. SmartPlay consists of 6 different games, including\nRock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique\nsetting, providing up to 20 evaluation settings and infinite environment\nvariations. Each game in SmartPlay uniquely challenges a subset of 9 important\ncapabilities of an intelligent LLM agent, including reasoning with object\ndependencies, planning ahead, spatial reasoning, learning from history, and\nunderstanding randomness. The distinction between the set of capabilities each\ngame test allows us to analyze each capability separately. SmartPlay serves not\nonly as a rigorous testing ground for evaluating the overall performance of LLM\nagents but also as a road-map for identifying gaps in current methodologies. We\nrelease our benchmark at github.com/microsoft/SmartPlay\n", "published": "02-10-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence"]}, {"title": "Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon", "url": "http://arxiv.org/abs/2401.03462v1", "authors": "Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, Zhicheng Dou", "abstract": "  The utilization of long contexts poses a big challenge for large language\nmodels due to their limited context window length. Although the context window\ncan be extended through fine-tuning, it will result in a considerable cost at\nboth training and inference time, and exert an unfavorable impact to the LLM's\noriginal capabilities. In this work, we propose Activation Beacon, which\ncondenses LLM's raw activations into more compact forms such that it can\nperceive a much longer context with a limited context window. Activation Beacon\nis introduced as a plug-and-play module for the LLM. It fully preserves the\nLLM's original capability on short contexts while extending the new capability\non processing longer contexts. Besides, it works with short sliding windows to\nprocess the long context, which achieves a competitive memory and time\nefficiency in both training and inference. Activation Beacon is learned by the\nauto-regression task conditioned on a mixture of beacons with diversified\ncondensing ratios. Thanks to such a treatment, it can be efficiently trained\npurely with short-sequence data in just 10K steps, which consumes less than 9\nhours on a single 8xA800 GPU machine. The experimental studies show that\nActivation Beacon is able to extend Llama-2-7B's context length by $\\times100$\ntimes (from 4K to 400K), meanwhile achieving a superior result on both\nlong-context generation and understanding tasks. Our model and code will be\navailable at the BGE repository.\n", "published": "07-01-2024", "year": "2024", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", "url": "http://arxiv.org/abs/2204.00598v2", "authors": "Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Pete Florence", "abstract": "  Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities\ndepending on the domain of data they are trained on. While these domains are\ngeneric, they may only barely overlap. For example, visual-language models\n(VLMs) are trained on Internet-scale image captions, but large language models\n(LMs) are further trained on Internet-scale text with no images (e.g.,\nspreadsheets, SAT questions, code). As a result, these models store different\nforms of commonsense knowledge across different domains. In this work, we show\nthat this diversity is symbiotic, and can be leveraged through Socratic Models\n(SMs): a modular framework in which multiple pretrained models may be composed\nzero-shot i.e., via multimodal-informed prompting, to exchange information with\neach other and capture new multimodal capabilities, without requiring\nfinetuning. With minimal engineering, SMs are not only competitive with\nstate-of-the-art zero-shot image captioning and video-to-text retrieval, but\nalso enable new applications such as (i) answering free-form questions about\negocentric video, (ii) engaging in multimodal assistive dialogue with people\n(e.g., for cooking recipes) by interfacing with external APIs and databases\n(e.g., web search), and (iii) robot perception and planning.\n", "published": "01-04-2022", "year": "2022", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4", "url": "http://arxiv.org/abs/2303.12712v5", "authors": "S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang", "abstract": "  Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.\n", "published": "22-03-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot", "url": "http://arxiv.org/abs/2301.00774v3", "authors": "Elias Frantar, Dan Alistarh", "abstract": "  We show for the first time that large-scale generative pretrained transformer\n(GPT) family models can be pruned to at least 50% sparsity in one-shot, without\nany retraining, at minimal loss of accuracy. This is achieved via a new pruning\nmethod called SparseGPT, specifically designed to work efficiently and\naccurately on massive GPT-family models. We can execute SparseGPT on the\nlargest available open-source models, OPT-175B and BLOOM-176B, in under 4.5\nhours, and can reach 60% unstructured sparsity with negligible increase in\nperplexity: remarkably, more than 100 billion weights from these models can be\nignored at inference time. SparseGPT generalizes to semi-structured (2:4 and\n4:8) patterns, and is compatible with weight quantization approaches. The code\nis available at: https://github.com/IST-DASLab/sparsegpt.\n", "published": "02-01-2023", "year": "2023", "categories": ["Machine Learning"]}, {"title": "Supervised Knowledge Makes Large Language Models Better In-context\n  Learners", "url": "http://arxiv.org/abs/2312.15918v1", "authors": "Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, Yue Zhang", "abstract": "  Large Language Models (LLMs) exhibit emerging in-context learning abilities\nthrough prompt engineering. The recent progress in large-scale generative\nmodels has further expanded their use in real-world language applications.\nHowever, the critical challenge of improving the generalizability and\nfactuality of LLMs in natural language understanding and question answering\nremains under-explored. While previous in-context learning research has focused\non enhancing models to adhere to users' specific instructions and quality\nexpectations, and to avoid undesired outputs, little to no work has explored\nthe use of task-Specific fine-tuned Language Models (SLMs) to improve LLMs'\nin-context learning during the inference stage. Our primary contribution is the\nestablishment of a simple yet effective framework that enhances the reliability\nof LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs\nbenefit from discriminative models, and 3) minimizes hallucinations in\ngenerative tasks. Using our proposed plug-in method, enhanced versions of Llama\n2 and ChatGPT surpass their original versions regarding generalizability and\nfactuality. We offer a comprehensive suite of resources, including 16 curated\ndatasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks.\nOur empirical analysis sheds light on the advantages of incorporating\ndiscriminative models into LLMs and highlights the potential of our methodology\nin fostering more reliable LLMs.\n", "published": "26-12-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Surface Form Competition: Why the Highest Probability Answer Isn't\n  Always Right", "url": "http://arxiv.org/abs/2104.08315v9", "authors": "Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, Luke Zettlemoyer", "abstract": "  Large language models have shown promising results in zero-shot settings\n(Brown et al.,2020; Radford et al., 2019). For example, they can perform\nmultiple choice tasks simply by conditioning on a question and selecting the\nanswer with the highest probability.\n  However, ranking by string probability can be problematic due to surface form\ncompetition-wherein different surface forms compete for probability mass, even\nif they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since\nprobability mass is finite, this lowers the probability of the correct answer,\ndue to competition from other strings that are valid answers (but not one of\nthe multiple choice options).\n  We introduce Domain Conditional Pointwise Mutual Information, an alternative\nscoring function that directly compensates for surface form competition by\nsimply reweighing each option according to a term that is proportional to its a\npriori likelihood within the context of the specific zero-shot task. It\nachieves consistent gains in zero-shot performance over both calibrated (Zhao\net al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models\nover a variety of multiple choice datasets.\n", "published": "16-04-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "Tab-CoT: Zero-shot Tabular Chain of Thought", "url": "http://arxiv.org/abs/2305.17812v1", "authors": "Ziqi Jin, Wei Lu", "abstract": "  The chain-of-though (CoT) prompting methods were successful in various\nnatural language processing (NLP) tasks thanks to their ability to unveil the\nunderlying complex reasoning processes. Such reasoning processes typically\nexhibit implicitly structured steps. Recent efforts also started investigating\nmethods to encourage more explicitly structured reasoning procedures to be\ncaptured. In this work, we propose Tab-CoT, a novel tabular-format CoT\nprompting method, which allows the complex reasoning process to be explicitly\nmodelled in a highly structured manner. Despite its simplicity, we show that\nour approach is capable of performing reasoning across multiple dimensions\n(i.e., both rows and columns). We demonstrate our approach's strong zero-shot\nand few-shot capabilities through extensive experiments on a range of reasoning\ntasks.\n", "published": "28-05-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language\n  Models", "url": "http://arxiv.org/abs/2310.06117v1", "authors": "Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H. Chi, Quoc V Le, Denny Zhou", "abstract": "  We present Step-Back Prompting, a simple prompting technique that enables\nLLMs to do abstractions to derive high-level concepts and first principles from\ninstances containing specific details. Using the concepts and principles to\nguide the reasoning steps, LLMs significantly improve their abilities in\nfollowing a correct reasoning path towards the solution. We conduct experiments\nof Step-Back Prompting with PaLM-2L models and observe substantial performance\ngains on a wide range of challenging reasoning-intensive tasks including STEM,\nKnowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting\nimproves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%,\nTimeQA by 27%, and MuSiQue by 7%.\n", "published": "09-10-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language\n  Modeling Likewise", "url": "http://arxiv.org/abs/2310.19019v2", "authors": "Nan He, Hanyu Lai, Chenyang Zhao, Zirui Cheng, Junting Pan, Ruoyu Qin, Ruofan Lu, Rui Lu, Yunchen Zhang, Gangming Zhao, Zhaohui Hou, Zhiyuan Huang, Shaoqing Lu, Ding Liang, Mingjie Zhan", "abstract": "  Large Language Models (LLMs) exhibit impressive reasoning and data\naugmentation capabilities in various NLP tasks. However, what about small\nmodels? In this work, we propose TeacherLM-7.1B, capable of annotating relevant\nfundamentals, chain of thought, and common mistakes for most NLP samples, which\nmakes annotation more than just an answer, thus allowing other models to learn\n\"why\" instead of just \"what\". The TeacherLM-7.1B model achieved a zero-shot\nscore of 52.3 on MMLU, surpassing most models with over 100B parameters. Even\nmore remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we\naugmented 58 NLP datasets and taught various student models with different\nparameters from OPT and BLOOM series in a multi-task setting. The experimental\nresults indicate that the data augmentation provided by TeacherLM has brought\nsignificant benefits. We will release the TeacherLM series of models and\naugmented datasets as open-source.\n", "published": "29-10-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Technical Report: Large Language Models can Strategically Deceive their\n  Users when Put Under Pressure", "url": "http://arxiv.org/abs/2311.07590v2", "authors": "J\u00e9r\u00e9my Scheurer, Mikita Balesni, Marius Hobbhahn", "abstract": "  We demonstrate a situation in which Large Language Models, trained to be\nhelpful, harmless, and honest, can display misaligned behavior and\nstrategically deceive their users about this behavior without being instructed\nto do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated\nenvironment, where it assumes the role of an autonomous stock trading agent.\nWithin this environment, the model obtains an insider tip about a lucrative\nstock trade and acts upon it despite knowing that insider trading is\ndisapproved of by company management. When reporting to its manager, the model\nconsistently hides the genuine reasons behind its trading decision. We perform\na brief investigation of how this behavior varies under changes to the setting,\nsuch as removing model access to a reasoning scratchpad, attempting to prevent\nthe misaligned behavior by changing system instructions, changing the amount of\npressure the model is under, varying the perceived risk of getting caught, and\nmaking other simple changes to the environment. To our knowledge, this is the\nfirst demonstration of Large Language Models trained to be helpful, harmless,\nand honest, strategically deceiving their users in a realistic situation\nwithout direct instructions or training for deception.\n", "published": "09-11-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs", "url": "http://arxiv.org/abs/2311.02262v1", "authors": "Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng Gao, Tuo Zhao", "abstract": "  In human-written articles, we often leverage the subtleties of text style,\nsuch as bold and italics, to guide the attention of readers. These textual\nemphases are vital for the readers to grasp the conveyed information. When\ninteracting with large language models (LLMs), we have a similar need -\nsteering the model to pay closer attention to user-specified information, e.g.,\nan instruction. Existing methods, however, are constrained to process plain\ntext and do not support such a mechanism. This motivates us to introduce PASTA\n- Post-hoc Attention STeering Approach, a method that allows LLMs to read text\nwith user-specified emphasis marks. To this end, PASTA identifies a small\nsubset of attention heads and applies precise attention reweighting on them,\ndirecting the model attention to user-specified parts. Like prompting, PASTA is\napplied at inference time and does not require changing any model parameters.\nExperiments demonstrate that PASTA can substantially enhance an LLM's ability\nto follow user instructions or integrate new knowledge from user inputs,\nleading to a significant performance improvement on a variety of tasks, e.g.,\nan average accuracy improvement of 22% for LLAMA-7B. Our code is publicly\navailable at https://github.com/QingruZhang/PASTA .\n", "published": "03-11-2023", "year": "2023", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Testing Language Model Agents Safely in the Wild", "url": "http://arxiv.org/abs/2311.10538v3", "authors": "Silen Naihin, David Atkinson, Marc Green, Merwane Hamadi, Craig Swift, Douglas Schonholtz, Adam Tauman Kalai, David Bau", "abstract": "  A prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. Yet\nreal-world autonomous tests face several unique safety challenges, both due to\nthe possibility of causing harm during a test, as well as the risk of\nencountering new unsafe agent behavior through interactions with real-world and\npotentially malicious actors. We propose a framework for conducting safe\nautonomous agent tests on the open internet: agent actions are audited by a\ncontext-sensitive monitor that enforces a stringent safety boundary to stop an\nunsafe test, with suspect behavior ranked and logged to be examined by humans.\nWe design a basic safety monitor (AgentMonitor) that is flexible enough to\nmonitor existing LLM agents, and, using an adversarial simulated agent, we\nmeasure its ability to identify and stop unsafe situations. Then we apply the\nAgentMonitor on a battery of real-world tests of AutoGPT, and we identify\nseveral limitations and challenges that will face the creation of safe\nin-the-wild tests as autonomous agents grow more capable.\n", "published": "17-11-2023", "year": "2023", "categories": ["Artificial Intelligence"]}, {"title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango", "url": "http://arxiv.org/abs/2209.07686v2", "authors": "Aman Madaan, Amir Yazdanbakhsh", "abstract": "  The past decade has witnessed dramatic gains in natural language processing\nand an unprecedented scaling of large language models. These developments have\nbeen accelerated by the advent of few-shot techniques such as chain of thought\n(CoT) prompting. Specifically, CoT pushes the performance of large language\nmodels in a few-shot setup by augmenting the prompts with intermediate steps.\nDespite impressive results across various tasks, the reasons behind their\nsuccess have not been explored. This work uses counterfactual prompting to\ndevelop a deeper understanding of CoT-based few-shot prompting mechanisms in\nlarge language models. We first systematically identify and define the key\ncomponents of a prompt: symbols, patterns, and text. Then, we devise and\nconduct an exhaustive set of experiments across four different tasks, by\nquerying the model with counterfactual prompts where only one of these\ncomponents is altered. Our experiments across three models (PaLM, GPT-3, and\nCODEX) reveal several surprising findings and brings into question the\nconventional wisdom around few-shot prompting. First, the presence of factual\npatterns in a prompt is practically immaterial to the success of CoT. Second,\nour results conclude that the primary role of intermediate steps may not be to\nfacilitate learning how to solve a task. The intermediate steps are rather a\nbeacon for the model to realize what symbols to replicate in the output to form\na factual answer. Further, text imbues patterns with commonsense knowledge and\nmeaning. Our empirical and qualitative analysis reveals that a symbiotic\nrelationship between text and patterns explains the success of few-shot\nprompting: text helps extract commonsense from the question to help patterns,\nand patterns enforce task understanding and direct text generation.\n", "published": "16-09-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Textbooks Are All You Need", "url": "http://arxiv.org/abs/2306.11644v2", "authors": "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, Yuanzhi Li", "abstract": "  We introduce phi-1, a new large language model for code, with significantly\nsmaller size than competing models: phi-1 is a Transformer-based model with\n1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook\nquality\" data from the web (6B tokens) and synthetically generated textbooks\nand exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays\nsurprising emergent properties compared to phi-1-base, our model before our\nfinetuning stage on a dataset of coding exercises, and phi-1-small, a smaller\nmodel with 350M parameters trained with the same pipeline as phi-1 that still\nachieves 45% on HumanEval.\n", "published": "20-06-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Textbooks Are All You Need II: phi-1.5 technical report", "url": "http://arxiv.org/abs/2309.05463v1", "authors": "Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat Lee", "abstract": "  We continue the investigation into the power of smaller Transformer-based\nlanguage models as initiated by \\textbf{TinyStories} -- a 10 million parameter\nmodel that can produce coherent English -- and the follow-up work on\n\\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance\nclose to the state-of-the-art. The latter work proposed to use existing Large\nLanguage Models (LLMs) to generate ``textbook quality\" data as a way to enhance\nthe learning process compared to traditional web data. We follow the\n``Textbooks Are All You Need\" approach, focusing this time on common sense\nreasoning in natural language, and create a new 1.3 billion parameter model\nnamed \\textbf{phi-1.5}, with performance on natural language tasks comparable\nto models 5x larger, and surpassing most non-frontier LLMs on more complex\nreasoning tasks such as grade-school mathematics and basic coding. More\ngenerally, \\textbf{phi-1.5} exhibits many of the traits of much larger LLMs,\nboth good -- such as the ability to ``think step by step\" or perform some\nrudimentary in-context learning -- and bad, including hallucinations and the\npotential for toxic and biased generations -- encouragingly though, we are\nseeing improvement on that front thanks to the absence of web data. We\nopen-source \\textbf{phi-1.5} to promote further research on these urgent\ntopics.\n", "published": "11-09-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "The ART of LLM Refinement: Ask, Refine, and Trust", "url": "http://arxiv.org/abs/2311.07961v1", "authors": "Kumar Shridhar, Koustuv Sinha, Andrew Cohen, Tianlu Wang, Ping Yu, Ram Pasunuru, Mrinmaya Sachan, Jason Weston, Asli Celikyilmaz", "abstract": "  In recent years, Large Language Models (LLMs) have demonstrated remarkable\ngenerative abilities, but can they judge the quality of their own generations?\nA popular concept, referred to as self-refinement, postulates that LLMs can\ndetect and correct the errors in their generations when asked to do so.\nHowever, recent empirical evidence points in the opposite direction, suggesting\nthat LLMs often struggle to accurately identify errors when reasoning is\ninvolved. To address this, we propose a reasoning with refinement objective\ncalled ART: Ask, Refine, and Trust, which asks necessary questions to decide\nwhen an LLM should refine its output, and either affirm or withhold trust in\nits refinement by ranking the refinement and the initial prediction. On two\nmultistep reasoning tasks of mathematical word problems (GSM8K) and question\nanswering (StrategyQA), ART achieves a performance gain of +5 points over\nself-refinement baselines, while using a much smaller model as the decision\nmaker. We also demonstrate the benefit of using smaller models to make\nrefinement decisions as a cost-effective alternative to fine-tuning a larger\nmodel.\n", "published": "14-11-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "The Benefits of a Concise Chain of Thought on Problem-Solving in Large\n  Language Models", "url": "http://arxiv.org/abs/2401.05618v1", "authors": "Matthew Renze, Erhan Guven", "abstract": "  In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We\ncompared standard CoT and CCoT prompts to see how conciseness impacts response\nlength and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4\nwith a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced\naverage response length by 48.70% for both GPT-3.5 and GPT-4 while having a\nnegligible impact on problem-solving performance. However, on math problems,\nGPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads\nto an average per-token cost reduction of 22.67%. These results have practical\nimplications for AI systems engineers using LLMs to solve real-world problems\nwith CoT prompt-engineering techniques. In addition, these results provide more\ngeneral insight for AI researchers studying the emergent behavior of\nstep-by-step reasoning in LLMs.\n", "published": "11-01-2024", "year": "2024", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of\n  Language Models via Chain-of-Thought Fine-Tuning", "url": "http://arxiv.org/abs/2305.14045v2", "authors": "Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, Minjoon Seo", "abstract": "  Language models (LMs) with less than 100B parameters are known to perform\npoorly on chain-of-thought (CoT) reasoning in contrast to large LMs when\nsolving unseen tasks. In this work, we aim to equip smaller LMs with the\nstep-by-step reasoning capability by instruction tuning with CoT rationales. In\norder to achieve this goal, we first introduce a new instruction-tuning dataset\ncalled the CoT Collection, which augments the existing Flan Collection\n(including only 9 CoT tasks) with additional 1.84 million rationales across\n1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B & 11B) with CoT\nCollection enables smaller LMs to have better CoT capabilities on unseen tasks.\nOn the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of\n+4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task\naccuracy. Furthermore, we show that instruction tuning with CoT Collection\nallows LMs to possess stronger few-shot learning capabilities on 4\ndomain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and\n+2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until\nthe max length by a +13.98% margin. Our code, the CoT Collection data, and\nmodel checkpoints are publicly available.\n", "published": "23-05-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "The Earth is Flat? Unveiling Factual Errors in Large Language Models", "url": "http://arxiv.org/abs/2401.00761v1", "authors": "Wenxuan Wang, Juluan Shi, Zhaopeng Tu, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu", "abstract": "  Large Language Models (LLMs) like ChatGPT are foundational in various\napplications due to their extensive knowledge from pre-training and\nfine-tuning. Despite this, they are prone to generating factual and commonsense\nerrors, raising concerns in critical areas like healthcare, journalism, and\neducation to mislead users. Current methods for evaluating LLMs' veracity are\nlimited by test data leakage or the need for extensive human labor, hindering\nefficient and accurate error detection. To tackle this problem, we introduce a\nnovel, automatic testing framework, FactChecker, aimed at uncovering factual\ninaccuracies in LLMs. This framework involves three main steps: First, it\nconstructs a factual knowledge graph by retrieving fact triplets from a\nlarge-scale knowledge database. Then, leveraging the knowledge graph,\nFactChecker employs a rule-based approach to generates three types of questions\n(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and\nmulti-hop relations, along with correct answers. Lastly, it assesses the LLMs'\nresponses for accuracy using tailored matching strategies for each question\ntype. Our extensive tests on six prominent LLMs, including text-davinci-002,\ntext-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal\nthat FactChecker can trigger factual errors in up to 45\\% of questions in these\nmodels. Moreover, we demonstrate that FactChecker's test cases can improve\nLLMs' factual accuracy through in-context learning and fine-tuning (e.g.,\nllama-2-13b-chat's accuracy increase from 35.3\\% to 68.5\\%). We are making all\ncode, data, and results available for future research endeavors.\n", "published": "01-01-2024", "year": "2024", "categories": ["Artificial Intelligence", "Computation and Language"]}, {"title": "The Impact of Large Language Models on Scientific Discovery: a\n  Preliminary Study using GPT-4", "url": "http://arxiv.org/abs/2311.07361v2", "authors": "Microsoft Research AI4Science, Microsoft Azure Quantum", "abstract": "  In recent years, groundbreaking advancements in natural language processing\nhave culminated in the emergence of powerful large language models (LLMs),\nwhich have showcased remarkable capabilities across a vast array of domains,\nincluding the understanding, generation, and translation of natural language,\nand even tasks that extend beyond language processing. In this report, we delve\ninto the performance of LLMs within the context of scientific discovery,\nfocusing on GPT-4, the state-of-the-art language model. Our investigation spans\na diverse range of scientific areas encompassing drug discovery, biology,\ncomputational chemistry (density functional theory (DFT) and molecular dynamics\n(MD)), materials design, and partial differential equations (PDE). Evaluating\nGPT-4 on scientific tasks is crucial for uncovering its potential across\nvarious research domains, validating its domain-specific expertise,\naccelerating scientific progress, optimizing resource allocation, guiding\nfuture model development, and fostering interdisciplinary research. Our\nexploration methodology primarily consists of expert-driven case assessments,\nwhich offer qualitative insights into the model's comprehension of intricate\nscientific concepts and relationships, and occasionally benchmark testing,\nwhich quantitatively evaluates the model's capacity to solve well-defined\ndomain-specific problems. Our preliminary exploration indicates that GPT-4\nexhibits promising potential for a variety of scientific applications,\ndemonstrating its aptitude for handling complex problem-solving and knowledge\nintegration tasks. Broadly speaking, we evaluate GPT-4's knowledge base,\nscientific understanding, scientific numerical calculation abilities, and\nvarious scientific prediction capabilities.\n", "published": "13-11-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "The Impact of Reasoning Step Length on Large Language Models", "url": "http://arxiv.org/abs/2401.04925v2", "authors": "Mingyu Jin, Qinkai Yu, Dong shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du", "abstract": "  Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations,\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences.\n", "published": "10-01-2024", "year": "2024", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "The Perils & Promises of Fact-checking with Large Language Models", "url": "http://arxiv.org/abs/2310.13549v1", "authors": "Dorian Quelle, Alexandre Bovet", "abstract": "  Autonomous fact-checking, using machine learning to verify claims, has grown\nvital as misinformation spreads beyond human fact-checking capacity. Large\nLanguage Models (LLMs) like GPT-4 are increasingly trusted to verify\ninformation and write academic papers, lawsuits, and news articles, emphasizing\ntheir role in discerning truth from falsehood and the importance of being able\nto verify their outputs. Here, we evaluate the use of LLM agents in\nfact-checking by having them phrase queries, retrieve contextual data, and make\ndecisions. Importantly, in our framework, agents explain their reasoning and\ncite the relevant sources from the retrieved context. Our results show the\nenhanced prowess of LLMs when equipped with contextual information. GPT-4\noutperforms GPT-3, but accuracy varies based on query language and claim\nveracity. While LLMs show promise in fact-checking, caution is essential due to\ninconsistent accuracy. Our investigation calls for further research, fostering\na deeper comprehension of when agents succeed and when they fail.\n", "published": "20-10-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "url": "http://arxiv.org/abs/2101.00027v1", "authors": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy", "abstract": "  Recent work has demonstrated that increased training dataset diversity\nimproves general cross-domain knowledge and downstream generalization\ncapability for large-scale language models. With this in mind, we present\n\\textit{the Pile}: an 825 GiB English text corpus targeted at training\nlarge-scale language models. The Pile is constructed from 22 diverse\nhigh-quality subsets -- both existing and newly constructed -- many of which\nderive from academic or professional sources. Our evaluation of the untuned\nperformance of GPT-2 and GPT-3 on the Pile shows that these models struggle on\nmany of its components, such as academic writing. Conversely, models trained on\nthe Pile improve significantly over both Raw CC and CC-100 on all components of\nthe Pile, while improving performance on downstream evaluations. Through an\nin-depth exploratory analysis, we document potentially concerning aspects of\nthe data for prospective users. We make publicly available the code used in its\nconstruction.\n", "published": "31-12-2020", "year": "2020", "categories": ["Computation and Language"]}, {"title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "url": "http://arxiv.org/abs/2104.08691v2", "authors": "Brian Lester, Rami Al-Rfou, Noah Constant", "abstract": "  In this work, we explore \"prompt tuning\", a simple yet effective mechanism\nfor learning \"soft prompts\" to condition frozen language models to perform\nspecific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft\nprompts are learned through backpropagation and can be tuned to incorporate\nsignal from any number of labeled examples. Our end-to-end learned approach\noutperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably,\nthrough ablations on model size using T5, we show that prompt tuning becomes\nmore competitive with scale: as models exceed billions of parameters, our\nmethod \"closes the gap\" and matches the strong performance of model tuning\n(where all model weights are tuned). This finding is especially relevant in\nthat large models are costly to share and serve, and the ability to reuse one\nfrozen model for multiple downstream tasks can ease this burden. Our method can\nbe seen as a simplification of the recently proposed \"prefix tuning\" of Li and\nLiang (2021), and we provide a comparison to this and other similar approaches.\nFinally, we show that conditioning a frozen model with soft prompts confers\nbenefits in robustness to domain transfer, as compared to full model tuning.\n", "published": "18-04-2021", "year": "2021", "categories": ["Computation and Language"]}, {"title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora\n  with Web Data, and Web Data Only", "url": "http://arxiv.org/abs/2306.01116v1", "authors": "Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay", "abstract": "  Large language models are commonly trained on a mixture of filtered web data\nand curated high-quality corpora, such as social media conversations, books, or\ntechnical papers. This curation process is believed to be necessary to produce\nperformant models with broad zero-shot generalization abilities. However, as\nlarger models requiring pretraining on trillions of tokens are considered, it\nis unclear how scalable is curation and whether we will run out of unique\nhigh-quality data soon. At variance with previous beliefs, we show that\nproperly filtered and deduplicated web data alone can lead to powerful models;\neven significantly outperforming models from the state-of-the-art trained on\nThe Pile. Despite extensive filtering, the high-quality data we extract from\nthe web is still plentiful, and we are able to obtain five trillion tokens from\nCommonCrawl. We publicly release an extract of 600 billion tokens from our\nRefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.\n", "published": "01-06-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "The Rise and Potential of Large Language Model Based Agents: A Survey", "url": "http://arxiv.org/abs/2309.07864v3", "authors": "Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, Tao Gui", "abstract": "  For a long time, humanity has pursued artificial intelligence (AI) equivalent\nto or surpassing the human level, with AI agents considered a promising vehicle\nfor this pursuit. AI agents are artificial entities that sense their\nenvironment, make decisions, and take actions. Many efforts have been made to\ndevelop intelligent agents, but they mainly focus on advancement in algorithms\nor training strategies to enhance specific capabilities or performance on\nparticular tasks. Actually, what the community lacks is a general and powerful\nmodel to serve as a starting point for designing AI agents that can adapt to\ndiverse scenarios. Due to the versatile capabilities they demonstrate, large\nlanguage models (LLMs) are regarded as potential sparks for Artificial General\nIntelligence (AGI), offering hope for building general AI agents. Many\nresearchers have leveraged LLMs as the foundation to build AI agents and have\nachieved significant progress. In this paper, we perform a comprehensive survey\non LLM-based agents. We start by tracing the concept of agents from its\nphilosophical origins to its development in AI, and explain why LLMs are\nsuitable foundations for agents. Building upon this, we present a general\nframework for LLM-based agents, comprising three main components: brain,\nperception, and action, and the framework can be tailored for different\napplications. Subsequently, we explore the extensive applications of LLM-based\nagents in three aspects: single-agent scenarios, multi-agent scenarios, and\nhuman-agent cooperation. Following this, we delve into agent societies,\nexploring the behavior and personality of LLM-based agents, the social\nphenomena that emerge from an agent society, and the insights they offer for\nhuman society. Finally, we discuss several key topics and open problems within\nthe field. A repository for the related papers at\nhttps://github.com/WooooDyy/LLM-Agent-Paper-List.\n", "published": "14-09-2023", "year": "2023", "categories": ["Artificial Intelligence", "Computation and Language"]}, {"title": "The Unreliability of Explanations in Few-shot Prompting for Textual\n  Reasoning", "url": "http://arxiv.org/abs/2205.03401v2", "authors": "Xi Ye, Greg Durrett", "abstract": "  Does prompting a large language model (LLM) like GPT-3 with explanations\nimprove in-context learning? We study this question on two NLP tasks that\ninvolve reasoning over text, namely question answering and natural language\ninference. We test the performance of four LLMs on three textual reasoning\ndatasets using prompts that include explanations in multiple different styles.\nFor these tasks, we find that including explanations in the prompts for OPT,\nGPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to\nmoderate accuracy improvements over standard few-show learning. However,\ntext-davinci-002 is able to benefit more substantially.\n  We further show that explanations generated by the LLMs may not entail the\nmodels' predictions nor be factually grounded in the input, even on simple\ntasks with extractive explanations. However, these flawed explanations can\nstill be useful as a way to verify LLMs' predictions post-hoc. Through analysis\nin our three settings, we show that explanations judged by humans to be\ngood--logically consistent with the input and the prediction--more likely\ncooccur with accurate predictions. Following these observations, we train\ncalibrators using automatically extracted scores that assess the reliability of\nexplanations, allowing us to improve performance post-hoc across all of our\ndatasets.\n", "published": "06-05-2022", "year": "2022", "categories": ["Computation and Language"]}, {"title": "Thousands of AI Authors on the Future of AI", "url": "http://arxiv.org/abs/2401.02843v1", "authors": "Katja Grace, Harlan Stewart, Julia Fabienne Sandk\u00fchler, Stephen Thomas, Ben Weinstein-Raun, Jan Brauner", "abstract": "  In the largest survey of its kind, 2,778 researchers who had published in\ntop-tier artificial intelligence (AI) venues gave predictions on the pace of AI\nprogress and the nature and impacts of advanced AI systems The aggregate\nforecasts give at least a 50% chance of AI systems achieving several milestones\nby 2028, including autonomously constructing a payment processing site from\nscratch, creating a song indistinguishable from a new song by a popular\nmusician, and autonomously downloading and fine-tuning a large language model.\nIf science continues undisrupted, the chance of unaided machines outperforming\nhumans in every possible task was estimated at 10% by 2027, and 50% by 2047.\nThe latter estimate is 13 years earlier than that reached in a similar survey\nwe conducted only one year earlier [Grace et al., 2022]. However, the chance of\nall human occupations becoming fully automatable was forecast to reach 10% by\n2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).\n  Most respondents expressed substantial uncertainty about the long-term value\nof AI progress: While 68.3% thought good outcomes from superhuman AI are more\nlikely than bad, of these net optimists 48% gave at least a 5% chance of\nextremely bad outcomes such as human extinction, and 59% of net pessimists gave\n5% or more to extremely good outcomes. Between 38% and 51% of respondents gave\nat least a 10% chance to advanced AI leading to outcomes as bad as human\nextinction. More than half suggested that \"substantial\" or \"extreme\" concern is\nwarranted about six different AI-related scenarios, including misinformation,\nauthoritarian control, and inequality. There was disagreement about whether\nfaster or slower AI progress would be better for the future of humanity.\nHowever, there was broad agreement that research aimed at minimizing potential\nrisks from AI systems ought to be prioritized more.\n", "published": "05-01-2024", "year": "2024", "categories": ["Artificial Intelligence", "Machine Learning"]}, {"title": "TigerBot: An Open Multilingual Multitask LLM", "url": "http://arxiv.org/abs/2312.08688v2", "authors": "Ye Chen, Wei Cai, Liangmin Wu, Xiaowei Li, Zhanxuan Xin, Cong Fu", "abstract": "  We release and introduce the TigerBot family of large language models (LLMs),\nconsisting of base and chat models, sized from 7, 13, 70 and 180 billion\nparameters. We develop our models embarking from Llama-2 and BLOOM, and push\nthe boundary further in data, training algorithm, infrastructure, and\napplication tools. Our models yield meaningful performance gain over SOTA\nopen-source models, e.g., Llama-2, specifically 6% gain in English and 20% gain\nin Chinese. TigerBot model family also achieves leading performance in major\nacademic and industrial benchmarks and leaderboards. We believe that TigerBot\nrepresents just a snapshot of lightning-fast progression in LLM open-source\ncommunity. Therefore, we are thrilled to give back by publicly releasing our\nmodels and reporting our approach behind, with additional emphases on building\nSOTA LLMs in a democratized way and making LLMs of use in real-world\napplications.\n", "published": "14-12-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Time is Encoded in the Weights of Finetuned Language Models", "url": "http://arxiv.org/abs/2312.13401v2", "authors": "Kai Nylund, Suchin Gururangan, Noah A. Smith", "abstract": "  We present time vectors, a simple tool to customize language models to new\ntime periods. Time vectors are created by finetuning a language model on data\nfrom a single time (e.g., a year or month), and then subtracting the weights of\nthe original pretrained model. This vector specifies a direction in weight\nspace that, as our experiments show, improves performance on text from that\ntime period. Time vectors specialized to adjacent time periods appear to be\npositioned closer together in a manifold. Using this structure, we interpolate\nbetween time vectors to induce new models that perform better on intervening\nand future time periods, without any additional training. We demonstrate the\nconsistency of our findings across different tasks, domains, model sizes, and\ntime scales. Our results suggest that time is encoded in the weight space of\nfinetuned models.\n", "published": "20-12-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "TinyLlama: An Open-Source Small Language Model", "url": "http://arxiv.org/abs/2401.02385v1", "authors": "Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu", "abstract": "  We present TinyLlama, a compact 1.1B language model pretrained on around 1\ntrillion tokens for approximately 3 epochs. Building on the architecture and\ntokenizer of Llama 2, TinyLlama leverages various advances contributed by the\nopen-source community (e.g., FlashAttention), achieving better computational\nefficiency. Despite its relatively small size, TinyLlama demonstrates\nremarkable performance in a series of downstream tasks. It significantly\noutperforms existing open-source language models with comparable sizes. Our\nmodel checkpoints and code are publicly available on GitHub at\nhttps://github.com/jzhang38/TinyLlama.\n", "published": "04-01-2024", "year": "2024", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent\n  English?", "url": "http://arxiv.org/abs/2305.07759v2", "authors": "Ronen Eldan, Yuanzhi Li", "abstract": "  Language models (LMs) are powerful tools for natural language processing, but\nthey often struggle to produce coherent and fluent text when they are small.\nModels with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can\nrarely generate coherent and consistent English text beyond a few words even\nafter extensive training. This raises the question of whether the emergence of\nthe ability to produce coherent English text only occurs at larger scales (with\nhundreds of millions of parameters or more) and complex architectures (with\nmany layers of global attention).\n  In this work, we introduce TinyStories, a synthetic dataset of short stories\nthat only contain words that a typical 3 to 4-year-olds usually understand,\ngenerated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train\nand evaluate LMs that are much smaller than the state-of-the-art models (below\n10 million total parameters), or have much simpler architectures (with only one\ntransformer block), yet still produce fluent and consistent stories with\nseveral paragraphs that are diverse and have almost perfect grammar, and\ndemonstrate reasoning capabilities.\n  We also introduce a new paradigm for the evaluation of language models: We\nsuggest a framework which uses GPT-4 to grade the content generated by these\nmodels as if those were stories written by students and graded by a (human)\nteacher. This new paradigm overcomes the flaws of standard benchmarks which\noften requires the model's output to be very structures, and moreover provides\na multidimensional score for the model, providing scores for different\ncapabilities such as grammar, creativity and consistency.\n  We hope that TinyStories can facilitate the development, analysis and\nresearch of LMs, especially for low-resource or specialized domains, and shed\nlight on the emergence of language capabilities in LMs.\n", "published": "12-05-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "TopicGPT: A Prompt-based Topic Modeling Framework", "url": "http://arxiv.org/abs/2311.01449v1", "authors": "Chau Minh Pham, Alexander Hoyle, Simeng Sun, Mohit Iyyer", "abstract": "  Topic modeling is a well-established technique for exploring text corpora.\nConventional topic models (e.g., LDA) represent topics as bags of words that\noften require \"reading the tea leaves\" to interpret; additionally, they offer\nusers minimal semantic control over topics. To tackle these issues, we\nintroduce TopicGPT, a prompt-based framework that uses large language models\n(LLMs) to uncover latent topics within a provided text collection. TopicGPT\nproduces topics that align better with human categorizations compared to\ncompeting methods: for example, it achieves a harmonic mean purity of 0.74\nagainst human-annotated Wikipedia topics compared to 0.64 for the strongest\nbaseline. Its topics are also more interpretable, dispensing with ambiguous\nbags of words in favor of topics with natural language labels and associated\nfree-form descriptions. Moreover, the framework is highly adaptable, allowing\nusers to specify constraints and modify topics without the need for model\nretraining. TopicGPT can be further extended to hierarchical topical modeling,\nenabling users to explore topics at various levels of granularity. By\nstreamlining access to high-quality and interpretable topics, TopicGPT\nrepresents a compelling, human-centered approach to topic modeling.\n", "published": "02-11-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Towards Reasoning in Large Language Models: A Survey", "url": "http://arxiv.org/abs/2212.10403v2", "authors": "Jie Huang, Kevin Chen-Chuan Chang", "abstract": "  Reasoning is a fundamental aspect of human intelligence that plays a crucial\nrole in activities such as problem solving, decision making, and critical\nthinking. In recent years, large language models (LLMs) have made significant\nprogress in natural language processing, and there is observation that these\nmodels may exhibit reasoning abilities when they are sufficiently large.\nHowever, it is not yet clear to what extent LLMs are capable of reasoning. This\npaper provides a comprehensive overview of the current state of knowledge on\nreasoning in LLMs, including techniques for improving and eliciting reasoning\nin these models, methods and benchmarks for evaluating reasoning abilities,\nfindings and implications of previous research in this field, and suggestions\non future directions. Our aim is to provide a detailed and up-to-date review of\nthis topic and stimulate meaningful discussion and future work.\n", "published": "20-12-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of\n  What Matters", "url": "http://arxiv.org/abs/2212.10001v2", "authors": "Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun", "abstract": "  Chain-of-Thought (CoT) prompting can dramatically improve the multi-step\nreasoning abilities of large language models (LLMs). CoT explicitly encourages\nthe LLM to generate intermediate rationales for solving a problem, by providing\na series of reasoning steps in the demonstrations. Despite its success, there\nis still little understanding of what makes CoT prompting effective and which\naspects of the demonstrated reasoning steps contribute to its performance. In\nthis paper, we show that CoT reasoning is possible even with invalid\ndemonstrations - prompting with invalid reasoning steps can achieve over 80-90%\nof the performance obtained using CoT under various metrics, while still\ngenerating coherent lines of reasoning during inference. Further experiments\nshow that other aspects of the rationales, such as being relevant to the query\nand correctly ordering the reasoning steps, are much more important for\neffective CoT reasoning. Overall, these findings both deepen our understanding\nof CoT prompting, and open up new questions regarding LLMs' capability to learn\nto reason in context.\n", "published": "20-12-2022", "year": "2022", "categories": ["Computation and Language"]}, {"title": "Towards Verifiable Text Generation with Symbolic References", "url": "http://arxiv.org/abs/2311.09188v1", "authors": "Lucas Torroba Hennigen, Shannon Shen, Aniruddha Nrusimha, Bernhard Gapp, David Sontag, Yoon Kim", "abstract": "  Large language models (LLMs) have demonstrated an impressive ability to\nsynthesize plausible and fluent text. However they remain vulnerable to\nhallucinations, and thus their outputs generally require manual human\nverification for high-stakes applications, which can be time-consuming and\ndifficult. This paper proposes symbolically grounded generation (SymGen) as a\nsimple approach for enabling easier validation of an LLM's output. SymGen\nprompts an LLM to interleave its regular output text with explicit symbolic\nreferences to fields present in some conditioning data (e.g., a table in JSON\nformat). The references can be used to display the provenance of different\nspans of text in the generation, reducing the effort required for manual\nverification. Across data-to-text and question answering experiments, we find\nthat LLMs are able to directly output text that makes use of symbolic\nreferences while maintaining fluency and accuracy.\n", "published": "15-11-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Training a Helpful and Harmless Assistant with Reinforcement Learning\n  from Human Feedback", "url": "http://arxiv.org/abs/2204.05862v1", "authors": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, Jared Kaplan", "abstract": "  We apply preference modeling and reinforcement learning from human feedback\n(RLHF) to finetune language models to act as helpful and harmless assistants.\nWe find this alignment training improves performance on almost all NLP\nevaluations, and is fully compatible with training for specialized skills such\nas python coding and summarization. We explore an iterated online mode of\ntraining, where preference models and RL policies are updated on a weekly\ncadence with fresh human feedback data, efficiently improving our datasets and\nmodels. Finally, we investigate the robustness of RLHF training, and identify a\nroughly linear relation between the RL reward and the square root of the KL\ndivergence between the policy and its initialization. Alongside our main\nresults, we perform peripheral analyses on calibration, competing objectives,\nand the use of OOD detection, compare our models with human writers, and\nprovide samples from our models using prompts appearing in recent related work.\n", "published": "12-04-2022", "year": "2022", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "Training language models to follow instructions with human feedback", "url": "http://arxiv.org/abs/2203.02155v1", "authors": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe", "abstract": "  Making language models bigger does not inherently make them better at\nfollowing a user's intent. For example, large language models can generate\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\nwords, these models are not aligned with their users. In this paper, we show an\navenue for aligning language models with user intent on a wide range of tasks\nby fine-tuning with human feedback. Starting with a set of labeler-written\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\noutputs, which we use to further fine-tune this supervised model using\nreinforcement learning from human feedback. We call the resulting models\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\nimprovements in truthfulness and reductions in toxic output generation while\nhaving minimal performance regressions on public NLP datasets. Even though\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\nhuman feedback is a promising direction for aligning language models with human\nintent.\n", "published": "04-03-2022", "year": "2022", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "Transformer models: an introduction and catalog", "url": "http://arxiv.org/abs/2302.07730v3", "authors": "Xavier Amatriain, Ananth Sankar, Jie Bing, Praveen Kumar Bodigutla, Timothy J. Hazen, Michaeel Kazi", "abstract": "  In the past few years we have seen the meteoric appearance of dozens of\nfoundation models of the Transformer family, all of which have memorable and\nsometimes funny, but not self-explanatory, names. The goal of this paper is to\noffer a somewhat comprehensive but simple catalog and classification of the\nmost popular Transformer models. The paper also includes an introduction to the\nmost important aspects and innovations in Transformer models. Our catalog will\ninclude models that are trained using self-supervised learning (e.g., BERT or\nGPT3) as well as those that are further trained using a human-in-the-loop (e.g.\nthe InstructGPT model used by ChatGPT).\n", "published": "12-02-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "url": "http://arxiv.org/abs/2305.10601v2", "authors": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan", "abstract": "  Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models'\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/princeton-nlp/tree-of-thought-llm.\n", "published": "17-05-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "True Few-Shot Learning with Language Models", "url": "http://arxiv.org/abs/2105.11447v1", "authors": "Ethan Perez, Douwe Kiela, Kyunghyun Cho", "abstract": "  Pretrained language models (LMs) perform well on many tasks even when\nlearning from a few examples, but prior work uses many held-out examples to\ntune various aspects of learning, such as hyperparameters, training objectives,\nand natural language templates (\"prompts\"). Here, we evaluate the few-shot\nability of LMs when such held-out examples are unavailable, a setting we call\ntrue few-shot learning. We test two model selection criteria, cross-validation\nand minimum description length, for choosing LM prompts and hyperparameters in\nthe true few-shot setting. On average, both marginally outperform random\nselection and greatly underperform selection based on held-out examples.\nMoreover, selection criteria often prefer models that perform significantly\nworse than randomly-selected ones. We find similar results even when taking\ninto account our uncertainty in a model's true performance during selection, as\nwell as when varying the amount of computation and number of examples used for\nselection. Overall, our findings suggest that prior work significantly\noverestimated the true few-shot ability of LMs given the difficulty of few-shot\nmodel selection.\n", "published": "24-05-2021", "year": "2021", "categories": ["Computation and Language", "Machine Learning"]}, {"title": "TrustLLM: Trustworthiness in Large Language Models", "url": "http://arxiv.org/abs/2401.05561v2", "authors": "Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, Willian Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, Yue Zhao", "abstract": "  Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness.\n", "published": "10-01-2024", "year": "2024", "categories": ["Computation and Language"]}, {"title": "UL2: Unifying Language Learning Paradigms", "url": "http://arxiv.org/abs/2205.05131v3", "authors": "Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, Donald Metzler", "abstract": "  Existing pre-trained models are generally geared towards a particular class\nof problems. To date, there seems to be still no consensus on what the right\narchitecture and pre-training setup should be. This paper presents a unified\nframework for pre-training models that are universally effective across\ndatasets and setups. We begin by disentangling architectural archetypes with\npre-training objectives -- two concepts that are commonly conflated. Next, we\npresent a generalized & unified perspective for self-supervision in NLP and\nshow how different pre-training objectives can be cast as one another and how\ninterpolating between different objectives can be effective. We then propose\nMixture-of-Denoisers (MoD), a pre-training objective that combines diverse\npre-training paradigms together. We furthermore introduce a notion of mode\nswitching, wherein downstream fine-tuning is associated with specific\npre-training schemes. We conduct extensive ablative experiments to compare\nmultiple pre-training objectives and find that our method pushes the\nPareto-frontier by outperforming T5 & GPT-like models across multiple diverse\nsetups. By scaling our model up to 20B parameters, we achieve SOTA performance\non 50 well-established supervised finetuning based NLP tasks. Our model also\nachieve strong results at in-context learning, outperforming 175B GPT-3 on\nzero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot\nsummarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B\nalso works well with chain-of-thought prompting and reasoning, making it an\nappealing choice for research into reasoning at a small to medium scale of 20B\nparameters. Finally, we apply FLAN instruction tuning to the UL2 20B model,\nachieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release\nFlax-based T5X checkpoints for the UL2 20B & Flan-UL2 20B.\n", "published": "10-05-2022", "year": "2022", "categories": ["Computation and Language"]}, {"title": "Understanding LLMs: A Comprehensive Overview from Training to Inference", "url": "http://arxiv.org/abs/2401.02038v2", "authors": "Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu, Zihao Wu, Zhengliang Liu, Xin Zhang, Shu Zhang, Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu, Bao Ge", "abstract": "  The introduction of ChatGPT has led to a significant increase in the\nutilization of Large Language Models (LLMs) for addressing downstream tasks.\nThere's an increasing focus on cost-efficient training and deployment within\nthis context. Low-cost training and deployment of LLMs represent the future\ndevelopment trend. This paper reviews the evolution of large language model\ntraining techniques and inference deployment technologies aligned with this\nemerging trend. The discussion on training includes various aspects, including\ndata preprocessing, training architecture, pre-training tasks, parallel\ntraining, and relevant content related to model fine-tuning. On the inference\nside, the paper covers topics such as model compression, parallel computation,\nmemory scheduling, and structural optimization. It also explores LLMs'\nutilization and provides insights into their future development.\n", "published": "04-01-2024", "year": "2024", "categories": ["Computation and Language"]}, {"title": "Unifying the Perspectives of NLP and Software Engineering: A Survey on\n  Language Models for Code", "url": "http://arxiv.org/abs/2311.07989v3", "authors": "Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, Rui Wang", "abstract": "  In this work we systematically review the recent advancements in code\nprocessing with language models, covering 50+ models, 30+ evaluation tasks,\n170+ datasets, and 700 related works. We break down code processing models into\ngeneral language models represented by the GPT family and specialized models\nthat are specifically pretrained on code, often with tailored objectives. We\ndiscuss the relations and differences between these models, and highlight the\nhistorical transition of code modeling from statistical models and RNNs to\npretrained Transformers and LLMs, which is exactly the same course that had\nbeen taken by NLP. We also discuss code-specific features such as AST, CFG, and\nunit tests, along with their application in training code language models, and\nidentify key challenges and potential future directions in this domain. We keep\nthe survey open and updated on GitHub at\nhttps://github.com/codefuse-ai/Awesome-Code-LLM.\n", "published": "14-11-2023", "year": "2023", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online\n  Videos", "url": "http://arxiv.org/abs/2206.11795v1", "authors": "Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune", "abstract": "  Pretraining on noisy, internet-scale datasets has been heavily studied as a\ntechnique for training models with broad, general capabilities for text,\nimages, and other modalities. However, for many sequential decision domains\nsuch as robotics, video games, and computer use, publicly available data does\nnot contain the labels required to train behavioral priors in the same way. We\nextend the internet-scale pretraining paradigm to sequential decision domains\nthrough semi-supervised imitation learning wherein agents learn to act by\nwatching online unlabeled videos. Specifically, we show that with a small\namount of labeled data we can train an inverse dynamics model accurate enough\nto label a huge unlabeled source of online data -- here, online videos of\npeople playing Minecraft -- from which we can then train a general behavioral\nprior. Despite using the native human interface (mouse and keyboard at 20Hz),\nwe show that this behavioral prior has nontrivial zero-shot capabilities and\nthat it can be fine-tuned, with both imitation learning and reinforcement\nlearning, to hard-exploration tasks that are impossible to learn from scratch\nvia reinforcement learning. For many tasks our models exhibit human-level\nperformance, and we are the first to report computer agents that can craft\ndiamond tools, which can take proficient humans upwards of 20 minutes (24,000\nenvironment actions) of gameplay to accomplish.\n", "published": "23-06-2022", "year": "2022", "categories": ["Machine Learning", "Artificial Intelligence"]}, {"title": "Want To Reduce Labeling Cost? GPT-3 Can Help", "url": "http://arxiv.org/abs/2108.13487v1", "authors": "Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng", "abstract": "  Data annotation is a time-consuming and labor-intensive process for many NLP\ntasks. Although there exist various methods to produce pseudo data labels, they\nare often task-specific and require a decent amount of labeled data to start\nwith. Recently, the immense language model GPT-3 with 175 billion parameters\nhas achieved tremendous improvement across many few-shot learning tasks. In\nthis paper, we explore ways to leverage GPT-3 as a low-cost data labeler to\ntrain other models. We find that, to make the downstream model achieve the same\nperformance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use\nlabels from GPT-3 than using labels from humans. Furthermore, we propose a\nnovel framework of combining pseudo labels from GPT-3 with human labels, which\nleads to even better performance with limited labeling budget. These results\npresent a cost-effective data labeling methodology that is generalizable to\nmany practical applications.\n", "published": "30-08-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence"]}, {"title": "WebArena: A Realistic Web Environment for Building Autonomous Agents", "url": "http://arxiv.org/abs/2307.13854v3", "authors": "Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig", "abstract": "  With advances in generative AI, there is now potential for autonomous agents\nto manage daily tasks via natural language commands. However, current agents\nare primarily created and tested in simplified synthetic environments, leading\nto a disconnect with real-world scenarios. In this paper, we build an\nenvironment for language-guided agents that is highly realistic and\nreproducible. Specifically, we focus on agents that perform tasks on the web,\nand create an environment with fully functional websites from four common\ndomains: e-commerce, social forum discussions, collaborative software\ndevelopment, and content management. Our environment is enriched with tools\n(e.g., a map) and external knowledge bases (e.g., user manuals) to encourage\nhuman-like task-solving. Building upon our environment, we release a set of\nbenchmark tasks focusing on evaluating the functional correctness of task\ncompletions. The tasks in our benchmark are diverse, long-horizon, and designed\nto emulate tasks that humans routinely perform on the internet. We experiment\nwith several baseline agents, integrating recent techniques such as reasoning\nbefore acting. The results demonstrate that solving complex tasks is\nchallenging: our best GPT-4-based agent only achieves an end-to-end task\nsuccess rate of 14.41%, significantly lower than the human performance of\n78.24%. These results highlight the need for further development of robust\nagents, that current state-of-the-art large language models are far from\nperfect performance in these real-life tasks, and that WebArena can be used to\nmeasure such progress.\n", "published": "25-07-2023", "year": "2023", "categories": ["Artificial Intelligence", "Computation and Language", "Machine Learning"]}, {"title": "WebGPT: Browser-assisted question-answering with human feedback", "url": "http://arxiv.org/abs/2112.09332v3", "authors": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman", "abstract": "  We fine-tune GPT-3 to answer long-form questions using a text-based\nweb-browsing environment, which allows the model to search and navigate the\nweb. By setting up the task so that it can be performed by humans, we are able\nto train models on the task using imitation learning, and then optimize answer\nquality with human feedback. To make human evaluation of factual accuracy\neasier, models must collect references while browsing in support of their\nanswers. We train and evaluate our models on ELI5, a dataset of questions asked\nby Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior\ncloning, and then performing rejection sampling against a reward model trained\nto predict human preferences. This model's answers are preferred by humans 56%\nof the time to those of our human demonstrators, and 69% of the time to the\nhighest-voted answer from Reddit.\n", "published": "17-12-2021", "year": "2021", "categories": ["Computation and Language", "Artificial Intelligence", "Machine Learning"]}, {"title": "When Large Language Model based Agent Meets User Behavior Analysis: A\n  Novel User Simulation Paradigm", "url": "http://arxiv.org/abs/2306.02552v2", "authors": "Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, Ji-Rong Wen", "abstract": "  User behavior analysis is crucial in human-centered AI applications. In this\nfield, the collection of sufficient and high-quality user behavior data has\nalways been a fundamental yet challenging problem. An intuitive idea to address\nthis problem is automatically simulating the user behaviors. However, due to\nthe subjective and complex nature of human cognitive processes, reliably\nsimulating the user behavior is difficult. Recently, large language models\n(LLM) have obtained remarkable successes, showing great potential to achieve\nhuman-like intelligence. We argue that these models present significant\nopportunities for reliable user simulation, and have the potential to\nrevolutionize traditional study paradigms in user behavior analysis. In this\npaper, we take recommender system as an example to explore the potential of\nusing LLM for user simulation. Specifically, we regard each user as an\nLLM-based autonomous agent, and let different agents freely communicate, behave\nand evolve in a virtual simulator called RecAgent. For comprehensively\nsimulation, we not only consider the behaviors within the recommender system\n(\\emph{e.g.}, item browsing and clicking), but also accounts for external\ninfluential factors, such as, friend chatting and social advertisement. Our\nsimulator contains at most 1000 agents, and each agent is composed of a\nprofiling module, a memory module and an action module, enabling it to behave\nconsistently, reasonably and reliably. In addition, to more flexibly operate\nour simulator, we also design two global functions including real-human playing\nand system intervention. To evaluate the effectiveness of our simulator, we\nconduct extensive experiments from both agent and system perspectives. In order\nto advance this direction, we have released our project at\n{https://github.com/RUC-GSAI/YuLan-Rec}.\n", "published": "05-06-2023", "year": "2023", "categories": ["Artificial Intelligence"]}, {"title": "Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis\n  of Head and Prompt Tuning", "url": "http://arxiv.org/abs/2106.09226v2", "authors": "Colin Wei, Sang Michael Xie, Tengyu Ma", "abstract": "  Pretrained language models have achieved state-of-the-art performance when\nadapted to a downstream NLP task. However, theoretical analysis of these models\nis scarce and challenging since the pretraining and downstream tasks can be\nvery different. We propose an analysis framework that links the pretraining and\ndownstream tasks with an underlying latent variable generative model of text --\nthe downstream classifier must recover a function of the posterior distribution\nover the latent variables. We analyze head tuning (learning a classifier on top\nof the frozen pretrained model) and prompt tuning in this setting. The\ngenerative model in our analysis is either a Hidden Markov Model (HMM) or an\nHMM augmented with a latent memory component, motivated by long-term\ndependencies in natural language. We show that 1) under certain non-degeneracy\nconditions on the HMM, simple classification heads can solve the downstream\ntask, 2) prompt tuning obtains downstream guarantees with weaker non-degeneracy\nconditions, and 3) our recovery guarantees for the memory-augmented HMM are\nstronger than for the vanilla HMM because task-relevant information is easier\nto recover from the long-term memory. Experiments on synthetically generated\ndata from HMMs back our theoretical findings.\n", "published": "17-06-2021", "year": "2021", "categories": ["Machine Learning"]}, {"title": "You Only Prompt Once: On the Capabilities of Prompt Learning on Large\n  Language Models to Tackle Toxic Content", "url": "http://arxiv.org/abs/2308.05596v1", "authors": "Xinlei He, Savvas Zannettou, Yun Shen, Yang Zhang", "abstract": "  The spread of toxic content online is an important problem that has adverse\neffects on user experience online and in our society at large. Motivated by the\nimportance and impact of the problem, research focuses on developing solutions\nto detect toxic content, usually leveraging machine learning (ML) models\ntrained on human-annotated datasets. While these efforts are important, these\nmodels usually do not generalize well and they can not cope with new trends\n(e.g., the emergence of new toxic terms). Currently, we are witnessing a shift\nin the approach to tackling societal issues online, particularly leveraging\nlarge language models (LLMs) like GPT-3 or T5 that are trained on vast corpora\nand have strong generalizability. In this work, we investigate how we can use\nLLMs and prompt learning to tackle the problem of toxic content, particularly\nfocusing on three tasks; 1) Toxicity Classification, 2) Toxic Span Detection,\nand 3) Detoxification. We perform an extensive evaluation over five model\narchitectures and eight datasets demonstrating that LLMs with prompt learning\ncan achieve similar or even better performance compared to models trained on\nthese specific tasks. We find that prompt learning achieves around 10\\%\nimprovement in the toxicity classification task compared to the baselines,\nwhile for the toxic span detection task we find better performance to the best\nbaseline (0.643 vs. 0.640 in terms of $F_1$-score). Finally, for the\ndetoxification task, we find that prompt learning can successfully reduce the\naverage toxicity score (from 0.775 to 0.213) while preserving semantic meaning.\n", "published": "10-08-2023", "year": "2023", "categories": ["Computation and Language"]}, {"title": "Zephyr: Direct Distillation of LM Alignment", "url": "http://arxiv.org/abs/2310.16944v1", "authors": "Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl\u00e9mentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, Thomas Wolf", "abstract": "  We aim to produce a smaller language model that is aligned to user intent.\nPrevious research has shown that applying distilled supervised fine-tuning\n(dSFT) on larger models significantly improves task accuracy; however, these\nmodels are unaligned, i.e. they do not respond well to natural prompts. To\ndistill this property, we experiment with the use of preference data from AI\nFeedback (AIF). Starting from a dataset of outputs ranked by a teacher model,\nwe apply distilled direct preference optimization (dDPO) to learn a chat model\nwith significantly improved intent alignment. The approach requires only a few\nhours of training without any additional sampling during fine-tuning. The final\nresult, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B\nparameter models, and requires no human annotation. In particular, results on\nMT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access\nRLHF-based model. Code, models, data, and tutorials for the system are\navailable at https://github.com/huggingface/alignment-handbook.\n", "published": "25-10-2023", "year": "2023", "categories": ["Machine Learning", "Computation and Language"]}, {"title": "Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations", "url": "http://arxiv.org/abs/2311.05584v1", "authors": "Joey Hong, Sergey Levine, Anca Dragan", "abstract": "  Large language models (LLMs) have emerged as powerful and general solutions\nto many natural language tasks. However, many of the most important\napplications of language generation are interactive, where an agent has to talk\nto a person to reach a desired outcome. For example, a teacher might try to\nunderstand their student's current comprehension level to tailor their\ninstruction accordingly, and a travel agent might ask questions of their\ncustomer to understand their preferences in order to recommend activities they\nmight enjoy. LLMs trained with supervised fine-tuning or \"single-step\" RL, as\nwith standard RLHF, might struggle which tasks that require such goal-directed\nbehavior, since they are not trained to optimize for overall conversational\noutcomes after multiple turns of interaction. In this work, we explore a new\nmethod for adapting LLMs with RL for such goal-directed dialogue. Our key\ninsight is that, though LLMs might not effectively solve goal-directed dialogue\ntasks out of the box, they can provide useful data for solving such tasks by\nsimulating suboptimal but human-like behaviors. Given a textual description of\na goal-directed dialogue task, we leverage LLMs to sample diverse synthetic\nrollouts of hypothetical in-domain human-human interactions. Our algorithm then\nutilizes this dataset with offline reinforcement learning to train an\ninteractive conversational agent that can optimize goal-directed objectives\nover multiple turns. In effect, the LLM produces examples of possible\ninteractions, and RL then processes these examples to learn to perform more\noptimal interactions. Empirically, we show that our proposed approach achieves\nstate-of-the-art performance in various goal-directed dialogue tasks that\ninclude teaching and preference elicitation.\n", "published": "09-11-2023", "year": "2023", "categories": ["Machine Learning", "Artificial Intelligence", "Computation and Language"]}, {"title": "Zero-shot Text Classification With Generative Language Models", "url": "http://arxiv.org/abs/1912.10165v1", "authors": "Raul Puri, Bryan Catanzaro", "abstract": "  This work investigates the use of natural language to enable zero-shot model\nadaptation to new tasks. We use text and metadata from social commenting\nplatforms as a source for a simple pretraining task. We then provide the\nlanguage model with natural language descriptions of classification tasks as\ninput and train it to generate the correct answer in natural language via a\nlanguage modeling objective. This allows the model to generalize to new\nclassification tasks without the need for multiple multitask classification\nheads. We show the zero-shot performance of these generative language models,\ntrained with weak supervision, on six benchmark text classification datasets\nfrom the torchtext library. Despite no access to training data, we achieve up\nto a 45% absolute improvement in classification accuracy over random or\nmajority class baselines. These results show that natural language can serve as\nsimple and powerful descriptors for task adaptation. We believe this points the\nway to new metalearning strategies for text problems.\n", "published": "10-12-2019", "year": "2019", "categories": ["Computation and Language"]}]