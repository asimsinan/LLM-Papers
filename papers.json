[{"2205.06175v3": {"paper_id": "2205.06175v3", "abs_url": "https://arxiv.org/abs/2205.06175v3", "pdf_url": "https://arxiv.org/pdf/2205.06175v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2205.06175v3_A_Generalist_Agent.pdf", "title": "A Generalist Agent", "year": 2022, "paper_venue": null, "authors": ["Scott Reed", "Konrad Zolna", "Emilio Parisotto", "Sergio Gomez Colmenarejo", "Alexander Novikov", "Gabriel Barth-Maron", "Mai Gimenez", "Yury Sulsky", "Jackie Kay", "Jost Tobias Springenberg", "Tom Eccles", "Jake Bruce", "Ali Razavi", "Ashley Edwards", "Nicolas Heess", "Yutian Chen", "Raia Hadsell", "Oriol Vinyals", "Mahyar Bordbar", "Nando de Freitas"], "abstract": "Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.", "comments": "Published at TMLR, 42 pages", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/a-generalist-agent", "bibtex": "@misc{reed2022generalist,\n      title={A Generalist Agent}, \n      author={Scott Reed and Konrad Zolna and Emilio Parisotto and Sergio Gomez Colmenarejo and Alexander Novikov and Gabriel Barth-Maron and Mai Gimenez and Yury Sulsky and Jackie Kay and Jost Tobias Springenberg and Tom Eccles and Jake Bruce and Ali Razavi and Ashley Edwards and Nicolas Heess and Yutian Chen and Raia Hadsell and Oriol Vinyals and Mahyar Bordbar and Nando de Freitas},\n      year={2022},\n      eprint={2205.06175},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "12-05-2022", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"]}, "2210.02441v3": {"paper_id": "2210.02441v3", "abs_url": "https://arxiv.org/abs/2210.02441v3", "pdf_url": "https://arxiv.org/pdf/2210.02441v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2210.02441v3_Ask_Me_Anything_A_simple_strategy_for_prompting_language_models.pdf", "title": "Ask Me Anything: A simple strategy for prompting language models", "year": 2022, "paper_venue": null, "authors": ["Simran Arora", "Avanika Narayan", "Mayee F. Chen", "Laurel Orr", "Neel Guha", "Kush Bhatia", "Ines Chami", "Frederic Sala", "Christopher R\u00e9"], "abstract": "", "comments": "", "official_code_urls": ["https://github.com/hazyresearch/ama_prompting"], "pwc_page_url": "https://paperswithcode.com/paper/ask-me-anything-a-simple-strategy-for", "bibtex": "@misc{arora2022ask,\n      title={Ask Me Anything: A simple strategy for prompting language models}, \n      author={Simran Arora and Avanika Narayan and Mayee F. Chen and Laurel Orr and Neel Guha and Kush Bhatia and Ines Chami and Frederic Sala and Christopher R\u00e9},\n      year={2022},\n      eprint={2210.02441},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "05-10-2022", "categories": ["cs.CL"]}, "2210.03493v1": {"paper_id": "2210.03493v1", "abs_url": "https://arxiv.org/abs/2210.03493v1", "pdf_url": "https://arxiv.org/pdf/2210.03493v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2210.03493v1_Automatic_Chain_of_Thought_Prompting_in_Large_Language_Models.pdf", "title": "Automatic Chain of Thought Prompting in Large Language Models", "year": 2022, "paper_venue": null, "authors": ["Zhuosheng Zhang", "Aston Zhang", "Mu Li", "Alex Smola"], "abstract": "", "comments": "", "official_code_urls": ["https://github.com/amazon-science/auto-cot"], "pwc_page_url": "https://paperswithcode.com/paper/automatic-chain-of-thought-prompting-in-large", "bibtex": "@misc{zhang2022automatic,\n      title={Automatic Chain of Thought Prompting in Large Language Models}, \n      author={Zhuosheng Zhang and Aston Zhang and Mu Li and Alex Smola},\n      year={2022},\n      eprint={2210.03493},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "07-10-2022", "categories": ["cs.CL", "cs.AI"]}, "2208.03188v3": {"paper_id": "2208.03188v3", "abs_url": "https://arxiv.org/abs/2208.03188v3", "pdf_url": "https://arxiv.org/pdf/2208.03188v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2208.03188v3_BlenderBot_3_a_deployed_conversational_agent_that_continually_learns_to_responsibly_engage.pdf", "title": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage", "year": 2022, "paper_venue": null, "authors": ["Kurt Shuster", "Jing Xu", "Mojtaba Komeili", "Da Ju", "Eric Michael Smith", "Stephen Roller", "Megan Ung", "Moya Chen", "Kushal Arora", "Joshua Lane", "Morteza Behrooz", "William Ngan", "Spencer Poff", "Naman Goyal", "Arthur Szlam", "Y-Lan Boureau", "Melanie Kambadur", "Jason Weston"], "abstract": "We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction.", "comments": "", "official_code_urls": ["https://github.com/facebookresearch/ParlAI/tree/main/projects/bb3"], "pwc_page_url": "https://paperswithcode.com/paper/blenderbot-3-a-deployed-conversational-agent", "bibtex": "@misc{shuster2022blenderbot,\n      title={BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage}, \n      author={Kurt Shuster and Jing Xu and Mojtaba Komeili and Da Ju and Eric Michael Smith and Stephen Roller and Megan Ung and Moya Chen and Kushal Arora and Joshua Lane and Morteza Behrooz and William Ngan and Spencer Poff and Naman Goyal and Arthur Szlam and Y-Lan Boureau and Melanie Kambadur and Jason Weston},\n      year={2022},\n      eprint={2208.03188},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "05-08-2022", "categories": ["cs.CL", "cs.AI"]}, "2201.11903v6": {"paper_id": "2201.11903v6", "abs_url": "https://arxiv.org/abs/2201.11903v6", "pdf_url": "https://arxiv.org/pdf/2201.11903v6.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2201.11903v6_Chain-of-Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models.pdf", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022, "paper_venue": null, "authors": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans", "Maarten Bosma", "Brian Ichter", "Fei Xia", "Ed Chi", "Quoc Le", "Denny Zhou"], "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/chain-of-thought-prompting-elicits-reasoning", "bibtex": "@misc{wei2023chainofthought,\n      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, \n      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},\n      year={2023},\n      eprint={2201.11903},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "28-01-2022", "categories": ["cs.CL", "cs.AI"]}, "2212.08073v1": {"paper_id": "2212.08073v1", "abs_url": "https://arxiv.org/abs/2212.08073v1", "pdf_url": "https://arxiv.org/pdf/2212.08073v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2212.08073v1_Constitutional_AI_Harmlessness_from_AI_Feedback.pdf", "title": "Constitutional AI: Harmlessness from AI Feedback", "year": 2022, "paper_venue": null, "authors": ["Yuntao Bai", "Saurav Kadavath", "Sandipan Kundu", "Amanda Askell", "Jackson Kernion", "Andy Jones", "Anna Chen", "Anna Goldie", "Azalia Mirhoseini", "Cameron McKinnon", "Carol Chen", "Catherine Olsson", "Christopher Olah", "Danny Hernandez", "Dawn Drain", "Deep Ganguli", "Dustin Li", "Eli Tran-Johnson", "Ethan Perez", "Jamie Kerr", "Jared Mueller", "Jeffrey Ladish", "Joshua Landau", "Kamal Ndousse", "Kamile Lukosuite", "Liane Lovitt", "Michael Sellitto", "Nelson Elhage", "Nicholas Schiefer", "Noemi Mercado", "Nova DasSarma", "Robert Lasenby", "Robin Larson", "Sam Ringer", "Scott Johnston", "Shauna Kravec", "Sheer El Showk", "Stanislav Fort", "Tamera Lanham", "Timothy Telleen-Lawton", "Tom Conerly", "Tom Henighan", "Tristan Hume", "Samuel R. Bowman", "Zac Hatfield-Dodds", "Ben Mann", "Dario Amodei", "Nicholas Joseph", "Sam McCandlish", "Tom Brown", "Jared Kaplan"], "abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.", "comments": "", "official_code_urls": ["https://github.com/anthropics/constitutionalharmlessnesspaper"], "pwc_page_url": "https://paperswithcode.com/paper/constitutional-ai-harmlessness-from-ai", "bibtex": "@misc{bai2022constitutional,\n      title={Constitutional AI: Harmlessness from AI Feedback}, \n      author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},\n      year={2022},\n      eprint={2212.08073},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "15-12-2022", "categories": ["cs.CL", "cs.AI"]}, "2212.14034v1": {"paper_id": "2212.14034v1", "abs_url": "https://arxiv.org/abs/2212.14034v1", "pdf_url": "https://arxiv.org/pdf/2212.14034v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2212.14034v1_Cramming_Training_a_Language_Model_on_a_Single_GPU_in_One_Day.pdf", "title": "Cramming: Training a Language Model on a Single GPU in One Day", "year": 2022, "paper_venue": null, "authors": ["Jonas Geiping", "Tom Goldstein"], "abstract": "We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to BERT, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting.", "comments": "22 pages, we provide code at this https URL", "official_code_urls": ["https://github.com/jonasgeiping/cramming"], "pwc_page_url": "https://paperswithcode.com/paper/cramming-training-a-language-model-on-a", "bibtex": "@misc{geiping2022cramming,\n      title={Cramming: Training a Language Model on a Single GPU in One Day}, \n      author={Jonas Geiping and Tom Goldstein},\n      year={2022},\n      eprint={2212.14034},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "28-12-2022", "categories": ["cs.CL", "cs.LG"]}, "2211.01786v2": {"paper_id": "2211.01786v2", "abs_url": "https://arxiv.org/abs/2211.01786v2", "pdf_url": "https://arxiv.org/pdf/2211.01786v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2211.01786v2_Crosslingual_Generalization_through_Multitask_Finetuning.pdf", "title": "Crosslingual Generalization through Multitask Finetuning", "year": 2022, "paper_venue": null, "authors": ["Niklas Muennighoff", "Thomas Wang", "Lintang Sutawika", "Adam Roberts", "Stella Biderman", "Teven Le Scao", "M Saiful Bari", "Sheng Shen", "Zheng-Xin Yong", "Hailey Schoelkopf", "Xiangru Tang", "Dragomir Radev", "Alham Fikri Aji", "Khalid Almubarak", "Samuel Albanie", "Zaid Alyafeai", "Albert Webson", "Edward Raff", "Colin Raffel"], "abstract": ".", "comments": "9 main pages (119 with appendix), 16 figures and 11 tables", "official_code_urls": ["https://github.com/bigscience-workshop/xmtf"], "pwc_page_url": "https://paperswithcode.com/paper/crosslingual-generalization-through-multitask", "bibtex": "@misc{muennighoff2023crosslingual,\n      title={Crosslingual Generalization through Multitask Finetuning}, \n      author={Niklas Muennighoff and Thomas Wang and Lintang Sutawika and Adam Roberts and Stella Biderman and Teven Le Scao and M Saiful Bari and Sheng Shen and Zheng-Xin Yong and Hailey Schoelkopf and Xiangru Tang and Dragomir Radev and Alham Fikri Aji and Khalid Almubarak and Samuel Albanie and Zaid Alyafeai and Albert Webson and Edward Raff and Colin Raffel},\n      year={2023},\n      eprint={2211.01786},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "03-11-2022", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2204.01691v2": {"paper_id": "2204.01691v2", "abs_url": "https://arxiv.org/abs/2204.01691v2", "pdf_url": "https://arxiv.org/pdf/2204.01691v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2204.01691v2_Do_As_I_Can_Not_As_I_Say_Grounding_Language_in_Robotic_Affordances.pdf", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances", "year": 2022, "paper_venue": null, "authors": ["Michael Ahn", "Anthony Brohan", "Noah Brown", "Yevgen Chebotar", "Omar Cortes", "Byron David", "Chelsea Finn", "Chuyuan Fu", "Keerthana Gopalakrishnan", "Karol Hausman", "Alex Herzog", "Daniel Ho", "Jasmine Hsu", "Julian Ibarz", "Brian Ichter", "Alex Irpan", "Eric Jang", "Rosario Jauregui Ruano", "Kyle Jeffrey", "Sally Jesmonth", "Nikhil J Joshi", "Ryan Julian", "Dmitry Kalashnikov", "Yuheng Kuang", "Kuang-Huei Lee", "Sergey Levine", "Yao Lu", "Linda Luu", "Carolina Parada", "Peter Pastor", "Jornell Quiambao", "Kanishka Rao", "Jarek Rettinghouse", "Diego Reyes", "Pierre Sermanet", "Nicolas Sievers", "Clayton Tan", "Alexander Toshev", "Vincent Vanhoucke", "Fei Xia", "Ted Xiao", "Peng Xu", "Sichun Xu", "Mengyuan Yan", "Andy Zeng"], "abstract": ".", "comments": "See website at this https URL V1. Initial Upload. V2. Added PaLM results. Added study about new capabilities (drawer manipulation, chain of thought prompting, multilingual instructions). Added an ablation study of language model size. Added an open-source version of \\algname on a simulated tabletop environment. Improved readability", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/do-as-i-can-not-as-i-say-grounding-language", "bibtex": "@misc{ahn2022i,\n      title={Do As I Can, Not As I Say: Grounding Language in Robotic Affordances}, \n      author={Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil J Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Reyes and Pierre Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan and Andy Zeng},\n      year={2022},\n      eprint={2204.01691},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO}\n}", "published_date": "04-04-2022", "categories": ["cs.RO", "cs.CL", "cs.LG"]}, "2206.07682v2": {"paper_id": "2206.07682v2", "abs_url": "https://arxiv.org/abs/2206.07682v2", "pdf_url": "https://arxiv.org/pdf/2206.07682v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2206.07682v2_Emergent_Abilities_of_Large_Language_Models.pdf", "title": "Emergent Abilities of Large Language Models", "year": 2022, "paper_venue": null, "authors": ["Jason Wei", "Yi Tay", "Rishi Bommasani", "Colin Raffel", "Barret Zoph", "Sebastian Borgeaud", "Dani Yogatama", "Maarten Bosma", "Denny Zhou", "Donald Metzler", "Ed H. Chi", "Tatsunori Hashimoto", "Oriol Vinyals", "Percy Liang", "Jeff Dean", "William Fedus"], "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.", "comments": "Transactions on Machine Learning Research (TMLR), 2022", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/emergent-abilities-of-large-language-models", "bibtex": "@misc{wei2022emergent,\n      title={Emergent Abilities of Large Language Models}, \n      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},\n      year={2022},\n      eprint={2206.07682},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "15-06-2022", "categories": ["cs.CL"]}, "2205.14135v2": {"paper_id": "2205.14135v2", "abs_url": "https://arxiv.org/abs/2205.14135v2", "pdf_url": "https://arxiv.org/pdf/2205.14135v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2205.14135v2_FlashAttention_Fast_and_Memory-Efficient_Exact_Attention_with_IO-Awareness.pdf", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "year": 2022, "paper_venue": null, "authors": ["Tri Dao", "Daniel Y. Fu", "Stefano Ermon", "Atri Rudra", "Christopher R\u00e9"], "abstract": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).", "comments": "", "official_code_urls": ["https://github.com/hazyresearch/flash-attention"], "pwc_page_url": "https://paperswithcode.com/paper/flashattention-fast-and-memory-efficient", "bibtex": "@misc{dao2022flashattention,\n      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, \n      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher R\u00e9},\n      year={2022},\n      eprint={2205.14135},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "27-05-2022", "categories": ["cs.LG"]}, "2206.05442v7": {"paper_id": "2206.05442v7", "abs_url": "https://arxiv.org/abs/2206.05442v7", "pdf_url": "https://arxiv.org/pdf/2206.05442v7.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2206.05442v7_From_Human_Days_to_Machine_Seconds_Automatically_Answering_and_Generating_Machine_Learning_Final_Exams.pdf", "title": "From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams", "year": 2022, "paper_venue": null, "authors": ["Iddo Drori", "Sarah J. Zhang", "Reece Shuttleworth", "Sarah Zhang", "Keith Tyser", "Zad Chin", "Pedro Lantigua", "Saisamrit Surbehera", "Gregory Hunter", "Derek Austin", "Leonard Tang", "Yann Hicke", "Sage Simhon", "Sathwik Karnik", "Darnell Granberry", "Madeleine Udell"], "abstract": "A final exam in machine learning at a top institution such as MIT, Harvard, or Cornell typically takes faculty days to write, and students hours to solve. We demonstrate that large language models pass machine learning finals at a human level, on finals available online after the models were trained, and automatically generate new human-quality final exam questions in seconds. Previous work has developed program synthesis and few-shot learning methods to solve university-level problem set questions in mathematics and STEM courses. In this work, we develop and compare methods that solve final exams, which differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We curate a dataset and benchmark of questions from machine learning final exams available online and code for answering these questions and generating new questions. We show how to generate new questions from other questions and course notes. For reproducibility and future research on this final exam benchmark, we use automatic checkers for multiple-choice, numeric, and questions with expression answers. We perform ablation studies comparing zero-shot learning with few-shot learning and chain-of-thought prompting using GPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that few-shot learning methods perform best. We highlight the transformative potential of language models to streamline the writing and solution of large-scale assessments, significantly reducing the workload from human days to mere machine seconds. Our results suggest that rather than banning large language models such as ChatGPT in class, instructors should teach students to harness them by asking students meta-questions about correctness, completeness, and originality of the responses generated, encouraging critical thinking in academic studies.", "comments": "9 pages", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/a-dataset-and-benchmark-for-automatically", "bibtex": "@misc{drori2023human,\n      title={From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams}, \n      author={Iddo Drori and Sarah J. Zhang and Reece Shuttleworth and Sarah Zhang and Keith Tyser and Zad Chin and Pedro Lantigua and Saisamrit Surbehera and Gregory Hunter and Derek Austin and Leonard Tang and Yann Hicke and Sage Simhon and Sathwik Karnik and Darnell Granberry and Madeleine Udell},\n      year={2023},\n      eprint={2206.05442},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "11-06-2022", "categories": ["cs.LG"]}, "2211.09527v1": {"paper_id": "2211.09527v1", "abs_url": "https://arxiv.org/abs/2211.09527v1", "pdf_url": "https://arxiv.org/pdf/2211.09527v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2211.09527v1_Ignore_Previous_Prompt_Attack_Techniques_For_Language_Models.pdf", "title": "Ignore Previous Prompt: Attack Techniques For Language Models", "year": 2022, "paper_venue": null, "authors": ["F\u00e1bio Perez", "Ian Ribeiro"], "abstract": ".", "comments": "ML Safety Workshop NeurIPS 2022", "official_code_urls": ["https://github.com/agencyenterprise/promptinject"], "pwc_page_url": "https://paperswithcode.com/paper/ignore-previous-prompt-attack-techniques-for", "bibtex": "@misc{perez2022ignore,\n      title={Ignore Previous Prompt: Attack Techniques For Language Models}, \n      author={F\u00e1bio Perez and Ian Ribeiro},\n      year={2022},\n      eprint={2211.09527},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "17-11-2022", "categories": ["cs.CL", "cs.AI"]}, "2204.13778v2": {"paper_id": "2204.13778v2", "abs_url": "https://arxiv.org/abs/2204.13778v2", "pdf_url": "https://arxiv.org/pdf/2204.13778v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2204.13778v2_Inferring_Implicit_Relations_in_Complex_Questions_with_Language_Models.pdf", "title": "Inferring Implicit Relations in Complex Questions with Language Models", "year": 2022, "paper_venue": null, "authors": ["Uri Katz", "Mor Geva", "Jonathan Berant"], "abstract": "A prominent challenge for modern language understanding systems is the ability to answer implicit reasoning questions, where the required reasoning steps for answering the question are not mentioned in the text explicitly. In this work, we investigate why current models struggle with implicit reasoning question answering (QA) tasks, by decoupling inference of reasoning steps from their execution. We define a new task of implicit relation inference and construct a benchmark, IMPLICITRELATIONS, where given a question, a model should output a list of concept-relation pairs, where the relations describe the implicit reasoning steps required for answering the question. Using IMPLICITRELATIONS, we evaluate models from the GPT-3 family and find that, while these models struggle on the implicit reasoning QA task, they often succeed at inferring implicit relations. This suggests that the challenge in implicit reasoning questions does not stem from the need to plan a reasoning strategy alone, but to do it while also retrieving and reasoning over relevant information.", "comments": "Findings of EMNLP 2022", "official_code_urls": ["https://github.com/katzurik/implicitrelations"], "pwc_page_url": "https://paperswithcode.com/paper/inferring-implicit-relations-with-language", "bibtex": "@misc{katz2022inferring,\n      title={Inferring Implicit Relations in Complex Questions with Language Models}, \n      author={Uri Katz and Mor Geva and Jonathan Berant},\n      year={2022},\n      eprint={2204.13778},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "28-04-2022", "categories": ["cs.CL"]}, "2207.05608v1": {"paper_id": "2207.05608v1", "abs_url": "https://arxiv.org/abs/2207.05608v1", "pdf_url": "https://arxiv.org/pdf/2207.05608v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2207.05608v1_Inner_Monologue_Embodied_Reasoning_through_Planning_with_Language_Models.pdf", "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models", "year": 2022, "paper_venue": null, "authors": ["Wenlong Huang", "Fei Xia", "Ted Xiao", "Harris Chan", "Jacky Liang", "Pete Florence", "Andy Zeng", "Jonathan Tompson", "Igor Mordatch", "Yevgen Chebotar", "Pierre Sermanet", "Noah Brown", "Tomas Jackson", "Linda Luu", "Sergey Levine", "Karol Hausman", "Brian Ichter"], "abstract": "Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.", "comments": "Project website: this https URL", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/inner-monologue-embodied-reasoning-through", "bibtex": "@misc{huang2022inner,\n      title={Inner Monologue: Embodied Reasoning through Planning with Language Models}, \n      author={Wenlong Huang and Fei Xia and Ted Xiao and Harris Chan and Jacky Liang and Pete Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and Pierre Sermanet and Noah Brown and Tomas Jackson and Linda Luu and Sergey Levine and Karol Hausman and Brian Ichter},\n      year={2022},\n      eprint={2207.05608},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO}\n}", "published_date": "12-07-2022", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"]}, "2210.06407v1": {"paper_id": "2210.06407v1", "abs_url": "https://arxiv.org/abs/2210.06407v1", "pdf_url": "https://arxiv.org/pdf/2210.06407v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2210.06407v1_Interactive_Language_Talking_to_Robots_in_Real_Time.pdf", "title": "Interactive Language: Talking to Robots in Real Time", "year": 2022, "paper_venue": null, "authors": ["Corey Lynch", "Ayzaan Wahid", "Jonathan Tompson", "Tianli Ding", "James Betker", "Robert Baruch", "Travis Armstrong", "Pete Florence"], "abstract": ".", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/interactive-language-talking-to-robots-in", "bibtex": "@misc{lynch2022interactive,\n      title={Interactive Language: Talking to Robots in Real Time}, \n      author={Corey Lynch and Ayzaan Wahid and Jonathan Tompson and Tianli Ding and James Betker and Robert Baruch and Travis Armstrong and Pete Florence},\n      year={2022},\n      eprint={2210.06407},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO}\n}", "published_date": "12-10-2022", "categories": ["cs.RO", "cs.AI", "cs.LG"]}, "2212.10650v1": {"paper_id": "2212.10650v1", "abs_url": "https://arxiv.org/abs/2212.10650v1", "pdf_url": "https://arxiv.org/pdf/2212.10650v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2212.10650v1_KronA_Parameter_Efficient_Tuning_with_Kronecker_Adapter.pdf", "title": "KronA: Parameter Efficient Tuning with Kronecker Adapter", "year": 2022, "paper_venue": null, "authors": ["Ali Edalati", "Marzieh Tahaei", "Ivan Kobyzev", "Vahid Partovi Nia", "James J. Clark", "Mehdi Rezagholizadeh"], "abstract": "Fine-tuning a Pre-trained Language Model (PLM) on a specific downstream task has been a well-known paradigm in Natural Language Processing. However, with the ever-growing size of PLMs, training the entire model on several downstream tasks becomes very expensive and resource-hungry. Recently, different Parameter Efficient Tuning (PET) techniques are proposed to improve the efficiency of fine-tuning PLMs. One popular category of PET methods is the low-rank adaptation methods which insert learnable truncated SVD modules into the original model either sequentially or in parallel. However, low-rank decomposition suffers from limited representation power. In this work, we address this problem using the Kronecker product instead of the low-rank representation. We introduce KronA, a Kronecker product-based adapter module for efficient fine-tuning of Transformer-based PLMs. We apply the proposed methods for fine-tuning T5 on the GLUE benchmark to show that incorporating the Kronecker-based modules can outperform state-of-the-art PET methods.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/krona-parameter-efficient-tuning-with", "bibtex": "@misc{edalati2022krona,\n      title={KronA: Parameter Efficient Tuning with Kronecker Adapter}, \n      author={Ali Edalati and Marzieh Tahaei and Ivan Kobyzev and Vahid Partovi Nia and James J. Clark and Mehdi Rezagholizadeh},\n      year={2022},\n      eprint={2212.10650},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "20-12-2022", "categories": ["cs.CL"]}, "2212.04088v3": {"paper_id": "2212.04088v3", "abs_url": "https://arxiv.org/abs/2212.04088v3", "pdf_url": "https://arxiv.org/pdf/2212.04088v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2212.04088v3_LLM-Planner_Few-Shot_Grounded_Planning_for_Embodied_Agents_with_Large_Language_Models.pdf", "title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models", "year": 2022, "paper_venue": null, "authors": ["Chan Hee Song", "Jiaman Wu", "Clayton Washington", "Brian M. Sadler", "Wei-Lun Chao", "Yu Su"], "abstract": "", "comments": "14 pages, 5 figures", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/llm-planner-few-shot-grounded-planning-for", "bibtex": "@misc{song2023llmplanner,\n      title={LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models}, \n      author={Chan Hee Song and Jiaman Wu and Clayton Washington and Brian M. Sadler and Wei-Lun Chao and Yu Su},\n      year={2023},\n      eprint={2212.04088},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "08-12-2022", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"]}, "2207.04429v2": {"paper_id": "2207.04429v2", "abs_url": "https://arxiv.org/abs/2207.04429v2", "pdf_url": "https://arxiv.org/pdf/2207.04429v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2207.04429v2_LM-Nav_Robotic_Navigation_with_Large_Pre-Trained_Models_of_Language_Vision_and_Action.pdf", "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action", "year": 2022, "paper_venue": null, "authors": ["Dhruv Shah", "Blazej Osinski", "Brian Ichter", "Sergey Levine"], "abstract": "", "comments": "Project page this https URL", "official_code_urls": ["https://github.com/blazejosinski/lm_nav"], "pwc_page_url": "https://paperswithcode.com/paper/lm-nav-robotic-navigation-with-large-pre", "bibtex": "@misc{shah2022lmnav,\n      title={LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action}, \n      author={Dhruv Shah and Blazej Osinski and Brian Ichter and Sergey Levine},\n      year={2022},\n      eprint={2207.04429},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO}\n}", "published_date": "10-07-2022", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"]}, "2210.03057v1": {"paper_id": "2210.03057v1", "abs_url": "https://arxiv.org/abs/2210.03057v1", "pdf_url": "https://arxiv.org/pdf/2210.03057v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2210.03057v1_Language_Models_are_Multilingual_Chain-of-Thought_Reasoners.pdf", "title": "Language Models are Multilingual Chain-of-Thought Reasoners", "year": 2022, "paper_venue": null, "authors": ["Freda Shi", "Mirac Suzgun", "Markus Freitag", "Xuezhi Wang", "Suraj Srivats", "Soroush Vosoughi", "Hyung Won Chung", "Yi Tay", "Sebastian Ruder", "Denny Zhou", "Dipanjan Das", "Jason Wei"], "abstract": ".", "comments": "", "official_code_urls": ["https://github.com/google-research/url-nlp"], "pwc_page_url": "https://paperswithcode.com/paper/language-models-are-multilingual-chain-of", "bibtex": "@misc{shi2022language,\n      title={Language Models are Multilingual Chain-of-Thought Reasoners}, \n      author={Freda Shi and Mirac Suzgun and Markus Freitag and Xuezhi Wang and Suraj Srivats and Soroush Vosoughi and Hyung Won Chung and Yi Tay and Sebastian Ruder and Denny Zhou and Dipanjan Das and Jason Wei},\n      year={2022},\n      eprint={2210.03057},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "06-10-2022", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2201.07207v2": {"paper_id": "2201.07207v2", "abs_url": "https://arxiv.org/abs/2201.07207v2", "pdf_url": "https://arxiv.org/pdf/2201.07207v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2201.07207v2_Language_Models_as_Zero-Shot_Planners_Extracting_Actionable_Knowledge_for_Embodied_Agents.pdf", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents", "year": 2022, "paper_venue": null, "authors": ["Wenlong Huang", "Pieter Abbeel", "Deepak Pathak", "Igor Mordatch"], "abstract": "", "comments": "Project website at this https URL", "official_code_urls": ["https://github.com/huangwl18/language-planner"], "pwc_page_url": "https://paperswithcode.com/paper/language-models-as-zero-shot-planners-1", "bibtex": "@misc{huang2022language,\n      title={Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents}, \n      author={Wenlong Huang and Pieter Abbeel and Deepak Pathak and Igor Mordatch},\n      year={2022},\n      eprint={2201.07207},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "18-01-2022", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.RO"]}, "2210.07128v3": {"paper_id": "2210.07128v3", "abs_url": "https://arxiv.org/abs/2210.07128v3", "pdf_url": "https://arxiv.org/pdf/2210.07128v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2210.07128v3_Language_Models_of_Code_are_Few-Shot_Commonsense_Learners.pdf", "title": "Language Models of Code are Few-Shot Commonsense Learners", "year": 2022, "paper_venue": null, "authors": ["Aman Madaan", "Shuyan Zhou", "Uri Alon", "Yiming Yang", "Graham Neubig"], "abstract": "We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event -- or a reasoning-graph. To employ large language models (LMs) for this task, existing approaches ``serialize'' the output graph as a flat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the target task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot setting.", "comments": "EMNLP 2022", "official_code_urls": ["https://github.com/madaan/cocogen"], "pwc_page_url": "https://paperswithcode.com/paper/language-models-of-code-are-few-shot", "bibtex": "@misc{madaan2022language,\n      title={Language Models of Code are Few-Shot Commonsense Learners}, \n      author={Aman Madaan and Shuyan Zhou and Uri Alon and Yiming Yang and Graham Neubig},\n      year={2022},\n      eprint={2210.07128},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "13-10-2022", "categories": ["cs.CL", "cs.LG"]}, "2211.01910v2": {"paper_id": "2211.01910v2", "abs_url": "https://arxiv.org/abs/2211.01910v2", "pdf_url": "https://arxiv.org/pdf/2211.01910v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2211.01910v2_Large_Language_Models_Are_Human-Level_Prompt_Engineers.pdf", "title": "Large Language Models Are Human-Level Prompt Engineers", "year": 2022, "paper_venue": null, "authors": ["Yongchao Zhou", "Andrei Ioan Muresanu", "Ziwen Han", "Keiran Paster", "Silviu Pitis", "Harris Chan", "Jimmy Ba"], "abstract": ".", "comments": "", "official_code_urls": ["https://github.com/keirp/automatic_prompt_engineer"], "pwc_page_url": "https://paperswithcode.com/paper/large-language-models-are-human-level-prompt", "bibtex": "@misc{zhou2023large,\n      title={Large Language Models Are Human-Level Prompt Engineers}, \n      author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},\n      year={2023},\n      eprint={2211.01910},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "03-11-2022", "categories": ["cs.LG", "cs.AI", "cs.CL"]}, "2212.10071v2": {"paper_id": "2212.10071v2", "abs_url": "https://arxiv.org/abs/2212.10071v2", "pdf_url": "https://arxiv.org/pdf/2212.10071v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2212.10071v2_Large_Language_Models_Are_Reasoning_Teachers.pdf", "title": "Large Language Models Are Reasoning Teachers", "year": 2022, "paper_venue": null, "authors": ["Namgyu Ho", "Laura Schmid", "Se-Young Yun"], "abstract": ".", "comments": "ACL 2023 camera-ready", "official_code_urls": ["https://github.com/itsnamgyu/reasoning-teacher"], "pwc_page_url": "https://paperswithcode.com/paper/large-language-models-are-reasoning-teachers", "bibtex": "@misc{ho2023large,\n      title={Large Language Models Are Reasoning Teachers}, \n      author={Namgyu Ho and Laura Schmid and Se-Young Yun},\n      year={2023},\n      eprint={2212.10071},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "20-12-2022", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2209.09513v2": {"paper_id": "2209.09513v2", "abs_url": "https://arxiv.org/abs/2209.09513v2", "pdf_url": "https://arxiv.org/pdf/2209.09513v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2209.09513v2_Learn_to_Explain_Multimodal_Reasoning_via_Thought_Chains_for_Science_Question_Answering.pdf", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering", "year": 2022, "paper_venue": null, "authors": ["Pan Lu", "Swaroop Mishra", "Tony Xia", "Liang Qiu", "Kai-Wei Chang", "Song-Chun Zhu", "Oyvind Tafjord", "Peter Clark", "Ashwin Kalyan"], "abstract": ".", "comments": "Accepted to NeurIPS 2022. 22 pages, 17 figures, 9 tables. Project: this https URL", "official_code_urls": ["https://github.com/lupantech/ScienceQA"], "pwc_page_url": "https://paperswithcode.com/paper/learn-to-explain-multimodal-reasoning-via", "bibtex": "@misc{lu2022learn,\n      title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering}, \n      author={Pan Lu and Swaroop Mishra and Tony Xia and Liang Qiu and Kai-Wei Chang and Song-Chun Zhu and Oyvind Tafjord and Peter Clark and Ashwin Kalyan},\n      year={2022},\n      eprint={2209.09513},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "20-09-2022", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"]}, "2205.10625v3": {"paper_id": "2205.10625v3", "abs_url": "https://arxiv.org/abs/2205.10625v3", "pdf_url": "https://arxiv.org/pdf/2205.10625v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2205.10625v3_Least-to-Most_Prompting_Enables_Complex_Reasoning_in_Large_Language_Models.pdf", "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "year": 2022, "paper_venue": null, "authors": ["Denny Zhou", "Nathanael Sch\u00e4rli", "Le Hou", "Jason Wei", "Nathan Scales", "Xuezhi Wang", "Dale Schuurmans", "Claire Cui", "Olivier Bousquet", "Quoc Le", "Ed Chi"], "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.", "comments": "ICLR 2023", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/least-to-most-prompting-enables-complex", "bibtex": "@misc{zhou2023leasttomost,\n      title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models}, \n      author={Denny Zhou and Nathanael Sch\u00e4rli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc Le and Ed Chi},\n      year={2023},\n      eprint={2205.10625},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "21-05-2022", "categories": ["cs.AI", "cs.CL"]}, "2206.02336v3": {"paper_id": "2206.02336v3", "abs_url": "https://arxiv.org/abs/2206.02336v3", "pdf_url": "https://arxiv.org/pdf/2206.02336v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2206.02336v3_Making_Large_Language_Models_Better_Reasoners_with_Step-Aware_Verifier.pdf", "title": "Making Large Language Models Better Reasoners with Step-Aware Verifier", "year": 2022, "paper_venue": null, "authors": ["Yifei Li", "Zeqi Lin", "Shizhuo Zhang", "Qiang Fu", "Bei Chen", "Jian-Guang Lou", "Weizhu Chen"], "abstract": "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DIVERSE has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DIVERSE on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/on-the-advance-of-making-language-models", "bibtex": "@misc{li2023making,\n      title={Making Large Language Models Better Reasoners with Step-Aware Verifier}, \n      author={Yifei Li and Zeqi Lin and Shizhuo Zhang and Qiang Fu and Bei Chen and Jian-Guang Lou and Weizhu Chen},\n      year={2023},\n      eprint={2206.02336},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "06-06-2022", "categories": ["cs.CL", "cs.AI"]}, "2206.08853v2": {"paper_id": "2206.08853v2", "abs_url": "https://arxiv.org/abs/2206.08853v2", "pdf_url": "https://arxiv.org/pdf/2206.08853v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2206.08853v2_MineDojo_Building_Open-Ended_Embodied_Agents_with_Internet-Scale_Knowledge.pdf", "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge", "year": 2022, "paper_venue": null, "authors": ["Linxi Fan", "Guanzhi Wang", "Yunfan Jiang", "Ajay Mandlekar", "Yuncong Yang", "Haoyi Zhu", "Andrew Tang", "De-An Huang", "Yuke Zhu", "Anima Anandkumar"], "abstract": ") to promote research towards the goal of generally capable embodied agents.", "comments": "Outstanding Paper Award at NeurIPS 2022. Project website: this https URL", "official_code_urls": ["https://github.com/MineDojo/MineDojo"], "pwc_page_url": "https://paperswithcode.com/paper/minedojo-building-open-ended-embodied-agents", "bibtex": "@misc{fan2022minedojo,\n      title={MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge}, \n      author={Linxi Fan and Guanzhi Wang and Yunfan Jiang and Ajay Mandlekar and Yuncong Yang and Haoyi Zhu and Andrew Tang and De-An Huang and Yuke Zhu and Anima Anandkumar},\n      year={2022},\n      eprint={2206.08853},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "17-06-2022", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"]}, "2211.10435v2": {"paper_id": "2211.10435v2", "abs_url": "https://arxiv.org/abs/2211.10435v2", "pdf_url": "https://arxiv.org/pdf/2211.10435v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2211.10435v2_PAL_Program-aided_Language_Models.pdf", "title": "PAL: Program-aided Language Models", "year": 2022, "paper_venue": null, "authors": ["Luyu Gao", "Aman Madaan", "Shuyan Zhou", "Uri Alon", "Pengfei Liu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "abstract": ".", "comments": "The first three authors contributed equally. Our code and data are publicly available at this http URL", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/pal-program-aided-language-models", "bibtex": "@misc{gao2023pal,\n      title={PAL: Program-aided Language Models}, \n      author={Luyu Gao and Aman Madaan and Shuyan Zhou and Uri Alon and Pengfei Liu and Yiming Yang and Jamie Callan and Graham Neubig},\n      year={2023},\n      eprint={2211.10435},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "18-11-2022", "categories": ["cs.CL", "cs.AI"]}, "2209.11302v1": {"paper_id": "2209.11302v1", "abs_url": "https://arxiv.org/abs/2209.11302v1", "pdf_url": "https://arxiv.org/pdf/2209.11302v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2209.11302v1_ProgPrompt_Generating_Situated_Robot_Task_Plans_using_Large_Language_Models.pdf", "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models", "year": 2022, "paper_venue": null, "authors": ["Ishika Singh", "Valts Blukis", "Arsalan Mousavian", "Ankit Goyal", "Danfei Xu", "Jonathan Tremblay", "Dieter Fox", "Jesse Thomason", "Animesh Garg"], "abstract": "", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/progprompt-generating-situated-robot-task", "bibtex": "@misc{singh2022progprompt,\n      title={ProgPrompt: Generating Situated Robot Task Plans using Large Language Models}, \n      author={Ishika Singh and Valts Blukis and Arsalan Mousavian and Ankit Goyal and Danfei Xu and Jonathan Tremblay and Dieter Fox and Jesse Thomason and Animesh Garg},\n      year={2022},\n      eprint={2209.11302},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO}\n}", "published_date": "22-09-2022", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"]}, "2211.12588v4": {"paper_id": "2211.12588v4", "abs_url": "https://arxiv.org/abs/2211.12588v4", "pdf_url": "https://arxiv.org/pdf/2211.12588v4.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2211.12588v4_Program_of_Thoughts_Prompting_Disentangling_Computation_from_Reasoning_for_Numerical_Reasoning_Tasks.pdf", "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks", "year": 2022, "paper_venue": null, "authors": ["Wenhu Chen", "Xueguang Ma", "Xinyi Wang", "William W. Cohen"], "abstract": "", "comments": "Published at TMLR 2023", "official_code_urls": ["https://github.com/wenhuchen/program-of-thoughts"], "pwc_page_url": "https://paperswithcode.com/paper/program-of-thoughts-prompting-disentangling", "bibtex": "@misc{chen2023program,\n      title={Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks}, \n      author={Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},\n      year={2023},\n      eprint={2211.12588},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "22-11-2022", "categories": ["cs.CL", "cs.AI"]}, "2209.11755v1": {"paper_id": "2209.11755v1", "abs_url": "https://arxiv.org/abs/2209.11755v1", "pdf_url": "https://arxiv.org/pdf/2209.11755v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2209.11755v1_Promptagator_Few-shot_Dense_Retrieval_From_8_Examples.pdf", "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples", "year": 2022, "paper_venue": null, "authors": ["Zhuyun Dai", "Vincent Y. Zhao", "Ji Ma", "Yi Luan", "Jianmo Ni", "Jing Lu", "Anton Bakalov", "Kelvin Guu", "Keith B. Hall", "Ming-Wei Chang"], "abstract": "Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by LLM's generalization ability, Promptagator makes it possible to create task-specific end-to-end retrievers solely based on a few examples {without} using Natural Questions or MS MARCO to train %question generators or dual encoders. Surprisingly, LLM prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on MS MARCO like ColBERT v2 by more than 1.2 nDCG on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point nDCG improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/promptagator-few-shot-dense-retrieval-from-8", "bibtex": "@misc{dai2022promptagator,\n      title={Promptagator: Few-shot Dense Retrieval From 8 Examples}, \n      author={Zhuyun Dai and Vincent Y. Zhao and Ji Ma and Yi Luan and Jianmo Ni and Jing Lu and Anton Bakalov and Kelvin Guu and Keith B. Hall and Ming-Wei Chang},\n      year={2022},\n      eprint={2209.11755},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "23-09-2022", "categories": ["cs.CL", "cs.IR"]}, "2210.03629v3": {"paper_id": "2210.03629v3", "abs_url": "https://arxiv.org/abs/2210.03629v3", "pdf_url": "https://arxiv.org/pdf/2210.03629v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2210.03629v3_ReAct_Synergizing_Reasoning_and_Acting_in_Language_Models.pdf", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "year": 2022, "paper_venue": null, "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "abstract": "", "comments": "v3 is the ICLR camera ready version with some typos fixed. Project site with code: this https URL", "official_code_urls": ["https://github.com/ysymyth/ReAct"], "pwc_page_url": "https://paperswithcode.com/paper/react-synergizing-reasoning-and-acting-in", "bibtex": "@misc{yao2023react,\n      title={ReAct: Synergizing Reasoning and Acting in Language Models}, \n      author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},\n      year={2023},\n      eprint={2210.03629},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "06-10-2022", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2212.09597v8": {"paper_id": "2212.09597v8", "abs_url": "https://arxiv.org/abs/2212.09597v8", "pdf_url": "https://arxiv.org/pdf/2212.09597v8.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2212.09597v8_Reasoning_with_Language_Model_Prompting_A_Survey.pdf", "title": "Reasoning with Language Model Prompting: A Survey", "year": 2022, "paper_venue": null, "authors": ["Shuofei Qiao", "Yixin Ou", "Ningyu Zhang", "Xiang Chen", "Yunzhi Yao", "Shumin Deng", "Chuanqi Tan", "Fei Huang", "Huajun Chen"], "abstract": "(updated periodically).", "comments": "ACL 2023, 24 pages, add references of theoretical analysis", "official_code_urls": ["https://github.com/zjunlp/Prompt4ReasoningPapers"], "pwc_page_url": "https://paperswithcode.com/paper/reasoning-with-language-model-prompting-a", "bibtex": "@misc{qiao2023reasoning,\n      title={Reasoning with Language Model Prompting: A Survey}, \n      author={Shuofei Qiao and Yixin Ou and Ningyu Zhang and Xiang Chen and Yunzhi Yao and Shumin Deng and Chuanqi Tan and Fei Huang and Huajun Chen},\n      year={2023},\n      eprint={2212.09597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "19-12-2022", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"]}, "2205.09712v1": {"paper_id": "2205.09712v1", "abs_url": "https://arxiv.org/abs/2205.09712v1", "pdf_url": "https://arxiv.org/pdf/2205.09712v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2205.09712v1_Selection-Inference_Exploiting_Large_Language_Models_for_Interpretable_Logical_Reasoning.pdf", "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning", "year": 2022, "paper_venue": null, "authors": ["Antonia Creswell", "Murray Shanahan", "Irina Higgins"], "abstract": "Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/selection-inference-exploiting-large-language", "bibtex": "@misc{creswell2022selectioninference,\n      title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning}, \n      author={Antonia Creswell and Murray Shanahan and Irina Higgins},\n      year={2022},\n      eprint={2205.09712},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "19-05-2022", "categories": ["cs.AI", "cs.CL"]}, "2203.11171v4": {"paper_id": "2203.11171v4", "abs_url": "https://arxiv.org/abs/2203.11171v4", "pdf_url": "https://arxiv.org/pdf/2203.11171v4.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2203.11171v4_Self-Consistency_Improves_Chain_of_Thought_Reasoning_in_Language_Models.pdf", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "year": 2022, "paper_venue": null, "authors": ["Xuezhi Wang", "Jason Wei", "Dale Schuurmans", "Quoc Le", "Ed Chi", "Sharan Narang", "Aakanksha Chowdhery", "Denny Zhou"], "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).", "comments": "Published at ICLR 2023. V2: added PaLM results; V3: added UL2 results; V4: camera ready version at ICLR 2023", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/self-consistency-improves-chain-of-thought", "bibtex": "@misc{wang2023selfconsistency,\n      title={Self-Consistency Improves Chain of Thought Reasoning in Language Models}, \n      author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},\n      year={2023},\n      eprint={2203.11171},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "21-03-2022", "categories": ["cs.CL", "cs.AI"]}, "2212.10560v2": {"paper_id": "2212.10560v2", "abs_url": "https://arxiv.org/abs/2212.10560v2", "pdf_url": "https://arxiv.org/pdf/2212.10560v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2212.10560v2_Self-Instruct_Aligning_Language_Models_with_Self-Generated_Instructions.pdf", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "year": 2022, "paper_venue": null, "authors": ["Yizhong Wang", "Yeganeh Kordi", "Swaroop Mishra", "Alisa Liu", "Noah A. Smith", "Daniel Khashabi", "Hannaneh Hajishirzi"], "abstract": ".", "comments": "ACL 2023 camera ready, 23 pages, 9 figures, 11 tables", "official_code_urls": ["https://github.com/yizhongw/self-instruct", "https://github.com/tatsu-lab/stanford_alpaca"], "pwc_page_url": "https://paperswithcode.com/paper/self-instruct-aligning-language-model-with", "bibtex": "@misc{wang2023selfinstruct,\n      title={Self-Instruct: Aligning Language Models with Self-Generated Instructions}, \n      author={Yizhong Wang and Yeganeh Kordi and Swaroop Mishra and Alisa Liu and Noah A. Smith and Daniel Khashabi and Hannaneh Hajishirzi},\n      year={2023},\n      eprint={2212.10560},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "20-12-2022", "categories": ["cs.CL", "cs.AI"]}, "2204.00598v2": {"paper_id": "2204.00598v2", "abs_url": "https://arxiv.org/abs/2204.00598v2", "pdf_url": "https://arxiv.org/pdf/2204.00598v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2204.00598v2_Socratic_Models_Composing_Zero-Shot_Multimodal_Reasoning_with_Language.pdf", "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", "year": 2022, "paper_venue": null, "authors": ["Andy Zeng", "Maria Attarian", "Brian Ichter", "Krzysztof Choromanski", "Adrian Wong", "Stefan Welker", "Federico Tombari", "Aveek Purohit", "Michael Ryoo", "Vikas Sindhwani", "Johnny Lee", "Vincent Vanhoucke", "Pete Florence"], "abstract": "Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.", "comments": "this https URL", "official_code_urls": ["https://github.com/google-research/google-research/tree/master/socraticmodels"], "pwc_page_url": "https://paperswithcode.com/paper/socratic-models-composing-zero-shot", "bibtex": "@misc{zeng2022socratic,\n      title={Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language}, \n      author={Andy Zeng and Maria Attarian and Brian Ichter and Krzysztof Choromanski and Adrian Wong and Stefan Welker and Federico Tombari and Aveek Purohit and Michael Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Pete Florence},\n      year={2022},\n      eprint={2204.00598},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "01-04-2022", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"]}, "2209.07686v2": {"paper_id": "2209.07686v2", "abs_url": "https://arxiv.org/abs/2209.07686v2", "pdf_url": "https://arxiv.org/pdf/2209.07686v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2209.07686v2_Text_and_Patterns_For_Effective_Chain_of_Thought_It_Takes_Two_to_Tango.pdf", "title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango", "year": 2022, "paper_venue": null, "authors": ["Aman Madaan", "Amir Yazdanbakhsh"], "abstract": "The past decade has witnessed dramatic gains in natural language processing and an unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (CoT) prompting. Specifically, CoT pushes the performance of large language models in a few-shot setup by augmenting the prompts with intermediate steps. Despite impressive results across various tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of CoT-based few-shot prompting mechanisms in large language models. We first systematically identify and define the key components of a prompt: symbols, patterns, and text. Then, we devise and conduct an exhaustive set of experiments across four different tasks, by querying the model with counterfactual prompts where only one of these components is altered. Our experiments across three models (PaLM, GPT-3, and CODEX) reveal several surprising findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success of CoT. Second, our results conclude that the primary role of intermediate steps may not be to facilitate learning how to solve a task. The intermediate steps are rather a beacon for the model to realize what symbols to replicate in the output to form a factual answer. Further, text imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis reveals that a symbiotic relationship between text and patterns explains the success of few-shot prompting: text helps extract commonsense from the question to help patterns, and patterns enforce task understanding and direct text generation.", "comments": "Shortened version with additional results from CODEX and GPT-3. The authors contributed equally. Work done when Aman Madaan was a student researcher at Google Research, Brain Team", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/text-and-patterns-for-effective-chain-of", "bibtex": "@misc{madaan2022text,\n      title={Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango}, \n      author={Aman Madaan and Amir Yazdanbakhsh},\n      year={2022},\n      eprint={2209.07686},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "16-09-2022", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2205.03401v2": {"paper_id": "2205.03401v2", "abs_url": "https://arxiv.org/abs/2205.03401v2", "pdf_url": "https://arxiv.org/pdf/2205.03401v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2205.03401v2_The_Unreliability_of_Explanations_in_Few-shot_Prompting_for_Textual_Reasoning.pdf", "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning", "year": 2022, "paper_venue": null, "authors": ["Xi Ye", "Greg Durrett"], "abstract": "We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.", "comments": "NeurIPS 2022", "official_code_urls": ["https://github.com/xiye17/textualexplincontext"], "pwc_page_url": "https://paperswithcode.com/paper/the-unreliability-of-explanations-in-few-shot", "bibtex": "@misc{ye2022unreliability,\n      title={The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning}, \n      author={Xi Ye and Greg Durrett},\n      year={2022},\n      eprint={2205.03401},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "06-05-2022", "categories": ["cs.CL"]}, "2212.10403v2": {"paper_id": "2212.10403v2", "abs_url": "https://arxiv.org/abs/2212.10403v2", "pdf_url": "https://arxiv.org/pdf/2212.10403v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2212.10403v2_Towards_Reasoning_in_Large_Language_Models_A_Survey.pdf", "title": "Towards Reasoning in Large Language Models: A Survey", "year": 2022, "paper_venue": null, "authors": ["Jie Huang", "Kevin Chen-Chuan Chang"], "abstract": "Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.", "comments": "ACL 2023 Findings, 15 pages", "official_code_urls": ["https://github.com/jeffhj/lm-reasoning"], "pwc_page_url": "https://paperswithcode.com/paper/towards-reasoning-in-large-language-models-a", "bibtex": "@misc{huang2023reasoning,\n      title={Towards Reasoning in Large Language Models: A Survey}, \n      author={Jie Huang and Kevin Chen-Chuan Chang},\n      year={2023},\n      eprint={2212.10403},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "20-12-2022", "categories": ["cs.CL", "cs.AI"]}, "2212.10001v2": {"paper_id": "2212.10001v2", "abs_url": "https://arxiv.org/abs/2212.10001v2", "pdf_url": "https://arxiv.org/pdf/2212.10001v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2212.10001v2_Towards_Understanding_Chain-of-Thought_Prompting_An_Empirical_Study_of_What_Matters.pdf", "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters", "year": 2022, "paper_venue": null, "authors": ["Boshi Wang", "Sewon Min", "Xiang Deng", "Jiaming Shen", "You Wu", "Luke Zettlemoyer", "Huan Sun"], "abstract": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs' capability to learn to reason in context.", "comments": "ACL-23 Camera Ready. Code and model input/output are available at this https URL", "official_code_urls": ["https://github.com/sunlab-osu/understanding-cot"], "pwc_page_url": "https://paperswithcode.com/paper/towards-understanding-chain-of-thought", "bibtex": "@misc{wang2023understanding,\n      title={Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters}, \n      author={Boshi Wang and Sewon Min and Xiang Deng and Jiaming Shen and You Wu and Luke Zettlemoyer and Huan Sun},\n      year={2023},\n      eprint={2212.10001},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "20-12-2022", "categories": ["cs.CL"]}, "2204.05862v1": {"paper_id": "2204.05862v1", "abs_url": "https://arxiv.org/abs/2204.05862v1", "pdf_url": "https://arxiv.org/pdf/2204.05862v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2204.05862v1_Training_a_Helpful_and_Harmless_Assistant_with_Reinforcement_Learning_from_Human_Feedback.pdf", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "year": 2022, "paper_venue": null, "authors": ["Yuntao Bai", "Andy Jones", "Kamal Ndousse", "Amanda Askell", "Anna Chen", "Nova DasSarma", "Dawn Drain", "Stanislav Fort", "Deep Ganguli", "Tom Henighan", "Nicholas Joseph", "Saurav Kadavath", "Jackson Kernion", "Tom Conerly", "Sheer El-Showk", "Nelson Elhage", "Zac Hatfield-Dodds", "Danny Hernandez", "Tristan Hume", "Scott Johnston", "Shauna Kravec", "Liane Lovitt", "Neel Nanda", "Catherine Olsson", "Dario Amodei", "Tom Brown", "Jack Clark", "Sam McCandlish", "Chris Olah", "Ben Mann", "Jared Kaplan"], "abstract": "We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.", "comments": "Data available at this https URL", "official_code_urls": ["https://github.com/anthropics/hh-rlhf"], "pwc_page_url": "https://paperswithcode.com/paper/training-a-helpful-and-harmless-assistant", "bibtex": "@misc{bai2022training,\n      title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, \n      author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},\n      year={2022},\n      eprint={2204.05862},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "12-04-2022", "categories": ["cs.CL", "cs.LG"]}, "2203.02155v1": {"paper_id": "2203.02155v1", "abs_url": "https://arxiv.org/abs/2203.02155v1", "pdf_url": "https://arxiv.org/pdf/2203.02155v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2203.02155v1_Training_language_models_to_follow_instructions_with_human_feedback.pdf", "title": "Training language models to follow instructions with human feedback", "year": 2022, "paper_venue": null, "authors": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "John Schulman", "Jacob Hilton", "Fraser Kelton", "Luke Miller", "Maddie Simens", "Amanda Askell", "Peter Welinder", "Paul Christiano", "Jan Leike", "Ryan Lowe"], "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.", "comments": "", "official_code_urls": ["https://github.com/openai/following-instructions-human-feedback"], "pwc_page_url": "https://paperswithcode.com/paper/training-language-models-to-follow", "bibtex": "@misc{ouyang2022training,\n      title={Training language models to follow instructions with human feedback}, \n      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},\n      year={2022},\n      eprint={2203.02155},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "04-03-2022", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2205.05131v3": {"paper_id": "2205.05131v3", "abs_url": "https://arxiv.org/abs/2205.05131v3", "pdf_url": "https://arxiv.org/pdf/2205.05131v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2205.05131v3_UL2_Unifying_Language_Learning_Paradigms.pdf", "title": "UL2: Unifying Language Learning Paradigms", "year": 2022, "paper_venue": null, "authors": ["Yi Tay", "Mostafa Dehghani", "Vinh Q. Tran", "Xavier Garcia", "Jason Wei", "Xuezhi Wang", "Hyung Won Chung", "Siamak Shakeri", "Dara Bahri", "Tal Schuster", "Huaixiu Steven Zheng", "Denny Zhou", "Neil Houlsby", "Donald Metzler"], "abstract": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized & unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 & GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B & Flan-UL2 20B.", "comments": "Updated Q1 2023 with Flan-UL2 20B release! :)", "official_code_urls": ["https://github.com/google-research/google-research"], "pwc_page_url": "https://paperswithcode.com/paper/unifying-language-learning-paradigms", "bibtex": "@misc{tay2023ul2,\n      title={UL2: Unifying Language Learning Paradigms}, \n      author={Yi Tay and Mostafa Dehghani and Vinh Q. Tran and Xavier Garcia and Jason Wei and Xuezhi Wang and Hyung Won Chung and Siamak Shakeri and Dara Bahri and Tal Schuster and Huaixiu Steven Zheng and Denny Zhou and Neil Houlsby and Donald Metzler},\n      year={2023},\n      eprint={2205.05131},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "10-05-2022", "categories": ["cs.CL"]}, "2206.11795v1": {"paper_id": "2206.11795v1", "abs_url": "https://arxiv.org/abs/2206.11795v1", "pdf_url": "https://arxiv.org/pdf/2206.11795v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2206.11795v1_Video_PreTraining_VPT_Learning_to_Act_by_Watching_Unlabeled_Online_Videos.pdf", "title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos", "year": 2022, "paper_venue": null, "authors": ["Bowen Baker", "Ilge Akkaya", "Peter Zhokhov", "Joost Huizinga", "Jie Tang", "Adrien Ecoffet", "Brandon Houghton", "Raul Sampedro", "Jeff Clune"], "abstract": "Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.", "comments": "", "official_code_urls": ["https://github.com/openai/Video-Pre-Training"], "pwc_page_url": "https://paperswithcode.com/paper/video-pretraining-vpt-learning-to-act-by", "bibtex": "@misc{baker2022video,\n      title={Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos}, \n      author={Bowen Baker and Ilge Akkaya and Peter Zhokhov and Joost Huizinga and Jie Tang and Adrien Ecoffet and Brandon Houghton and Raul Sampedro and Jeff Clune},\n      year={2022},\n      eprint={2206.11795},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "23-06-2022", "categories": ["cs.LG", "cs.AI"]}}, {"2401.01286v3": {"paper_id": "2401.01286v3", "abs_url": "https://arxiv.org/abs/2401.01286v3", "pdf_url": "https://arxiv.org/pdf/2401.01286v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.01286v3_A_Comprehensive_Study_of_Knowledge_Editing_for_Large_Language_Models.pdf", "title": "A Comprehensive Study of Knowledge Editing for Large Language Models", "year": 2024, "paper_venue": null, "authors": ["Ningyu Zhang", "Yunzhi Yao", "Bozhong Tian", "Peng Wang", "Shumin Deng", "Mengru Wang", "Zekun Xi", "Shengyu Mao", "Jintian Zhang", "Yuansheng Ni", "Siyuan Cheng", "Ziwen Xu", "Xin Xu", "Jia-Chen Gu", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Lei Liang", "Zhiqiang Zhang", "Xiaowei Zhu", "Jun Zhou", "Huajun Chen"], "abstract": "Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the knowledge editing problem and then provide a comprehensive review of cutting-edge approaches. Drawing inspiration from educational and cognitive research theories, we propose a unified categorization criterion that classifies knowledge editing methods into three groups: resorting to external knowledge, merging knowledge into the model, and editing intrinsic knowledge. Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive empirical evaluation of representative knowledge editing approaches. Additionally, we provide an in-depth analysis of knowledge location, which can give a deeper understanding of the knowledge structures inherent within LLMs. Finally, we discuss several potential applications of knowledge editing, outlining its broad and impactful implications.", "comments": "Ongoing work; 52 pages, 282 citations; benchmark is available at this https URL code is available at this https URL paper list is available at this https URL", "official_code_urls": ["https://github.com/zjunlp/knowledgeeditingpapers", "https://github.com/zjunlp/easyedit"], "pwc_page_url": "https://paperswithcode.com/paper/a-comprehensive-study-of-knowledge-editing", "bibtex": "@misc{zhang2024comprehensive,\n      title={A Comprehensive Study of Knowledge Editing for Large Language Models}, \n      author={Ningyu Zhang and Yunzhi Yao and Bozhong Tian and Peng Wang and Shumin Deng and Mengru Wang and Zekun Xi and Shengyu Mao and Jintian Zhang and Yuansheng Ni and Siyuan Cheng and Ziwen Xu and Xin Xu and Jia-Chen Gu and Yong Jiang and Pengjun Xie and Fei Huang and Lei Liang and Zhiqiang Zhang and Xiaowei Zhu and Jun Zhou and Huajun Chen},\n      year={2024},\n      eprint={2401.01286},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "02-01-2024", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"]}, "2401.01313v3": {"paper_id": "2401.01313v3", "abs_url": "https://arxiv.org/abs/2401.01313v3", "pdf_url": "https://arxiv.org/pdf/2401.01313v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.01313v3_A_Comprehensive_Survey_of_Hallucination_Mitigation_Techniques_in_Large_Language_Models.pdf", "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models", "year": 2024, "paper_venue": null, "authors": ["S.M Towhidul Islam Tonmoy", "S M Mehedi Zaman", "Vinija Jain", "Anku Rani", "Vipula Rawte", "Aman Chadha", "Amitava Das"], "abstract": "As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/a-comprehensive-survey-of-hallucination", "bibtex": "@misc{tonmoy2024comprehensive,\n      title={A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models}, \n      author={S. M Towhidul Islam Tonmoy and S M Mehedi Zaman and Vinija Jain and Anku Rani and Vipula Rawte and Aman Chadha and Amitava Das},\n      year={2024},\n      eprint={2401.01313},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "02-01-2024", "categories": ["cs.CL"]}, "2401.00820v1": {"paper_id": "2401.00820v1", "abs_url": "https://arxiv.org/abs/2401.00820v1", "pdf_url": "https://arxiv.org/pdf/2401.00820v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.00820v1_A_Computational_Framework_for_Behavioral_Assessment_of_LLM_Therapists.pdf", "title": "A Computational Framework for Behavioral Assessment of LLM Therapists", "year": 2024, "paper_venue": null, "authors": ["Yu Ying Chiu", "Ashish Sharma", "Inna Wanyin Lin", "Tim Althoff"], "abstract": "The emergence of ChatGPT and other large language models (LLMs) has greatly increased interest in utilizing LLMs as therapists to support individuals struggling with mental health challenges. However, due to the lack of systematic studies, our understanding of how LLM therapists behave, i.e., ways in which they respond to clients, is significantly limited. Understanding their behavior across a wide range of clients and situations is crucial to accurately assess their capabilities and limitations in the high-risk setting of mental health, where undesirable behaviors can lead to severe consequences. In this paper, we propose BOLT, a novel computational framework to study the conversational behavior of LLMs when employed as therapists. We develop an in-context learning method to quantitatively measure the behavior of LLMs based on 13 different psychotherapy techniques including reflections, questions, solutions, normalizing, and psychoeducation. Subsequently, we compare the behavior of LLM therapists against that of high- and low-quality human therapy, and study how their behavior can be modulated to better reflect behaviors observed in high-quality therapy. Our analysis of GPT and Llama-variants reveals that these LLMs often resemble behaviors more commonly exhibited in low-quality therapy rather than high-quality therapy, such as offering a higher degree of problem-solving advice when clients share emotions, which is against typical recommendations. At the same time, unlike low-quality therapy, LLMs reflect significantly more upon clients' needs and strengths. Our analysis framework suggests that despite the ability of LLMs to generate anecdotal examples that appear similar to human therapists, LLM therapists are currently not fully consistent with high-quality care, and thus require additional research to ensure quality care.", "comments": "", "official_code_urls": ["https://github.com/behavioral-data/bolt"], "pwc_page_url": "https://paperswithcode.com/paper/a-computational-framework-for-behavioral", "bibtex": "@misc{chiu2024computational,\n      title={A Computational Framework for Behavioral Assessment of LLM Therapists}, \n      author={Yu Ying Chiu and Ashish Sharma and Inna Wanyin Lin and Tim Althoff},\n      year={2024},\n      eprint={2401.00820},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "01-01-2024", "categories": ["cs.CL", "cs.HC"]}, "2401.05268v2": {"paper_id": "2401.05268v2", "abs_url": "https://arxiv.org/abs/2401.05268v2", "pdf_url": "https://arxiv.org/pdf/2401.05268v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.05268v2_AUTOACT_Automatic_Agent_Learning_from_Scratch_via_Self-Planning.pdf", "title": "AUTOACT: Automatic Agent Learning from Scratch via Self-Planning", "year": 2024, "paper_venue": null, "authors": ["Shuofei Qiao", "Ningyu Zhang", "Runnan Fang", "Yujie Luo", "Wangchunshu Zhou", "Yuchen Eleanor Jiang", "Chengfei Lv", "Huajun Chen"], "abstract": ".", "comments": "Work in progress", "official_code_urls": ["https://github.com/zjunlp/autoact"], "pwc_page_url": "https://paperswithcode.com/paper/autoact-automatic-agent-learning-from-scratch", "bibtex": "@misc{qiao2024autoact,\n      title={AUTOACT: Automatic Agent Learning from Scratch via Self-Planning}, \n      author={Shuofei Qiao and Ningyu Zhang and Runnan Fang and Yujie Luo and Wangchunshu Zhou and Yuchen Eleanor Jiang and Chengfei Lv and Huajun Chen},\n      year={2024},\n      eprint={2401.05268},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "10-01-2024", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "cs.MA"]}, "2401.00625v2": {"paper_id": "2401.00625v2", "abs_url": "https://arxiv.org/abs/2401.00625v2", "pdf_url": "https://arxiv.org/pdf/2401.00625v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.00625v2_Beyond_Efficiency_A_Systematic_Survey_of_Resource-Efficient_Large_Language_Models.pdf", "title": "Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models", "year": 2024, "paper_venue": null, "authors": ["Guangji Bai", "Zheng Chai", "Chen Ling", "Shiyu Wang", "Jiaying Lu", "Nan Zhang", "Tingwei Shi", "Ziyang Yu", "Mengdan Zhu", "Yifei Zhang", "Carl Yang", "Yue Cheng", "Liang Zhao"], "abstract": "The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between various resources and corresponding optimization techniques. A standardized set of evaluation metrics and datasets is also presented to facilitate consistent and fair comparisons across different models and techniques. By offering a comprehensive overview of the current sota and identifying open research avenues, this survey serves as a foundational reference for researchers and practitioners, aiding them in developing more sustainable and efficient LLMs in a rapidly evolving landscape.", "comments": "Preprint. GitHub repo: this https URL", "official_code_urls": ["https://github.com/tiingweii-shii/awesome-resource-efficient-llm-papers"], "pwc_page_url": "https://paperswithcode.com/paper/beyond-efficiency-a-systematic-survey-of", "bibtex": "@misc{bai2024efficiency,\n      title={Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models}, \n      author={Guangji Bai and Zheng Chai and Chen Ling and Shiyu Wang and Jiaying Lu and Nan Zhang and Tingwei Shi and Ziyang Yu and Mengdan Zhu and Yifei Zhang and Carl Yang and Yue Cheng and Liang Zhao},\n      year={2024},\n      eprint={2401.00625},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "01-01-2024", "categories": ["cs.LG"]}, "2401.04398v1": {"paper_id": "2401.04398v1", "abs_url": "https://arxiv.org/abs/2401.04398v1", "pdf_url": "https://arxiv.org/pdf/2401.04398v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.04398v1_Chain-of-Table_Evolving_Tables_in_the_Reasoning_Chain_for_Table_Understanding.pdf", "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding", "year": 2024, "paper_venue": null, "authors": ["Zilong Wang", "Hao Zhang", "Chun-Liang Li", "Julian Martin Eisenschlos", "Vincent Perot", "Zifeng Wang", "Lesly Miculicich", "Yasuhisa Fujii", "Jingbo Shang", "Chen-Yu Lee", "Tomas Pfister"], "abstract": "Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.", "comments": "Preprint", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/chain-of-table-evolving-tables-in-the", "bibtex": "@misc{wang2024chainoftable,\n      title={Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding}, \n      author={Zilong Wang and Hao Zhang and Chun-Liang Li and Julian Martin Eisenschlos and Vincent Perot and Zifeng Wang and Lesly Miculicich and Yasuhisa Fujii and Jingbo Shang and Chen-Yu Lee and Tomas Pfister},\n      year={2024},\n      eprint={2401.04398},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "09-01-2024", "categories": ["cs.CL"]}, "2401.02404v2": {"paper_id": "2401.02404v2", "abs_url": "https://arxiv.org/abs/2401.02404v2", "pdf_url": "https://arxiv.org/pdf/2401.02404v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.02404v2_Correctness_Comparison_of_ChatGPT-4_Bard_Claude-2_and_Copilot_for_Spatial_Tasks.pdf", "title": "Correctness Comparison of ChatGPT-4, Bard, Claude-2, and Copilot for Spatial Tasks", "year": 2024, "paper_venue": null, "authors": ["Hartwig H. Hochmair", "Levente Juhasz", "Takoda Kemp"], "abstract": "Generative AI including large language models (LLMs) have recently gained significant interest in the geo-science community through its versatile task-solving capabilities including coding, spatial computations, generation of sample data, time-series forecasting, toponym recognition, or image classification. So far, the assessment of LLMs for spatial tasks has primarily focused on ChatGPT, arguably the most prominent AI chatbot, whereas other chatbots received less attention. To narrow this research gap, this study evaluates the correctness of responses for a set of 54 spatial tasks assigned to four prominent chatbots, i.e., ChatGPT-4, Bard, Claude-2, and Copilot. Overall, the chatbots performed well on spatial literacy, GIS theory, and interpretation of programming code and given functions, but revealed weaknesses in mapping, code generation, and code translation. ChatGPT-4 outperformed other chatbots across most task categories.", "comments": "Submitted for review in Transactions in GIS", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/correctness-comparison-of-chatgpt-4-bard", "bibtex": "@misc{hochmair2024correctness,\n      title={Correctness Comparison of ChatGPT-4, Bard, Claude-2, and Copilot for Spatial Tasks}, \n      author={Hartwig H. Hochmair and Levente Juhasz and Takoda Kemp},\n      year={2024},\n      eprint={2401.02404},\n      archivePrefix={arXiv},\n      primaryClass={cs.CY}\n}", "published_date": "04-01-2024", "categories": ["cs.CY"]}, "2401.02954v1": {"paper_id": "2401.02954v1", "abs_url": "https://arxiv.org/abs/2401.02954v1", "pdf_url": "https://arxiv.org/pdf/2401.02954v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.02954v1_DeepSeek_LLM_Scaling_Open-Source_Language_Models_with_Longtermism.pdf", "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism", "year": 2024, "paper_venue": null, "authors": ["DeepSeek-AI", ":", "Xiao Bi", "Deli Chen", "Guanting Chen", "Shanhuang Chen", "Damai Dai", "Chengqi Deng", "Honghui Ding", "Kai Dong", "Qiushi Du", "Zhe Fu", "Huazuo Gao", "Kaige Gao", "Wenjun Gao", "Ruiqi Ge", "Kang Guan", "Daya Guo", "Jianzhong Guo", "Guangbo Hao", "Zhewen Hao", "Ying He", "Wenjie Hu", "Panpan Huang", "Erhang Li", "Guowei Li", "Jiashi Li", "Yao Li", "Y.K. Li", "Wenfeng Liang", "Fangyun Lin", "A.X. Liu", "Bo Liu", "Wen Liu", "Xiaodong Liu", "Xin Liu", "Yiyuan Liu", "Haoyu Lu", "Shanghao Lu", "Fuli Luo", "Shirong Ma", "Xiaotao Nie", "Tian Pei", "Yishi Piao", "Junjie Qiu", "Hui Qu", "Tongzheng Ren", "Zehui Ren", "Chong Ruan", "Zhangli Sha", "Zhihong Shao", "Junxiao Song", "Xuecheng Su", "Jingxiang Sun", "Yaofeng Sun", "Minghui Tang", "Bingxuan Wang", "Peiyi Wang", "Shiyu Wang", "Yaohui Wang", "Yongji Wang", "Tong Wu", "Y. Wu", "Xin Xie", "Zhenda Xie", "Ziwei Xie", "Yiliang Xiong", "Hanwei Xu", "R.X. Xu", "Yanhong Xu", "Dejian Yang", "Yuxiang You", "Shuiping Yu", "Xingkai Yu", "B. Zhang", "Haowei Zhang", "Lecong Zhang", "Liyue Zhang", "Mingchuan Zhang", "Minghua Zhang", "Wentao Zhang", "Yichao Zhang", "Chenggang Zhao", "Yao Zhao", "Shangyan Zhou", "Shunfeng Zhou", "Qihao Zhu", "Yuheng Zou"], "abstract": "The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.", "comments": "", "official_code_urls": ["https://github.com/deepseek-ai/deepseek-llm"], "pwc_page_url": "https://paperswithcode.com/paper/deepseek-llm-scaling-open-source-language", "bibtex": "@misc{deepseekai2024deepseek,\n      title={DeepSeek LLM: Scaling Open-Source Language Models with Longtermism}, \n      author={DeepSeek-AI and : and Xiao Bi and Deli Chen and Guanting Chen and Shanhuang Chen and Damai Dai and Chengqi Deng and Honghui Ding and Kai Dong and Qiushi Du and Zhe Fu and Huazuo Gao and Kaige Gao and Wenjun Gao and Ruiqi Ge and Kang Guan and Daya Guo and Jianzhong Guo and Guangbo Hao and Zhewen Hao and Ying He and Wenjie Hu and Panpan Huang and Erhang Li and Guowei Li and Jiashi Li and Yao Li and Y. K. Li and Wenfeng Liang and Fangyun Lin and A. X. Liu and Bo Liu and Wen Liu and Xiaodong Liu and Xin Liu and Yiyuan Liu and Haoyu Lu and Shanghao Lu and Fuli Luo and Shirong Ma and Xiaotao Nie and Tian Pei and Yishi Piao and Junjie Qiu and Hui Qu and Tongzheng Ren and Zehui Ren and Chong Ruan and Zhangli Sha and Zhihong Shao and Junxiao Song and Xuecheng Su and Jingxiang Sun and Yaofeng Sun and Minghui Tang and Bingxuan Wang and Peiyi Wang and Shiyu Wang and Yaohui Wang and Yongji Wang and Tong Wu and Y. Wu and Xin Xie and Zhenda Xie and Ziwei Xie and Yiliang Xiong and Hanwei Xu and R. X. Xu and Yanhong Xu and Dejian Yang and Yuxiang You and Shuiping Yu and Xingkai Yu and B. Zhang and Haowei Zhang and Lecong Zhang and Liyue Zhang and Mingchuan Zhang and Minghua Zhang and Wentao Zhang and Yichao Zhang and Chenggang Zhao and Yao Zhao and Shangyan Zhou and Shunfeng Zhou and Qihao Zhu and Yuheng Zou},\n      year={2024},\n      eprint={2401.02954},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "05-01-2024", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2401.01583v1": {"paper_id": "2401.01583v1", "abs_url": "https://arxiv.org/abs/2401.01583v1", "pdf_url": "https://arxiv.org/pdf/2401.01583v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.01583v1_Enhancing_the_medical_foundation_model_with_multi-scale_and_cross-modality_feature_learning.pdf", "title": "Enhancing the medical foundation model with multi-scale and cross-modality feature learning", "year": 2024, "paper_venue": null, "authors": ["Weijian Huang", "Cheng Li", "Hong-Yu Zhou", "Jiarun Liu", "Hao Yang", "Yong Liang", "Shanshan Wang"], "abstract": "The development of multi-modal medical foundation models has attracted significant attention in the field of medicine and healthcare due to their promising prospects in various clinical applications. One area of focus in this research direction is the extractions of features at different scales. While previous studies have explored feature learning at individual scales, investigation on integrating the diverse scales and modalities of information is lacking, which may hinder the potential for mutual reinforcement among these features. This paper aims to bridge this gap by proposing a method that effectively exploits multi-scale and cross-modality information to enhance the performance of medical foundation models. The proposed method simultaneously exploit features at the local, instance, modality and global aspects, facilitating comprehensive representation learning within the models. We evaluate the effectiveness of the proposed method on six open-source datasets across different clinical tasks, demonstrating its ability to enhance the performance of medical foundation models.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/enhancing-the-medical-foundation-model-with", "bibtex": "@misc{huang2024enhancing,\n      title={Enhancing the medical foundation model with multi-scale and cross-modality feature learning}, \n      author={Weijian Huang and Cheng Li and Hong-Yu Zhou and Jiarun Liu and Hao Yang and Yong Liang and Shanshan Wang},\n      year={2024},\n      eprint={2401.01583},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "03-01-2024", "categories": ["cs.CV"]}, "2401.01519v2": {"paper_id": "2401.01519v2", "abs_url": "https://arxiv.org/abs/2401.01519v2", "pdf_url": "https://arxiv.org/pdf/2401.01519v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.01519v2_Exploring_the_Frontiers_of_LLMs_in_Psychological_Applications_A_Comprehensive_Review.pdf", "title": "Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review", "year": 2024, "paper_venue": null, "authors": ["Luoma Ke", "(1),", "Song Tong", "(1),", "Peng Cheng", "(2),", "Kaiping Peng", "(1) ((1) Department of Psychology, Tsinghua University, (2) School of Social Science, Tsinghua University)"], "abstract": "This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologies in psychology, the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations. Researchers should responsibly use LLMs in psychological studies, adhering to ethical standards and considering the potential consequences of deploying these technologies in sensitive areas. Overall, the article provides a comprehensive overview of the current state of LLMs in psychology, exploring potential benefits and challenges. It serves as a call to action for researchers to leverage LLMs' advantages responsibly while addressing associated risks.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/exploring-the-frontiers-of-llms-in", "bibtex": "@misc{ke2024exploring,\n      title={Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review}, \n      author={Luoma Ke and Song Tong and Peng Cheng and Kaiping Peng},\n      year={2024},\n      eprint={2401.01519},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "03-01-2024", "categories": ["cs.LG", "cs.AI"]}, "2401.01736v2": {"paper_id": "2401.01736v2", "abs_url": "https://arxiv.org/abs/2401.01736v2", "pdf_url": "https://arxiv.org/pdf/2401.01736v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.01736v2_Few-shot_Adaptation_of_Multi-modal_Foundation_Models_A_Survey.pdf", "title": "Few-shot Adaptation of Multi-modal Foundation Models: A Survey", "year": 2024, "paper_venue": null, "authors": ["Fan Liu", "Tianshu Zhang", "Wenwen Dai", "Wenwen Cai", "Xiaocong Zhou", "Delong Chen"], "abstract": "Multi-modal (vision-language) models, such as CLIP, are replacing traditional supervised pre-training models (e.g., ImageNet-based pre-training) as the new generation of visual foundation models. These models with robust and aligned semantic representations learned from billions of internet image-text pairs and can be applied to various downstream tasks in a zero-shot manner. However, in some fine-grained domains like medical imaging and remote sensing, the performance of multi-modal foundation models often leaves much to be desired. Consequently, many researchers have begun to explore few-shot adaptation methods for these models, gradually deriving three main technical approaches: 1) prompt-based methods, 2) adapter-based methods, and 3) external knowledge-based methods. Nevertheless, this rapidly developing field has produced numerous results without a comprehensive survey to systematically organize the research progress. Therefore, in this survey, we introduce and analyze the research advancements in few-shot adaptation methods for multi-modal models, summarizing commonly used datasets and experimental setups, and comparing the results of different methods. In addition, due to the lack of reliable theoretical support for existing methods, we derive the few-shot adaptation generalization error bound for multi-modal models. The theorem reveals that the generalization error of multi-modal foundation models is constrained by three factors: domain gap, model capacity, and sample size. Based on this, we propose three possible solutions from the following aspects: 1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive knowledge utilization.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/few-shot-adaptation-of-multi-modal-foundation", "bibtex": "@misc{liu2024fewshot,\n      title={Few-shot Adaptation of Multi-modal Foundation Models: A Survey}, \n      author={Fan Liu and Tianshu Zhang and Wenwen Dai and Wenwen Cai and Xiaocong Zhou and Delong Chen},\n      year={2024},\n      eprint={2401.01736},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "03-01-2024", "categories": ["cs.CV"]}, "2401.02777v1": {"paper_id": "2401.02777v1", "abs_url": "https://arxiv.org/abs/2401.02777v1", "pdf_url": "https://arxiv.org/pdf/2401.02777v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.02777v1_From_LLM_to_Conversational_Agent_A_Memory_Enhanced_Architecture_with_Fine-Tuning_of_Large_Language_Models.pdf", "title": "From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models", "year": 2024, "paper_venue": null, "authors": ["Na Liu", "Liangyu Chen", "Xiaoyu Tian", "Wei Zou", "Kaijiang Chen", "Ming Cui"], "abstract": "This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile conversational agents.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/from-llm-to-conversational-agent-a-memory", "bibtex": "@misc{liu2024llm,\n      title={From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models}, \n      author={Na Liu and Liangyu Chen and Xiaoyu Tian and Wei Zou and Kaijiang Chen and Ming Cui},\n      year={2024},\n      eprint={2401.02777},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "05-01-2024", "categories": ["cs.CL", "cs.AI"]}, "2401.00678v1": {"paper_id": "2401.00678v1", "abs_url": "https://arxiv.org/abs/2401.00678v1", "pdf_url": "https://arxiv.org/pdf/2401.00678v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.00678v1_General-purpose_foundation_models_for_increased_autonomy_in_robot-assisted_surgery.pdf", "title": "General-purpose foundation models for increased autonomy in robot-assisted surgery", "year": 2024, "paper_venue": null, "authors": ["Samuel Schmidgall", "Ji Woong Kim", "Alan Kuntz", "Ahmed Ezzat Ghazi", "Axel Krieger"], "abstract": "The dominant paradigm for end-to-end robot learning focuses on optimizing task-specific objectives that solve a single robotic problem such as picking up an object or reaching a target position. However, recent work on high-capacity models in robotics has shown promise toward being trained on large collections of diverse and task-agnostic datasets of video demonstrations. These models have shown impressive levels of generalization to unseen circumstances, especially as the amount of data and the model complexity scale. Surgical robot systems that learn from data have struggled to advance as quickly as other fields of robot learning for a few reasons: (1) there is a lack of existing large-scale open-source data to train models, (2) it is challenging to model the soft-body deformations that these robots work with during surgery because simulation cannot match the physical and visual complexity of biological tissue, and (3) surgical robots risk harming patients when tested in clinical trials and require more extensive safety measures. This perspective article aims to provide a path toward increasing robot autonomy in robot-assisted surgery through the development of a multi-modal, multi-task, vision-language-action model for surgical robots. Ultimately, we argue that surgical robots are uniquely positioned to benefit from general-purpose models and provide three guiding actions toward increased autonomy in robot-assisted surgery.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/general-purpose-foundation-models-for", "bibtex": "@misc{schmidgall2024generalpurpose,\n      title={General-purpose foundation models for increased autonomy in robot-assisted surgery}, \n      author={Samuel Schmidgall and Ji Woong Kim and Alan Kuntz and Ahmed Ezzat Ghazi and Axel Krieger},\n      year={2024},\n      eprint={2401.00678},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO}\n}", "published_date": "01-01-2024", "categories": ["cs.RO", "cs.LG", "q-bio.TO"]}, "2401.06373v1": {"paper_id": "2401.06373v1", "abs_url": "https://arxiv.org/abs/2401.06373v1", "pdf_url": "https://arxiv.org/pdf/2401.06373v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.06373v1_How_Johnny_Can_Persuade_LLMs_to_Jailbreak_Them_Rethinking_Persuasion_to_Challenge_AI_Safety_by_Humanizing_LLMs.pdf", "title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs", "year": 2024, "paper_venue": null, "authors": ["Yi Zeng", "Hongpeng Lin", "Jingwen Zhang", "Diyi Yang", "Ruoxi Jia", "Weiyan Shi"], "abstract": "Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP and, found a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs", "comments": "14 pages of the main text, qualitative examples of jailbreaks may be harmful in nature", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/how-johnny-can-persuade-llms-to-jailbreak", "bibtex": "@misc{zeng2024johnny,\n      title={How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs}, \n      author={Yi Zeng and Hongpeng Lin and Jingwen Zhang and Diyi Yang and Ruoxi Jia and Weiyan Shi},\n      year={2024},\n      eprint={2401.06373},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "12-01-2024", "categories": ["cs.CL", "cs.AI"]}, "2401.00812v2": {"paper_id": "2401.00812v2", "abs_url": "https://arxiv.org/abs/2401.00812v2", "pdf_url": "https://arxiv.org/pdf/2401.00812v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.00812v2_If_LLM_Is_the_Wizard_Then_Code_Is_the_Wand_A_Survey_on_How_Code_Empowers_Large_Language_Models_to_Serve_as_Intelligent_Agents.pdf", "title": "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents", "year": 2024, "paper_venue": null, "authors": ["Ke Yang", "Jiateng Liu", "John Wu", "Chaoqi Yang", "Yi R. Fung", "Sha Li", "Zixuan Huang", "Xu Cao", "Xingyao Wang", "Yiquan Wang", "Heng Ji", "Chengxiang Zhai"], "abstract": "The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/if-llm-is-the-wizard-then-code-is-the-wand-a", "bibtex": "@misc{yang2024llm,\n      title={If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents}, \n      author={Ke Yang and Jiateng Liu and John Wu and Chaoqi Yang and Yi R. Fung and Sha Li and Zixuan Huang and Xu Cao and Xingyao Wang and Yiquan Wang and Heng Ji and Chengxiang Zhai},\n      year={2024},\n      eprint={2401.00812},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "01-01-2024", "categories": ["cs.CL"]}, "2401.06561v1": {"paper_id": "2401.06561v1", "abs_url": "https://arxiv.org/abs/2401.06561v1", "pdf_url": "https://arxiv.org/pdf/2401.06561v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.06561v1_Intention_Analysis_Prompting_Makes_Large_Language_Models_A_Good_Jailbreak_Defender.pdf", "title": "Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender", "year": 2024, "paper_venue": null, "authors": ["Yuqi Zhang", "Liang Ding", "Lefei Zhang", "Dacheng Tao"], "abstract": "", "comments": "9 pages, 5 figures", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/intention-analysis-prompting-makes-large", "bibtex": "@misc{zhang2024intention,\n      title={Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender}, \n      author={Yuqi Zhang and Liang Ding and Lefei Zhang and Dacheng Tao},\n      year={2024},\n      eprint={2401.06561},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "12-01-2024", "categories": ["cs.CL"]}, "2401.02412v1": {"paper_id": "2401.02412v1", "abs_url": "https://arxiv.org/abs/2401.02412v1", "pdf_url": "https://arxiv.org/pdf/2401.02412v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.02412v1_LLM_Augmented_LLMs_Expanding_Capabilities_through_Composition.pdf", "title": "LLM Augmented LLMs: Expanding Capabilities through Composition", "year": 2024, "paper_venue": null, "authors": ["Rachit Bansal", "Bidisha Samanta", "Siddharth Dalmia", "Nitish Gupta", "Shikhar Vashishth", "Sriram Ganapathy", "Abhishek Bapna", "Prateek Jain", "Partha Talukdar"], "abstract": "Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40\\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.", "comments": "17 pages, 2 figures, 8 tables", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/llm-augmented-llms-expanding-capabilities", "bibtex": "@misc{bansal2024llm,\n      title={LLM Augmented LLMs: Expanding Capabilities through Composition}, \n      author={Rachit Bansal and Bidisha Samanta and Siddharth Dalmia and Nitish Gupta and Shikhar Vashishth and Sriram Ganapathy and Abhishek Bapna and Prateek Jain and Partha Talukdar},\n      year={2024},\n      eprint={2401.02412},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "04-01-2024", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"]}, "2401.01325v1": {"paper_id": "2401.01325v1", "abs_url": "https://arxiv.org/abs/2401.01325v1", "pdf_url": "https://arxiv.org/pdf/2401.01325v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.01325v1_LLM_Maybe_LongLM_Self-Extend_LLM_Context_Window_Without_Tuning.pdf", "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning", "year": 2024, "paper_venue": null, "authors": ["Hongye Jin", "Xiaotian Han", "Jingfeng Yang", "Zhimeng Jiang", "Zirui Liu", "Chia-Yuan Chang", "Huiyuan Chen", "Xia Hu"], "abstract": "This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can effectively extend existing LLMs' context window's length.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/llm-maybe-longlm-self-extend-llm-context", "bibtex": "@misc{jin2024llm,\n      title={LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning}, \n      author={Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Zirui Liu and Chia-Yuan Chang and Huiyuan Chen and Xia Hu},\n      year={2024},\n      eprint={2401.01325},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "02-01-2024", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2401.01055v2": {"paper_id": "2401.01055v2", "abs_url": "https://arxiv.org/abs/2401.01055v2", "pdf_url": "https://arxiv.org/pdf/2401.01055v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.01055v2_LLaMA_Beyond_English_An_Empirical_Study_on_Language_Capability_Transfer.pdf", "title": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer", "year": 2024, "paper_venue": null, "authors": ["Jun Zhao", "Zhihao Zhang", "Luhui Gao", "Qi Zhang", "Tao Gui", "Xuanjing Huang"], "abstract": "In recent times, substantial advancements have been witnessed in large language models (LLMs), exemplified by ChatGPT, showcasing remarkable proficiency across a range of complex tasks. However, many mainstream LLMs (e.g. LLaMA) are pretrained on English-dominant corpus, which limits their performance in other non-English languages. In this paper, we focus on how to effectively transfer the capabilities of language generation and following instructions to a non-English language. To answer this question, we conduct an extensive empirical investigation based on LLaMA, accumulating over 1440 GPU hours. We analyze the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer. To accurately assess the model's level of knowledge, we employ four widely used standardized testing benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a comprehensive evaluation of the model's response quality is conducted, considering aspects such as accuracy, fluency, informativeness, logical coherence, and harmlessness, based on LLM-Eval, a benchmarks consisting instruction tasks from 17 diverse categories. Our evaluation results demonstrate that comparable performance to state-of-the-art transfer models can be achieved with less than 1% of the pretraining data, both in terms of knowledge alignment and response quality. Furthermore, the experimental outcomes across the thirteen low-resource languages also exhibit similar trends. We anticipate that the conclusions revealed by the experiments will aid the community in developing non-English LLMs.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/llama-beyond-english-an-empirical-study-on", "bibtex": "@misc{zhao2024llama,\n      title={LLaMA Beyond English: An Empirical Study on Language Capability Transfer}, \n      author={Jun Zhao and Zhihao Zhang and Luhui Gao and Qi Zhang and Tao Gui and Xuanjing Huang},\n      year={2024},\n      eprint={2401.01055},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "02-01-2024", "categories": ["cs.CL", "cs.AI"]}, "2401.02415v1": {"paper_id": "2401.02415v1", "abs_url": "https://arxiv.org/abs/2401.02415v1", "pdf_url": "https://arxiv.org/pdf/2401.02415v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.02415v1_LLaMA_Pro_Progressive_LLaMA_with_Block_Expansion.pdf", "title": "LLaMA Pro: Progressive LLaMA with Block Expansion", "year": 2024, "paper_venue": null, "authors": ["Chengyue Wu", "Yukang Gan", "Yixiao Ge", "Zeyu Lu", "Jiahao Wang", "Ye Feng", "Ping Luo", "Ying Shan"], "abstract": "Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.", "comments": "", "official_code_urls": ["https://github.com/tencentarc/llama-pro"], "pwc_page_url": "https://paperswithcode.com/paper/llama-pro-progressive-llama-with-block", "bibtex": "@misc{wu2024llama,\n      title={LLaMA Pro: Progressive LLaMA with Block Expansion}, \n      author={Chengyue Wu and Yukang Gan and Yixiao Ge and Zeyu Lu and Jiahao Wang and Ye Feng and Ping Luo and Ying Shan},\n      year={2024},\n      eprint={2401.02415},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "04-01-2024", "categories": ["cs.CL"]}, "2401.02330v2": {"paper_id": "2401.02330v2", "abs_url": "https://arxiv.org/abs/2401.02330v2", "pdf_url": "https://arxiv.org/pdf/2401.02330v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.02330v2_LLaVA-Phi_Efficient_Multi-Modal_Assistant_with_Small_Language_Model.pdf", "title": "LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model", "year": 2024, "paper_venue": null, "authors": ["Yichen Zhu", "Minjie Zhu", "Ning Liu", "Zhicai Ou", "Xiaofeng Mou", "Jian Tang"], "abstract": "}.", "comments": "technique report", "official_code_urls": ["https://github.com/zhuyiche/llava-phi"], "pwc_page_url": "https://paperswithcode.com/paper/llava-ph-efficient-multi-modal-assistant-with", "bibtex": "@misc{zhu2024llavaphi,\n      title={LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model}, \n      author={Yichen Zhu and Minjie Zhu and Ning Liu and Zhicai Ou and Xiaofeng Mou and Jian Tang},\n      year={2024},\n      eprint={2401.02330},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "04-01-2024", "categories": ["cs.CV", "cs.CL"]}, "2401.01814v1": {"paper_id": "2401.01814v1", "abs_url": "https://arxiv.org/abs/2401.01814v1", "pdf_url": "https://arxiv.org/pdf/2401.01814v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.01814v1_Large_Language_Models_Relearn_Removed_Concepts.pdf", "title": "Large Language Models Relearn Removed Concepts", "year": 2024, "paper_venue": null, "authors": ["Michelle Lo", "Shay B. Cohen", "Fazl Barez"], "abstract": "Advances in model editing through neuron pruning hold promise for removing undesirable concepts from large language models. However, it remains unclear whether models have the capacity to reacquire pruned concepts after editing. To investigate this, we evaluate concept relearning in models by tracking concept saliency and similarity in pruned neurons during retraining. Our findings reveal that models can quickly regain performance post-pruning by relocating advanced concepts to earlier layers and reallocating pruned concepts to primed neurons with similar semantics. This demonstrates that models exhibit polysemantic capacities and can blend old and new concepts in individual neurons. While neuron pruning provides interpretability into model concepts, our results highlight the challenges of permanent concept removal for improved model \\textit{safety}. Monitoring concept reemergence and developing techniques to mitigate relearning of unsafe concepts will be important directions for more robust model editing. Overall, our work strongly demonstrates the resilience and fluidity of concept representations in LLMs post concept removal.", "comments": "", "official_code_urls": ["https://github.com/fbarez/neuroplasticity"], "pwc_page_url": "https://paperswithcode.com/paper/large-language-models-relearn-removed", "bibtex": "@misc{lo2024large,\n      title={Large Language Models Relearn Removed Concepts}, \n      author={Michelle Lo and Shay B. Cohen and Fazl Barez},\n      year={2024},\n      eprint={2401.01814},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "03-01-2024", "categories": ["cs.AI"]}, "2401.04088v1": {"paper_id": "2401.04088v1", "abs_url": "https://arxiv.org/abs/2401.04088v1", "pdf_url": "https://arxiv.org/pdf/2401.04088v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.04088v1_Mixtral_of_Experts.pdf", "title": "Mixtral of Experts", "year": 2024, "paper_venue": null, "authors": ["Albert Q. Jiang", "Alexandre Sablayrolles", "Antoine Roux", "Arthur Mensch", "Blanche Savary", "Chris Bamford", "Devendra Singh Chaplot", "Diego de las Casas", "Emma Bou Hanna", "Florian Bressand", "Gianna Lengyel", "Guillaume Bour", "Guillaume Lample", "L\u00e9lio Renard Lavaud", "Lucile Saulnier", "Marie-Anne Lachaux", "Pierre Stock", "Sandeep Subramanian", "Sophia Yang", "Szymon Antoniak", "Teven Le Scao", "Th\u00e9ophile Gervet", "Thibaut Lavril", "Thomas Wang", "Timoth\u00e9e Lacroix", "William El Sayed"], "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.", "comments": "See more details at this https URL", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/mixtral-of-experts", "bibtex": "@misc{jiang2024mixtral,\n      title={Mixtral of Experts}, \n      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and L\u00e9lio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Th\u00e9ophile Gervet and Thibaut Lavril and Thomas Wang and Timoth\u00e9e Lacroix and William El Sayed},\n      year={2024},\n      eprint={2401.04088},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "08-01-2024", "categories": ["cs.LG", "cs.CL"]}, "2401.08406v2": {"paper_id": "2401.08406v2", "abs_url": "https://arxiv.org/abs/2401.08406v2", "pdf_url": "https://arxiv.org/pdf/2401.08406v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.08406v2_RAG_vs_Fine-tuning_Pipelines_Tradeoffs_and_a_Case_Study_on_Agriculture.pdf", "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture", "year": 2024, "paper_venue": null, "authors": ["Angels Balaguer", "Vinamra Benara", "Renato Luiz de Freitas Cunha", "Roberto de M. Estev\u00e3o Filho", "Todd Hendry", "Daniel Holstein", "Jennifer Marsman", "Nick Mecklenburg", "Sara Malvar", "Leonardo O. Nunes", "Rafael Padilha", "Morris Sharp", "Bruno Silva", "Swati Sharma", "Vijay Aski", "Ranveer Chandra"], "abstract": "There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/rag-vs-fine-tuning-pipelines-tradeoffs-and-a", "bibtex": "@misc{balaguer2024rag,\n      title={RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture}, \n      author={Angels Balaguer and Vinamra Benara and Renato Luiz de Freitas Cunha and Roberto de M. Estev\u00e3o Filho and Todd Hendry and Daniel Holstein and Jennifer Marsman and Nick Mecklenburg and Sara Malvar and Leonardo O. Nunes and Rafael Padilha and Morris Sharp and Bruno Silva and Swati Sharma and Vijay Aski and Ranveer Chandra},\n      year={2024},\n      eprint={2401.08406},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "16-01-2024", "categories": ["cs.CL", "cs.LG"]}, "2401.05778v1": {"paper_id": "2401.05778v1", "abs_url": "https://arxiv.org/abs/2401.05778v1", "pdf_url": "https://arxiv.org/pdf/2401.05778v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.05778v1_Risk_Taxonomy_Mitigation_and_Assessment_Benchmarks_of_Large_Language_Model_Systems.pdf", "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems", "year": 2024, "paper_venue": null, "authors": ["Tianyu Cui", "Yanling Wang", "Chuanpu Fu", "Yong Xiao", "Sijia Li", "Xinhao Deng", "Yunpeng Liu", "Qinglin Zhang", "Ziyi Qiu", "Peiyang Li", "Zhixing Tan", "Junwu Xiong", "Xinyu Kong", "Zujie Wen", "Ke Xu", "Qi Li"], "abstract": "Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to facilitate the risk assessment of LLM systems. We hope that this paper can help LLM participants embrace a systematic perspective to build their responsible LLM systems.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/risk-taxonomy-mitigation-and-assessment", "bibtex": "@misc{cui2024risk,\n      title={Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems}, \n      author={Tianyu Cui and Yanling Wang and Chuanpu Fu and Yong Xiao and Sijia Li and Xinhao Deng and Yunpeng Liu and Qinglin Zhang and Ziyi Qiu and Peiyang Li and Zhixing Tan and Junwu Xiong and Xinyu Kong and Zujie Wen and Ke Xu and Qi Li},\n      year={2024},\n      eprint={2401.05778},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "11-01-2024", "categories": ["cs.CL", "cs.AI"]}, "2401.05856v1": {"paper_id": "2401.05856v1", "abs_url": "https://arxiv.org/abs/2401.05856v1", "pdf_url": "https://arxiv.org/pdf/2401.05856v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.05856v1_Seven_Failure_Points_When_Engineering_a_Retrieval_Augmented_Generation_System.pdf", "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System", "year": 2024, "paper_venue": null, "authors": ["Scott Barnett", "Stefanus Kurniawan", "Srikanth Thudumu", "Zach Brannelly", "Mohamed Abdelrazek"], "abstract": "Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/seven-failure-points-when-engineering-a", "bibtex": "@misc{barnett2024seven,\n      title={Seven Failure Points When Engineering a Retrieval Augmented Generation System}, \n      author={Scott Barnett and Stefanus Kurniawan and Srikanth Thudumu and Zach Brannelly and Mohamed Abdelrazek},\n      year={2024},\n      eprint={2401.05856},\n      archivePrefix={arXiv},\n      primaryClass={cs.SE}\n}", "published_date": "11-01-2024", "categories": ["cs.SE"]}, "2401.05566v3": {"paper_id": "2401.05566v3", "abs_url": "https://arxiv.org/abs/2401.05566v3", "pdf_url": "https://arxiv.org/pdf/2401.05566v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.05566v3_Sleeper_Agents_Training_Deceptive_LLMs_that_Persist_Through_Safety_Training.pdf", "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training", "year": 2024, "paper_venue": null, "authors": ["Evan Hubinger", "Carson Denison", "Jesse Mu", "Mike Lambert", "Meg Tong", "Monte MacDiarmid", "Tamera Lanham", "Daniel M. Ziegler", "Tim Maxwell", "Newton Cheng", "Adam Jermyn", "Amanda Askell", "Ansh Radhakrishnan", "Cem Anil", "David Duvenaud", "Deep Ganguli", "Fazl Barez", "Jack Clark", "Kamal Ndousse", "Kshitij Sachan", "Michael Sellitto", "Mrinank Sharma", "Nova DasSarma", "Roger Grosse", "Shauna Kravec", "Yuntao Bai", "Zachary Witten", "Marina Favaro", "Jan Brauner", "Holden Karnofsky", "Paul Christiano", "Samuel R. Bowman", "Logan Graham", "Jared Kaplan", "S\u00f6ren Mindermann", "Ryan Greenblatt", "Buck Shlegeris", "Nicholas Schiefer", "Ethan Perez"], "abstract": "Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.", "comments": "updated to add missing acknowledgements", "official_code_urls": ["https://github.com/anthropics/sleeper-agents-paper"], "pwc_page_url": "https://paperswithcode.com/paper/sleeper-agents-training-deceptive-llms-that", "bibtex": "@misc{hubinger2024sleeper,\n      title={Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training}, \n      author={Evan Hubinger and Carson Denison and Jesse Mu and Mike Lambert and Meg Tong and Monte MacDiarmid and Tamera Lanham and Daniel M. Ziegler and Tim Maxwell and Newton Cheng and Adam Jermyn and Amanda Askell and Ansh Radhakrishnan and Cem Anil and David Duvenaud and Deep Ganguli and Fazl Barez and Jack Clark and Kamal Ndousse and Kshitij Sachan and Michael Sellitto and Mrinank Sharma and Nova DasSarma and Roger Grosse and Shauna Kravec and Yuntao Bai and Zachary Witten and Marina Favaro and Jan Brauner and Holden Karnofsky and Paul Christiano and Samuel R. Bowman and Logan Graham and Jared Kaplan and S\u00f6ren Mindermann and Ryan Greenblatt and Buck Shlegeris and Nicholas Schiefer and Ethan Perez},\n      year={2024},\n      eprint={2401.05566},\n      archivePrefix={arXiv},\n      primaryClass={cs.CR}\n}", "published_date": "10-01-2024", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"]}, "2401.03462v1": {"paper_id": "2401.03462v1", "abs_url": "https://arxiv.org/abs/2401.03462v1", "pdf_url": "https://arxiv.org/pdf/2401.03462v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.03462v1_Soaring_from_4K_to_400K_Extending_LLMs_Context_with_Activation_Beacon.pdf", "title": "Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon", "year": 2024, "paper_venue": null, "authors": ["Peitian Zhang", "Zheng Liu", "Shitao Xiao", "Ninglu Shao", "Qiwei Ye", "Zhicheng Dou"], "abstract": "The utilization of long contexts poses a big challenge for large language models due to their limited context window length. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose Activation Beacon, which condenses LLM's raw activations into more compact forms such that it can perceive a much longer context with a limited context window. Activation Beacon is introduced as a plug-and-play module for the LLM. It fully preserves the LLM's original capability on short contexts while extending the new capability on processing longer contexts. Besides, it works with short sliding windows to process the long context, which achieves a competitive memory and time efficiency in both training and inference. Activation Beacon is learned by the auto-regression task conditioned on a mixture of beacons with diversified condensing ratios. Thanks to such a treatment, it can be efficiently trained purely with short-sequence data in just 10K steps, which consumes less than 9 hours on a single 8xA800 GPU machine. The experimental studies show that Activation Beacon is able to extend Llama-2-7B's context length by $\\times100$ times (from 4K to 400K), meanwhile achieving a superior result on both long-context generation and understanding tasks. Our model and code will be available at the BGE repository.", "comments": "", "official_code_urls": ["https://github.com/flagopen/flagembedding"], "pwc_page_url": "https://paperswithcode.com/paper/soaring-from-4k-to-400k-extending-llm-s", "bibtex": "@misc{zhang2024soaring,\n      title={Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon}, \n      author={Peitian Zhang and Zheng Liu and Shitao Xiao and Ninglu Shao and Qiwei Ye and Zhicheng Dou},\n      year={2024},\n      eprint={2401.03462},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "07-01-2024", "categories": ["cs.CL", "cs.AI"]}, "2401.05618v1": {"paper_id": "2401.05618v1", "abs_url": "https://arxiv.org/abs/2401.05618v1", "pdf_url": "https://arxiv.org/pdf/2401.05618v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.05618v1_The_Benefits_of_a_Concise_Chain_of_Thought_on_Problem-Solving_in_Large_Language_Models.pdf", "title": "The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models", "year": 2024, "paper_venue": null, "authors": ["Matthew Renze", "Erhan Guven"], "abstract": "In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance. However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads to an average per-token cost reduction of 22.67%. These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques. In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs.", "comments": "All code, data, and supplemental materials are available on GitHub at this https URL", "official_code_urls": ["https://github.com/matthewrenze/jhu-concise-cot"], "pwc_page_url": "https://paperswithcode.com/paper/the-benefits-of-a-concise-chain-of-thought-on", "bibtex": "@misc{renze2024benefits,\n      title={The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models}, \n      author={Matthew Renze and Erhan Guven},\n      year={2024},\n      eprint={2401.05618},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "11-01-2024", "categories": ["cs.CL", "cs.AI"]}, "2401.00761v1": {"paper_id": "2401.00761v1", "abs_url": "https://arxiv.org/abs/2401.00761v1", "pdf_url": "https://arxiv.org/pdf/2401.00761v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.00761v1_The_Earth_is_Flat?_Unveiling_Factual_Errors_in_Large_Language_Models.pdf", "title": "The Earth is Flat? Unveiling Factual Errors in Large Language Models", "year": 2024, "paper_venue": null, "authors": ["Wenxuan Wang", "Juluan Shi", "Zhaopeng Tu", "Youliang Yuan", "Jen-tse Huang", "Wenxiang Jiao", "Michael R. Lyu"], "abstract": "Large Language Models (LLMs) like ChatGPT are foundational in various applications due to their extensive knowledge from pre-training and fine-tuning. Despite this, they are prone to generating factual and commonsense errors, raising concerns in critical areas like healthcare, journalism, and education to mislead users. Current methods for evaluating LLMs' veracity are limited by test data leakage or the need for extensive human labor, hindering efficient and accurate error detection. To tackle this problem, we introduce a novel, automatic testing framework, FactChecker, aimed at uncovering factual inaccuracies in LLMs. This framework involves three main steps: First, it constructs a factual knowledge graph by retrieving fact triplets from a large-scale knowledge database. Then, leveraging the knowledge graph, FactChecker employs a rule-based approach to generates three types of questions (Yes-No, Multiple-Choice, and WH questions) that involve single-hop and multi-hop relations, along with correct answers. Lastly, it assesses the LLMs' responses for accuracy using tailored matching strategies for each question type. Our extensive tests on six prominent LLMs, including text-davinci-002, text-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal that FactChecker can trigger factual errors in up to 45\\% of questions in these models. Moreover, we demonstrate that FactChecker's test cases can improve LLMs' factual accuracy through in-context learning and fine-tuning (e.g., llama-2-13b-chat's accuracy increase from 35.3\\% to 68.5\\%). We are making all code, data, and results available for future research endeavors.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/the-earth-is-flat-unveiling-factual-errors-in", "bibtex": "@misc{wang2024earth,\n      title={The Earth is Flat? Unveiling Factual Errors in Large Language Models}, \n      author={Wenxuan Wang and Juluan Shi and Zhaopeng Tu and Youliang Yuan and Jen-tse Huang and Wenxiang Jiao and Michael R. Lyu},\n      year={2024},\n      eprint={2401.00761},\n      archivePrefix={arXiv},\n      primaryClass={cs.SE}\n}", "published_date": "01-01-2024", "categories": ["cs.SE", "cs.AI", "cs.CL"]}, "2401.04925v2": {"paper_id": "2401.04925v2", "abs_url": "https://arxiv.org/abs/2401.04925v2", "pdf_url": "https://arxiv.org/pdf/2401.04925v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.04925v2_The_Impact_of_Reasoning_Step_Length_on_Large_Language_Models.pdf", "title": "The Impact of Reasoning Step Length on Large Language Models", "year": 2024, "paper_venue": null, "authors": ["Mingyu Jin", "Qinkai Yu", "Dong shu", "Haiyan Zhao", "Wenyue Hua", "Yanda Meng", "Yongfeng Zhang", "Mengnan Du"], "abstract": "Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/the-impact-of-reasoning-step-length-on-large", "bibtex": "@misc{jin2024impact,\n      title={The Impact of Reasoning Step Length on Large Language Models}, \n      author={Mingyu Jin and Qinkai Yu and Dong shu and Haiyan Zhao and Wenyue Hua and Yanda Meng and Yongfeng Zhang and Mengnan Du},\n      year={2024},\n      eprint={2401.04925},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "10-01-2024", "categories": ["cs.CL", "cs.AI"]}, "2401.02843v1": {"paper_id": "2401.02843v1", "abs_url": "https://arxiv.org/abs/2401.02843v1", "pdf_url": "https://arxiv.org/pdf/2401.02843v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.02843v1_Thousands_of_AI_Authors_on_the_Future_of_AI.pdf", "title": "Thousands of AI Authors on the Future of AI", "year": 2024, "paper_venue": null, "authors": ["Katja Grace", "Harlan Stewart", "Julia Fabienne Sandk\u00fchler", "Stephen Thomas", "Ben Weinstein-Raun", "Jan Brauner"], "abstract": "Most respondents expressed substantial uncertainty about the long-term value of AI progress: While 68.3% thought good outcomes from superhuman AI are more likely than bad, of these net optimists 48% gave at least a 5% chance of extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes. Between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that \"substantial\" or \"extreme\" concern is warranted about six different AI-related scenarios, including misinformation, authoritarian control, and inequality. There was disagreement about whether faster or slower AI progress would be better for the future of humanity. However, there was broad agreement that research aimed at minimizing potential risks from AI systems ought to be prioritized more.", "comments": "The asterisk indicates the corresponding author. The dagger indicates equal contribution", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/thousands-of-ai-authors-on-the-future-of-ai", "bibtex": "@misc{grace2024thousands,\n      title={Thousands of AI Authors on the Future of AI}, \n      author={Katja Grace and Harlan Stewart and Julia Fabienne Sandk\u00fchler and Stephen Thomas and Ben Weinstein-Raun and Jan Brauner},\n      year={2024},\n      eprint={2401.02843},\n      archivePrefix={arXiv},\n      primaryClass={cs.CY}\n}", "published_date": "05-01-2024", "categories": ["cs.CY", "cs.AI", "cs.LG"]}, "2401.02385v1": {"paper_id": "2401.02385v1", "abs_url": "https://arxiv.org/abs/2401.02385v1", "pdf_url": "https://arxiv.org/pdf/2401.02385v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.02385v1_TinyLlama_An_Open-Source_Small_Language_Model.pdf", "title": "TinyLlama: An Open-Source Small Language Model", "year": 2024, "paper_venue": null, "authors": ["Peiyuan Zhang", "Guangtao Zeng", "Tianduo Wang", "Wei Lu"], "abstract": ".", "comments": "Technical Report", "official_code_urls": ["https://github.com/jzhang38/tinyllama"], "pwc_page_url": "https://paperswithcode.com/paper/tinyllama-an-open-source-small-language-model", "bibtex": "@misc{zhang2024tinyllama,\n      title={TinyLlama: An Open-Source Small Language Model}, \n      author={Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},\n      year={2024},\n      eprint={2401.02385},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "04-01-2024", "categories": ["cs.CL", "cs.AI"]}, "2401.05561v2": {"paper_id": "2401.05561v2", "abs_url": "https://arxiv.org/abs/2401.05561v2", "pdf_url": "https://arxiv.org/pdf/2401.05561v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.05561v2_TrustLLM_Trustworthiness_in_Large_Language_Models.pdf", "title": "TrustLLM: Trustworthiness in Large Language Models", "year": 2024, "paper_venue": null, "authors": ["Lichao Sun", "Yue Huang", "Haoran Wang", "Siyuan Wu", "Qihui Zhang", "Chujie Gao", "Yixin Huang", "Wenhan Lyu", "Yixuan Zhang", "Xiner Li", "Zhengliang Liu", "Yixin Liu", "Yijue Wang", "Zhikun Zhang", "Bhavya Kailkhura", "Caiming Xiong", "Chaowei Xiao", "Chunyuan Li", "Eric Xing", "Furong Huang", "Hao Liu", "Heng Ji", "Hongyi Wang", "Huan Zhang", "Huaxiu Yao", "Manolis Kellis", "Marinka Zitnik", "Meng Jiang", "Mohit Bansal", "James Zou", "Jian Pei", "Jian Liu", "Jianfeng Gao", "Jiawei Han", "Jieyu Zhao", "Jiliang Tang", "Jindong Wang", "John Mitchell", "Kai Shu", "Kaidi Xu", "Kai-Wei Chang", "Lifang He", "Lifu Huang", "Michael Backes", "Neil Zhenqiang Gong", "Philip S. Yu", "Pin-Yu Chen", "Quanquan Gu", "Ran Xu", "Rex Ying", "Shuiwang Ji", "Suman Jana", "Tianlong Chen", "Tianming Liu", "Tianyi Zhou", "Willian Wang", "Xiang Li", "Xiangliang Zhang", "Xiao Wang", "Xing Xie", "Xun Chen", "Xuyu Wang", "Yan Liu", "Yanfang Ye", "Yinzhi Cao", "Yong Chen", "Yue Zhao"], "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.", "comments": "This work is still under work and we welcome your contribution", "official_code_urls": ["https://github.com/HowieHwong/TrustLLM"], "pwc_page_url": "https://paperswithcode.com/paper/trustllm-trustworthiness-in-large-language", "bibtex": "@misc{sun2024trustllm,\n      title={TrustLLM: Trustworthiness in Large Language Models}, \n      author={Lichao Sun and Yue Huang and Haoran Wang and Siyuan Wu and Qihui Zhang and Chujie Gao and Yixin Huang and Wenhan Lyu and Yixuan Zhang and Xiner Li and Zhengliang Liu and Yixin Liu and Yijue Wang and Zhikun Zhang and Bhavya Kailkhura and Caiming Xiong and Chaowei Xiao and Chunyuan Li and Eric Xing and Furong Huang and Hao Liu and Heng Ji and Hongyi Wang and Huan Zhang and Huaxiu Yao and Manolis Kellis and Marinka Zitnik and Meng Jiang and Mohit Bansal and James Zou and Jian Pei and Jian Liu and Jianfeng Gao and Jiawei Han and Jieyu Zhao and Jiliang Tang and Jindong Wang and John Mitchell and Kai Shu and Kaidi Xu and Kai-Wei Chang and Lifang He and Lifu Huang and Michael Backes and Neil Zhenqiang Gong and Philip S. Yu and Pin-Yu Chen and Quanquan Gu and Ran Xu and Rex Ying and Shuiwang Ji and Suman Jana and Tianlong Chen and Tianming Liu and Tianyi Zhou and Willian Wang and Xiang Li and Xiangliang Zhang and Xiao Wang and Xing Xie and Xun Chen and Xuyu Wang and Yan Liu and Yanfang Ye and Yinzhi Cao and Yong Chen and Yue Zhao},\n      year={2024},\n      eprint={2401.05561},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "10-01-2024", "categories": ["cs.CL"]}, "2401.02038v2": {"paper_id": "2401.02038v2", "abs_url": "https://arxiv.org/abs/2401.02038v2", "pdf_url": "https://arxiv.org/pdf/2401.02038v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.02038v2_Understanding_LLMs_A_Comprehensive_Overview_from_Training_to_Inference.pdf", "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference", "year": 2024, "paper_venue": null, "authors": ["Yiheng Liu", "Hao He", "Tianle Han", "Xu Zhang", "Mengyuan Liu", "Jiaming Tian", "Yutong Zhang", "Jiaqi Wang", "Xiaohui Gao", "Tianyang Zhong", "Yi Pan", "Shaochen Xu", "Zihao Wu", "Zhengliang Liu", "Xin Zhang", "Shu Zhang", "Xintao Hu", "Tuo Zhang", "Ning Qiang", "Tianming Liu", "Bao Ge"], "abstract": "The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.", "comments": "30 pages,6 figures", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/understanding-llms-a-comprehensive-overview", "bibtex": "@misc{liu2024understanding,\n      title={Understanding LLMs: A Comprehensive Overview from Training to Inference}, \n      author={Yiheng Liu and Hao He and Tianle Han and Xu Zhang and Mengyuan Liu and Jiaming Tian and Yutong Zhang and Jiaqi Wang and Xiaohui Gao and Tianyang Zhong and Yi Pan and Shaochen Xu and Zihao Wu and Zhengliang Liu and Xin Zhang and Shu Zhang and Xintao Hu and Tuo Zhang and Ning Qiang and Tianming Liu and Bao Ge},\n      year={2024},\n      eprint={2401.02038},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "04-01-2024", "categories": ["cs.CL"]}}, {"2312.06908v1": {"paper_id": "2312.06908v1", "abs_url": "https://arxiv.org/abs/2312.06908v1", "pdf_url": "https://arxiv.org/pdf/2312.06908v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.06908v1_I_Want_It_That_Way_Enabling_Interactive_Decision_Support_Using_Large_Language_Models_and_Constraint_Programming.pdf", "title": "\"I Want It That Way\": Enabling Interactive Decision Support Using Large Language Models and Constraint Programming", "year": 2023, "paper_venue": null, "authors": ["Connor Lawless", "Jakob Schoeffer", "Lindy Le", "Kael Rowan", "Shilad Sen", "Cristina St. Hill", "Jina Suh", "Bahar Sarrafzadeh"], "abstract": "A critical factor in the success of decision support systems is the accurate modeling of user preferences. Psychology research has demonstrated that users often develop their preferences during the elicitation process, highlighting the pivotal role of system-user interaction in developing personalized systems. This paper introduces a novel approach, combining Large Language Models (LLMs) with Constraint Programming to facilitate interactive decision support. We study this hybrid framework through the lens of meeting scheduling, a time-consuming daily activity faced by a multitude of information workers. We conduct three studies to evaluate the novel framework, including a diary study (n=64) to characterize contextual scheduling preferences, a quantitative evaluation of the system's performance, and a user study (n=10) with a prototype system. Our work highlights the potential for a hybrid LLM and optimization approach for iterative preference elicitation and design considerations for building systems that support human-system collaborative decision-making processes.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/i-want-it-that-way-enabling-interactive", "bibtex": "@misc{lawless2023i,\n      title={\"I Want It That Way\": Enabling Interactive Decision Support Using Large Language Models and Constraint Programming}, \n      author={Connor Lawless and Jakob Schoeffer and Lindy Le and Kael Rowan and Shilad Sen and Cristina St. Hill and Jina Suh and Bahar Sarrafzadeh},\n      year={2023},\n      eprint={2312.06908},\n      archivePrefix={arXiv},\n      primaryClass={cs.HC}\n}", "published_date": "12-12-2023", "categories": ["cs.HC"]}, "2304.02020v1": {"paper_id": "2304.02020v1", "abs_url": "https://arxiv.org/abs/2304.02020v1", "pdf_url": "https://arxiv.org/pdf/2304.02020v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2304.02020v1_A_Bibliometric_Review_of_Large_Language_Models_Research_from_2017_to_2023.pdf", "title": "A Bibliometric Review of Large Language Models Research from 2017 to 2023", "year": 2023, "paper_venue": null, "authors": ["Lizhou Fan", "Lingyao Li", "Zihui Ma", "Sanggyu Lee", "Huizi Yu", "Libby Hemphill"], "abstract": "Large language models (LLMs) are a class of language models that have demonstrated outstanding performance across a range of natural language processing (NLP) tasks and have become a highly sought-after research area, because of their ability to generate human-like language and their potential to revolutionize science and technology. In this study, we conduct bibliometric and discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000 publications, this paper serves as a roadmap for researchers, practitioners, and policymakers to navigate the current landscape of LLMs research. We present the research trends from 2017 to early 2023, identifying patterns in research paradigms and collaborations. We start with analyzing the core algorithm developments and NLP tasks that are fundamental in LLMs research. We then investigate the applications of LLMs in various fields and domains including medicine, engineering, social science, and humanities. Our review also reveals the dynamic, fast-paced evolution of LLMs research. Overall, this paper offers valuable insights into the current state, impact, and potential of LLMs research and its applications.", "comments": "36 pages, 9 figures, and 4 tables", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/a-bibliometric-review-of-large-language", "bibtex": "@misc{fan2023bibliometric,\n      title={A Bibliometric Review of Large Language Models Research from 2017 to 2023}, \n      author={Lizhou Fan and Lingyao Li and Zihui Ma and Sanggyu Lee and Huizi Yu and Libby Hemphill},\n      year={2023},\n      eprint={2304.02020},\n      archivePrefix={arXiv},\n      primaryClass={cs.DL}\n}", "published_date": "03-04-2023", "categories": ["cs.DL", "cs.CL", "cs.CY", "cs.SI"]}, "2312.10982v1": {"paper_id": "2312.10982v1", "abs_url": "https://arxiv.org/abs/2312.10982v1", "pdf_url": "https://arxiv.org/pdf/2312.10982v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.10982v1_A_Comprehensive_Survey_of_Attack_Techniques_Implementation_and_Mitigation_Strategies_in_Large_Language_Models.pdf", "title": "A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Aysan Esmradi", "Daniel Wankit Yip", "Chun Fai Chan"], "abstract": "Ensuring the security of large language models (LLMs) is an ongoing challenge despite their widespread popularity. Developers work to enhance LLMs security, but vulnerabilities persist, even in advanced versions like GPT-4. Attackers exploit these weaknesses, highlighting the need for proactive cybersecurity measures in AI model development. This article explores two attack categories: attacks on models themselves and attacks on model applications. The former requires expertise, access to model data, and significant implementation time, while the latter is more accessible to attackers and has seen increased attention. Our study reviews over 100 recent research works, providing an in-depth analysis of each attack type. We identify the latest attack methods and explore various approaches to carry them out. We thoroughly investigate mitigation techniques, assessing their effectiveness and limitations. Furthermore, we summarize future defenses against these attacks. We also examine real-world techniques, including reported and our implemented attacks on LLMs, to consolidate our findings. Our research highlights the urgency of addressing security concerns and aims to enhance the understanding of LLM attacks, contributing to robust defense development in this evolving domain.", "comments": "Accepted to be published in the Proceedings of the 3rd International Conference on Ubiquitous Security 2023 (UbiSec-2023)", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/a-comprehensive-survey-of-attack-techniques", "bibtex": "@misc{esmradi2023comprehensive,\n      title={A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models}, \n      author={Aysan Esmradi and Daniel Wankit Yip and Chun Fai Chan},\n      year={2023},\n      eprint={2312.10982},\n      archivePrefix={arXiv},\n      primaryClass={cs.CR}\n}", "published_date": "18-12-2023", "categories": ["cs.CR"]}, "2302.11382v1": {"paper_id": "2302.11382v1", "abs_url": "https://arxiv.org/abs/2302.11382v1", "pdf_url": "https://arxiv.org/pdf/2302.11382v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2302.11382v1_A_Prompt_Pattern_Catalog_to_Enhance_Prompt_Engineering_with_ChatGPT.pdf", "title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT", "year": 2023, "paper_venue": null, "authors": ["Jules White", "Quchen Fu", "Sam Hays", "Michael Sandborn", "Carlos Olea", "Henry Gilbert", "Ashraf Elnashar", "Jesse Spencer-Smith", "Douglas C. Schmidt"], "abstract": "Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/a-prompt-pattern-catalog-to-enhance-prompt", "bibtex": "@misc{white2023prompt,\n      title={A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT}, \n      author={Jules White and Quchen Fu and Sam Hays and Michael Sandborn and Carlos Olea and Henry Gilbert and Ashraf Elnashar and Jesse Spencer-Smith and Douglas C. Schmidt},\n      year={2023},\n      eprint={2302.11382},\n      archivePrefix={arXiv},\n      primaryClass={cs.SE}\n}", "published_date": "21-02-2023", "categories": ["cs.SE", "cs.AI"]}, "2307.12856v3": {"paper_id": "2307.12856v3", "abs_url": "https://arxiv.org/abs/2307.12856v3", "pdf_url": "https://arxiv.org/pdf/2307.12856v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2307.12856v3_A_Real-World_WebAgent_with_Planning_Long_Context_Understanding_and_Program_Synthesis.pdf", "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis", "year": 2023, "paper_venue": null, "authors": ["Izzeddin Gur", "Hiroki Furuta", "Austin Huang", "Mustafa Safdari", "Yutaka Matsuo", "Douglas Eck", "Aleksandra Faust"], "abstract": "Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/a-real-world-webagent-with-planning-long", "bibtex": "@misc{gur2023realworld,\n      title={A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis}, \n      author={Izzeddin Gur and Hiroki Furuta and Austin Huang and Mustafa Safdari and Yutaka Matsuo and Douglas Eck and Aleksandra Faust},\n      year={2023},\n      eprint={2307.12856},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "24-07-2023", "categories": ["cs.LG", "cs.AI", "cs.CL"]}, "2311.05112v2": {"paper_id": "2311.05112v2", "abs_url": "https://arxiv.org/abs/2311.05112v2", "pdf_url": "https://arxiv.org/pdf/2311.05112v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.05112v2_A_Survey_of_Large_Language_Models_in_Medicine_Principles_Applications_and_Challenges.pdf", "title": "A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges", "year": 2023, "paper_venue": null, "authors": ["Hongjian Zhou", "Fenglin Liu", "Boyang Gu", "Xinyu Zou", "Jinfa Huang", "Jinge Wu", "Yiru Li", "Sam S. Chen", "Peilin Zhou", "Junling Liu", "Yining Hua", "Chengfeng Mao", "Xian Wu", "Yefeng Zheng", "Lei Clifton", "Zheng Li", "Jiebo Luo", "David A. Clifton"], "abstract": ".", "comments": "Preprint. Version 2. 53 pages", "official_code_urls": ["https://github.com/ai-in-health/medllmspracticalguide"], "pwc_page_url": "https://paperswithcode.com/paper/a-survey-of-large-language-models-in-medicine", "bibtex": "@misc{zhou2023survey,\n      title={A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges}, \n      author={Hongjian Zhou and Fenglin Liu and Boyang Gu and Xinyu Zou and Jinfa Huang and Jinge Wu and Yiru Li and Sam S. Chen and Peilin Zhou and Junling Liu and Yining Hua and Chengfeng Mao and Xian Wu and Yefeng Zheng and Lei Clifton and Zheng Li and Jiebo Luo and David A. Clifton},\n      year={2023},\n      eprint={2311.05112},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "09-11-2023", "categories": ["cs.CL", "cs.AI"]}, "2312.11562v4": {"paper_id": "2312.11562v4", "abs_url": "https://arxiv.org/abs/2312.11562v4", "pdf_url": "https://arxiv.org/pdf/2312.11562v4.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.11562v4_A_Survey_of_Reasoning_with_Foundation_Models.pdf", "title": "A Survey of Reasoning with Foundation Models", "year": 2023, "paper_venue": null, "authors": ["Jiankai Sun", "Chuanyang Zheng", "Enze Xie", "Zhengying Liu", "Ruihang Chu", "Jianing Qiu", "Jiaqi Xu", "Mingyu Ding", "Hongyang Li", "Mengzhe Geng", "Yue Wu", "Wenhai Wang", "Junsong Chen", "Zhangyue Yin", "Xiaozhe Ren", "Jie Fu", "Junxian He", "Wu Yuan", "Qi Liu", "Xihui Liu", "Yu Li", "Hao Dong", "Yu Cheng", "Ming Zhang", "Pheng Ann Heng", "Jifeng Dai", "Ping Luo", "Jingdong Wang", "Ji-Rong Wen", "Xipeng Qiu", "Yike Guo", "Hui Xiong", "Qun Liu", "Zhenguo Li"], "abstract": "Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advancements in reasoning with foundation models, and contribute to the development of AGI.", "comments": "20 Figures, 160 Pages, 750+ References, Project Page this https URL", "official_code_urls": ["https://github.com/reasoning-survey/awesome-reasoning-foundation-models"], "pwc_page_url": "https://paperswithcode.com/paper/a-survey-of-reasoning-with-foundation-models", "bibtex": "@misc{sun2023survey,\n      title={A Survey of Reasoning with Foundation Models}, \n      author={Jiankai Sun and Chuanyang Zheng and Enze Xie and Zhengying Liu and Ruihang Chu and Jianing Qiu and Jiaqi Xu and Mingyu Ding and Hongyang Li and Mengzhe Geng and Yue Wu and Wenhai Wang and Junsong Chen and Zhangyue Yin and Xiaozhe Ren and Jie Fu and Junxian He and Wu Yuan and Qi Liu and Xihui Liu and Yu Li and Hao Dong and Yu Cheng and Ming Zhang and Pheng Ann Heng and Jifeng Dai and Ping Luo and Jingdong Wang and Ji-Rong Wen and Xipeng Qiu and Yike Guo and Hui Xiong and Qun Liu and Zhenguo Li},\n      year={2023},\n      eprint={2312.11562},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "17-12-2023", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"]}, "2311.05232v1": {"paper_id": "2311.05232v1", "abs_url": "https://arxiv.org/abs/2311.05232v1", "pdf_url": "https://arxiv.org/pdf/2311.05232v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.05232v1_A_Survey_on_Hallucination_in_Large_Language_Models_Principles_Taxonomy_Challenges_and_Open_Questions.pdf", "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions", "year": 2023, "paper_venue": null, "authors": ["Lei Huang", "Weijiang Yu", "Weitao Ma", "Weihong Zhong", "Zhangyin Feng", "Haotian Wang", "Qianglong Chen", "Weihua Peng", "Xiaocheng Feng", "Bing Qin", "Ting Liu"], "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs.", "comments": "Work in progress; 49 pages", "official_code_urls": ["https://github.com/luckyyysta/awesome-llm-hallucination"], "pwc_page_url": "https://paperswithcode.com/paper/a-survey-on-hallucination-in-large-language", "bibtex": "@misc{huang2023survey,\n      title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}, \n      author={Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},\n      year={2023},\n      eprint={2311.05232},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "09-11-2023", "categories": ["cs.CL"]}, "2312.10059v1": {"paper_id": "2312.10059v1", "abs_url": "https://arxiv.org/abs/2312.10059v1", "pdf_url": "https://arxiv.org/pdf/2312.10059v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.10059v1_A_collection_of_principles_for_guiding_and_evaluating_large_language_models.pdf", "title": "A collection of principles for guiding and evaluating large language models", "year": 2023, "paper_venue": null, "authors": ["Konstantin Hebenstreit", "Robert Praas", "Matthias Samwald"], "abstract": "Large language models (LLMs) demonstrate outstanding capabilities, but challenges remain regarding their ability to solve complex reasoning tasks, as well as their transparency, robustness, truthfulness, and ethical alignment. In this preliminary study, we compile a set of core principles for steering and evaluating the reasoning of LLMs by curating literature from several relevant strands of work: structured reasoning in LLMs, self-evaluation/self-reflection, explainability, AI system safety/security, guidelines for human critical thinking, and ethical/regulatory guidelines for AI. We identify and curate a list of 220 principles from literature, and derive a set of 37 core principles organized into seven categories: assumptions and perspectives, reasoning, information and evidence, robustness and security, ethics, utility, and implications. We conduct a small-scale expert survey, eliciting the subjective importance experts assign to different principles and lay out avenues for future work beyond our preliminary results. We envision that the development of a shared model of principles can serve multiple purposes: monitoring and steering models at inference time, improving model behavior during training, and guiding human evaluation of model reasoning.", "comments": "Accepted at Socially Responsible Language Modelling Research (SoLaR) workshop, NeurIPS 2023 ( this https URL ). Based on previous manuscript version: doi: https://doi.org/10.2139/ssrn.4446991", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/a-collection-of-principles-for-guiding-and", "bibtex": "@misc{hebenstreit2023collection,\n      title={A collection of principles for guiding and evaluating large language models}, \n      author={Konstantin Hebenstreit and Robert Praas and Matthias Samwald},\n      year={2023},\n      eprint={2312.10059},\n      archivePrefix={arXiv},\n      primaryClass={cs.CY}\n}", "published_date": "04-12-2023", "categories": ["cs.CY"]}, "2311.05772v1": {"paper_id": "2311.05772v1", "abs_url": "https://arxiv.org/abs/2311.05772v1", "pdf_url": "https://arxiv.org/pdf/2311.05772v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.05772v1_ADaPT_As-Needed_Decomposition_and_Planning_with_Language_Models.pdf", "title": "ADaPT: As-Needed Decomposition and Planning with Language Models", "year": 2023, "paper_venue": null, "authors": ["Archiki Prasad", "Alexander Koller", "Mareike Hartmann", "Peter Clark", "Ashish Sabharwal", "Mohit Bansal", "Tushar Khot"], "abstract": "Large Language Models (LLMs) are increasingly being used for interactive decision-making tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADaPT substantially outperforms established strong baselines, achieving success rates up to 28.3% higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADaPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity.", "comments": "Project Page: this https URL", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/adapt-as-needed-decomposition-and-planning", "bibtex": "@misc{prasad2023adapt,\n      title={ADaPT: As-Needed Decomposition and Planning with Language Models}, \n      author={Archiki Prasad and Alexander Koller and Mareike Hartmann and Peter Clark and Ashish Sabharwal and Mohit Bansal and Tushar Khot},\n      year={2023},\n      eprint={2311.05772},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "08-11-2023", "categories": ["cs.AI", "cs.CL", "cs.LG"]}, "2310.14820v1": {"paper_id": "2310.14820v1", "abs_url": "https://arxiv.org/abs/2310.14820v1", "pdf_url": "https://arxiv.org/pdf/2310.14820v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.14820v1_ALCUNA_Large_Language_Models_Meet_New_Knowledge.pdf", "title": "ALCUNA: Large Language Models Meet New Knowledge", "year": 2023, "paper_venue": null, "authors": ["Xunjian Yin", "Baizhou Huang", "Xiaojun Wan"], "abstract": "With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models' capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs' ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to assess LLMs' abilities in knowledge understanding, differentiation, and association. We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge. We also explore the impact of entity similarity on the model's understanding of entity knowledge and the influence of contextual entities. We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge.", "comments": "EMNLP 2023", "official_code_urls": ["https://github.com/arvid-pku/alcuna"], "pwc_page_url": "https://paperswithcode.com/paper/alcuna-large-language-models-meet-new", "bibtex": "@misc{yin2023alcuna,\n      title={ALCUNA: Large Language Models Meet New Knowledge}, \n      author={Xunjian Yin and Baizhou Huang and Xiaojun Wan},\n      year={2023},\n      eprint={2310.14820},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "23-10-2023", "categories": ["cs.CL"]}, "2303.09014v1": {"paper_id": "2303.09014v1", "abs_url": "https://arxiv.org/abs/2303.09014v1", "pdf_url": "https://arxiv.org/pdf/2303.09014v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2303.09014v1_ART_Automatic_multi-step_reasoning_and_tool-use_for_large_language_models.pdf", "title": "ART: Automatic multi-step reasoning and tool-use for large language models", "year": 2023, "paper_venue": null, "authors": ["Bhargavi Paranjape", "Scott Lundberg", "Sameer Singh", "Hannaneh Hajishirzi", "Luke Zettlemoyer", "Marco Tulio Ribeiro"], "abstract": "Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/art-automatic-multi-step-reasoning-and-tool", "bibtex": "@misc{paranjape2023art,\n      title={ART: Automatic multi-step reasoning and tool-use for large language models}, \n      author={Bhargavi Paranjape and Scott Lundberg and Sameer Singh and Hannaneh Hajishirzi and Luke Zettlemoyer and Marco Tulio Ribeiro},\n      year={2023},\n      eprint={2303.09014},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "16-03-2023", "categories": ["cs.CL"]}, "2302.12246v3": {"paper_id": "2302.12246v3", "abs_url": "https://arxiv.org/abs/2302.12246v3", "pdf_url": "https://arxiv.org/pdf/2302.12246v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2302.12246v3_Active_Prompting_with_Chain-of-Thought_for_Large_Language_Models.pdf", "title": "Active Prompting with Chain-of-Thought for Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Shizhe Diao", "Pengcheng Wang", "Yong Lin", "Tong Zhang"], "abstract": ".", "comments": "20 pages, 3 figures, 11 tables", "official_code_urls": ["https://github.com/shizhediao/active-cot", "https://github.com/shizhediao/active-prompt"], "pwc_page_url": "https://paperswithcode.com/paper/active-prompting-with-chain-of-thought-for", "bibtex": "@misc{diao2023active,\n      title={Active Prompting with Chain-of-Thought for Large Language Models}, \n      author={Shizhe Diao and Pengcheng Wang and Yong Lin and Tong Zhang},\n      year={2023},\n      eprint={2302.12246},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "23-02-2023", "categories": ["cs.CL"]}, "2303.10512v2": {"paper_id": "2303.10512v2", "abs_url": "https://arxiv.org/abs/2303.10512v2", "pdf_url": "https://arxiv.org/pdf/2303.10512v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2303.10512v2_AdaLoRA_Adaptive_Budget_Allocation_for_Parameter-Efficient_Fine-Tuning.pdf", "title": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning", "year": 2023, "paper_venue": null, "authors": ["Qingru Zhang", "Minshuo Chen", "Alexander Bukharin", "Nikos Karampatziakis", "Pengcheng He", "Yu Cheng", "Weizhu Chen", "Tuo Zhao"], "abstract": ".", "comments": "The 11th International Conference on Learning Representations (ICLR 2023)", "official_code_urls": ["https://github.com/qingruzhang/adalora"], "pwc_page_url": "https://paperswithcode.com/paper/adaptive-budget-allocation-for-parameter", "bibtex": "@misc{zhang2023adalora,\n      title={AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning}, \n      author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},\n      year={2023},\n      eprint={2303.10512},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "18-03-2023", "categories": ["cs.CL", "cs.LG"]}, "2310.03710v1": {"paper_id": "2310.03710v1", "abs_url": "https://arxiv.org/abs/2310.03710v1", "pdf_url": "https://arxiv.org/pdf/2310.03710v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.03710v1_Agent_Instructs_Large_Language_Models_to_be_General_Zero-Shot_Reasoners.pdf", "title": "Agent Instructs Large Language Models to be General Zero-Shot Reasoners", "year": 2023, "paper_venue": null, "authors": ["Nicholas Crispino", "Kyle Montgomery", "Fankun Zeng", "Dawn Song", "Chenguang Wang"], "abstract": "We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.", "comments": "", "official_code_urls": ["https://github.com/wang-research-lab/agentinstruct"], "pwc_page_url": "https://paperswithcode.com/paper/agent-instructs-large-language-models-to-be", "bibtex": "@misc{crispino2023agent,\n      title={Agent Instructs Large Language Models to be General Zero-Shot Reasoners}, \n      author={Nicholas Crispino and Kyle Montgomery and Fankun Zeng and Dawn Song and Chenguang Wang},\n      year={2023},\n      eprint={2310.03710},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "05-10-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2309.07870v3": {"paper_id": "2309.07870v3", "abs_url": "https://arxiv.org/abs/2309.07870v3", "pdf_url": "https://arxiv.org/pdf/2309.07870v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.07870v3_Agents_An_Open-source_Framework_for_Autonomous_Language_Agents.pdf", "title": "Agents: An Open-source Framework for Autonomous Language Agents", "year": 2023, "paper_venue": null, "authors": ["Wangchunshu Zhou", "Yuchen Eleanor Jiang", "Long Li", "Jialong Wu", "Tiannan Wang", "Shi Qiu", "Jintian Zhang", "Jing Chen", "Ruipu Wu", "Shuai Wang", "Shiding Zhu", "Jiyu Chen", "Wentao Zhang", "Xiangru Tang", "Ningyu Zhang", "Huajun Chen", "Peng Cui", "Mrinmaya Sachan"], "abstract": ".", "comments": "Code available at this https URL", "official_code_urls": ["https://github.com/aiwaves-cn/agents"], "pwc_page_url": "https://paperswithcode.com/paper/agents-an-open-source-framework-for", "bibtex": "@misc{zhou2023agents,\n      title={Agents: An Open-source Framework for Autonomous Language Agents}, \n      author={Wangchunshu Zhou and Yuchen Eleanor Jiang and Long Li and Jialong Wu and Tiannan Wang and Shi Qiu and Jintian Zhang and Jing Chen and Ruipu Wu and Shuai Wang and Shiding Zhu and Jiyu Chen and Wentao Zhang and Xiangru Tang and Ningyu Zhang and Huajun Chen and Peng Cui and Mrinmaya Sachan},\n      year={2023},\n      eprint={2309.07870},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "14-09-2023", "categories": ["cs.CL"]}, "2311.15249v1": {"paper_id": "2311.15249v1", "abs_url": "https://arxiv.org/abs/2311.15249v1", "pdf_url": "https://arxiv.org/pdf/2311.15249v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.15249v1_Algorithm_Evolution_Using_Large_Language_Model.pdf", "title": "Algorithm Evolution Using Large Language Model", "year": 2023, "paper_venue": null, "authors": ["Fei Liu", "Xialiang Tong", "Mingxuan Yuan", "Qingfu Zhang"], "abstract": "Optimization can be found in many real-life applications. Designing an effective algorithm for a specific optimization problem typically requires a tedious amount of effort from human experts with domain knowledge and algorithm design skills. In this paper, we propose a novel approach called Algorithm Evolution using Large Language Model (AEL). It utilizes a large language model (LLM) to automatically generate optimization algorithms via an evolutionary framework. AEL does algorithm-level evolution without model training. Human effort and requirements for domain knowledge can be significantly reduced. We take constructive methods for the salesman traveling problem as a test example, we show that the constructive algorithm obtained by AEL outperforms simple hand-crafted and LLM-generated heuristics. Compared with other domain deep learning model-based algorithms, these methods exhibit excellent scalability across different problem sizes. AEL is also very different from previous attempts that utilize LLMs as search operators in algorithms.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/algorithm-evolution-using-large-language", "bibtex": "@misc{liu2023algorithm,\n      title={Algorithm Evolution Using Large Language Model}, \n      author={Fei Liu and Xialiang Tong and Mingxuan Yuan and Qingfu Zhang},\n      year={2023},\n      eprint={2311.15249},\n      archivePrefix={arXiv},\n      primaryClass={cs.NE}\n}", "published_date": "26-11-2023", "categories": ["cs.NE", "cs.AI", "cs.LG"]}, "2312.07000v1": {"paper_id": "2312.07000v1", "abs_url": "https://arxiv.org/abs/2312.07000v1", "pdf_url": "https://arxiv.org/pdf/2312.07000v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.07000v1_Alignment_for_Honesty.pdf", "title": "Alignment for Honesty", "year": 2023, "paper_venue": null, "authors": ["Yuqing Yang", "Ethan Chern", "Xipeng Qiu", "Graham Neubig", "Pengfei Liu"], "abstract": ", including honesty-aligned models, training and evaluation datasets for honesty alignment, concept glossary, as well as all relevant source code.", "comments": "", "official_code_urls": ["https://github.com/gair-nlp/alignment-for-honesty"], "pwc_page_url": "https://paperswithcode.com/paper/alignment-for-honesty", "bibtex": "@misc{yang2023alignment,\n      title={Alignment for Honesty}, \n      author={Yuqing Yang and Ethan Chern and Xipeng Qiu and Graham Neubig and Pengfei Liu},\n      year={2023},\n      eprint={2312.07000},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "12-12-2023", "categories": ["cs.CL", "cs.AI"]}, "2312.11444v2": {"paper_id": "2312.11444v2", "abs_url": "https://arxiv.org/abs/2312.11444v2", "pdf_url": "https://arxiv.org/pdf/2312.11444v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.11444v2_An_In-depth_Look_at_Geminis_Language_Abilities.pdf", "title": "An In-depth Look at Gemini's Language Abilities", "year": 2023, "paper_venue": null, "authors": ["Syeda Nahida Akter", "Zichun Yu", "Aashiq Muhamed", "Tianyue Ou", "Alex B\u00e4uerle", "\u00c1ngel Alexander Cabrera", "Krish Dholakia", "Chenyan Xiong", "Graham Neubig"], "abstract": "", "comments": "", "official_code_urls": ["https://github.com/neulab/gemini-benchmark"], "pwc_page_url": "https://paperswithcode.com/paper/an-in-depth-look-at-gemini-s-language", "bibtex": "@misc{akter2023indepth,\n      title={An In-depth Look at Gemini's Language Abilities}, \n      author={Syeda Nahida Akter and Zichun Yu and Aashiq Muhamed and Tianyue Ou and Alex B\u00e4uerle and \u00c1ngel Alexander Cabrera and Krish Dholakia and Chenyan Xiong and Graham Neubig},\n      year={2023},\n      eprint={2312.11444},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "18-12-2023", "categories": ["cs.CL", "cs.AI"]}, "2312.04511v1": {"paper_id": "2312.04511v1", "abs_url": "https://arxiv.org/abs/2312.04511v1", "pdf_url": "https://arxiv.org/pdf/2312.04511v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.04511v1_An_LLM_Compiler_for_Parallel_Function_Calling.pdf", "title": "An LLM Compiler for Parallel Function Calling", "year": 2023, "paper_venue": null, "authors": ["Sehoon Kim", "Suhong Moon", "Ryan Tabrizi", "Nicholas Lee", "Michael W. Mahoney", "Kurt Keutzer", "Amir Gholami"], "abstract": "Large Language Models (LLMs) have shown remarkable results on various complex reasoning benchmarks. The reasoning capabilities of LLMs enable them to execute function calls, using user-provided functions to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has expanded LLMs' scope to include multi-function calling, where LLMs are equipped with a variety of functions and select the proper functions based on the context. Multi-function calling abilities of LLMs have catalyzed LLM-based software development, allowing them to tackle more complex problems. However, current methods for multi-function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multi-function calling. Drawing from the principles of classical compilers, LLMCompiler streamlines parallel function calling with three components: (i) an LLM Planner, formulating execution strategies and dependencies; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. LLMCompiler automatically computes an optimized orchestration for the function calls and can be used with open-source models such as LLaMA-2. We have benchmarked LLMCompiler on a range of tasks including cases with non-trivial inter-dependency between function calls, as well as cases that require dynamic replanning based on intermediate results. We observe consistent latency speedup of up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to ~9% as compared to ReAct. Additionally, LLMCompiler achieves up to 1.35x latency gain over OpenAI's recent parallel function calling, while achieving similar accuracy.", "comments": "", "official_code_urls": ["https://github.com/squeezeailab/llmcompiler"], "pwc_page_url": "https://paperswithcode.com/paper/an-llm-compiler-for-parallel-function-calling", "bibtex": "@misc{kim2023llm,\n      title={An LLM Compiler for Parallel Function Calling}, \n      author={Sehoon Kim and Suhong Moon and Ryan Tabrizi and Nicholas Lee and Michael W. Mahoney and Kurt Keutzer and Amir Gholami},\n      year={2023},\n      eprint={2312.04511},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "07-12-2023", "categories": ["cs.CL"]}, "2312.13771v2": {"paper_id": "2312.13771v2", "abs_url": "https://arxiv.org/abs/2312.13771v2", "pdf_url": "https://arxiv.org/pdf/2312.13771v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.13771v2_AppAgent_Multimodal_Agents_as_Smartphone_Users.pdf", "title": "AppAgent: Multimodal Agents as Smartphone Users", "year": 2023, "paper_venue": null, "authors": ["Chi Zhang", "Zhao Yang", "Jiaxuan Liu", "Yucheng Han", "Xin Chen", "Zebiao Huang", "Bin Fu", "Gang Yu"], "abstract": "Recent advancements in large language models (LLMs) have led to the creation of intelligent agents capable of performing complex tasks. This paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications. Our framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps. Central to our agent's functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates a knowledge base that the agent refers to for executing complex tasks across different applications. To demonstrate the practicality of our agent, we conducted extensive testing over 50 tasks in 10 different applications, including social media, email, maps, shopping, and sophisticated image editing tools. The results affirm our agent's proficiency in handling a diverse array of high-level tasks.", "comments": "Project Page is this https URL", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/appagent-multimodal-agents-as-smartphone", "bibtex": "@misc{zhang2023appagent,\n      title={AppAgent: Multimodal Agents as Smartphone Users}, \n      author={Chi Zhang and Zhao Yang and Jiaxuan Liu and Yucheng Han and Xin Chen and Zebiao Huang and Bin Fu and Gang Yu},\n      year={2023},\n      eprint={2312.13771},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "21-12-2023", "categories": ["cs.CV"]}, "2312.04860v1": {"paper_id": "2312.04860v1", "abs_url": "https://arxiv.org/abs/2312.04860v1", "pdf_url": "https://arxiv.org/pdf/2312.04860v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.04860v1_Are_We_Testing_or_Being_Tested?_Exploring_the_Practical_Applications_of_Large_Language_Models_in_Software_Testing.pdf", "title": "Are We Testing or Being Tested? Exploring the Practical Applications of Large Language Models in Software Testing", "year": 2023, "paper_venue": null, "authors": ["Robson Santos", "Italo Santos", "Cleyton Magalhaes", "Ronnie de Souza Santos"], "abstract": "A Large Language Model (LLM) represents a cutting-edge artificial intelligence model that generates coherent content, including grammatically precise sentences, human-like paragraphs, and syntactically accurate code snippets. LLMs can play a pivotal role in software development, including software testing. LLMs go beyond traditional roles such as requirement analysis and documentation and can support test case generation, making them valuable tools that significantly enhance testing practices within the field. Hence, we explore the practical application of LLMs in software testing within an industrial setting, focusing on their current use by professional testers. In this context, rather than relying on existing data, we conducted a cross-sectional survey and collected data within real working contexts, specifically, engaging with practitioners in industrial settings. We applied quantitative and qualitative techniques to analyze and synthesize our collected data. Our findings demonstrate that LLMs effectively enhance testing documents and significantly assist testing professionals in programming tasks like debugging and test case automation. LLMs can support individuals engaged in manual testing who need to code. However, it is crucial to emphasize that, at this early stage, software testing professionals should use LLMs with caution while well-defined methods and guidelines are being built for the secure adoption of these tools.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/are-we-testing-or-being-tested-exploring-the", "bibtex": "@misc{santos2023testing,\n      title={Are We Testing or Being Tested? Exploring the Practical Applications of Large Language Models in Software Testing}, \n      author={Robson Santos and Italo Santos and Cleyton Magalhaes and Ronnie de Souza Santos},\n      year={2023},\n      eprint={2312.04860},\n      archivePrefix={arXiv},\n      primaryClass={cs.SE}\n}", "published_date": "08-12-2023", "categories": ["cs.SE"]}, "2309.17288v2": {"paper_id": "2309.17288v2", "abs_url": "https://arxiv.org/abs/2309.17288v2", "pdf_url": "https://arxiv.org/pdf/2309.17288v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.17288v2_AutoAgents_A_Framework_for_Automatic_Agent_Generation.pdf", "title": "AutoAgents: A Framework for Automatic Agent Generation", "year": 2023, "paper_venue": null, "authors": ["Guangyao Chen", "Siwei Dong", "Yu Shu", "Ge Zhang", "Jaward Sesay", "B\u00f6rje F. Karlsson", "Jie Fu", "Yemin Shi"], "abstract": ".", "comments": "", "official_code_urls": ["https://github.com/Link-AGI/AutoAgents"], "pwc_page_url": "https://paperswithcode.com/paper/autoagents-a-framework-for-automatic-agent", "bibtex": "@misc{chen2023autoagents,\n      title={AutoAgents: A Framework for Automatic Agent Generation}, \n      author={Guangyao Chen and Siwei Dong and Yu Shu and Ge Zhang and Jaward Sesay and B\u00f6rje F. Karlsson and Jie Fu and Yemin Shi},\n      year={2023},\n      eprint={2309.17288},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "29-09-2023", "categories": ["cs.AI"]}, "2305.03495v2": {"paper_id": "2305.03495v2", "abs_url": "https://arxiv.org/abs/2305.03495v2", "pdf_url": "https://arxiv.org/pdf/2305.03495v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.03495v2_Automatic_Prompt_Optimization_with_Gradient_Descent_and_Beam_Search.pdf", "title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search", "year": 2023, "paper_venue": null, "authors": ["Reid Pryzant", "Dan Iter", "Jerry Li", "Yin Tat Lee", "Chenguang Zhu", "Michael Zeng"], "abstract": "Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language \"gradients\" that criticize the current prompt. The gradients are then \"propagated\" into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.", "comments": "EMNLP 2023", "official_code_urls": ["https://github.com/microsoft/lmops"], "pwc_page_url": "https://paperswithcode.com/paper/automatic-prompt-optimization-with-gradient", "bibtex": "@misc{pryzant2023automatic,\n      title={Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search}, \n      author={Reid Pryzant and Dan Iter and Jerry Li and Yin Tat Lee and Chenguang Zhu and Michael Zeng},\n      year={2023},\n      eprint={2305.03495},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "04-05-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2304.01196v4": {"paper_id": "2304.01196v4", "abs_url": "https://arxiv.org/abs/2304.01196v4", "pdf_url": "https://arxiv.org/pdf/2304.01196v4.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2304.01196v4_Baize_An_Open-Source_Chat_Model_with_Parameter-Efficient_Tuning_on_Self-Chat_Data.pdf", "title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data", "year": 2023, "paper_venue": null, "authors": ["Canwen Xu", "Daya Guo", "Nan Duan", "Julian McAuley"], "abstract": ".", "comments": "Baize v2; EMNLP 2023", "official_code_urls": ["https://github.com/project-baize/baize-chatbot", "https://github.com/project-baize/baize"], "pwc_page_url": "https://paperswithcode.com/paper/baize-an-open-source-chat-model-with", "bibtex": "@misc{xu2023baize,\n      title={Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data}, \n      author={Canwen Xu and Daya Guo and Nan Duan and Julian McAuley},\n      year={2023},\n      eprint={2304.01196},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "03-04-2023", "categories": ["cs.CL", "cs.AI"]}, "2310.07298v1": {"paper_id": "2310.07298v1", "abs_url": "https://arxiv.org/abs/2310.07298v1", "pdf_url": "https://arxiv.org/pdf/2310.07298v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.07298v1_Beyond_Memorization_Violating_Privacy_Via_Inference_with_Large_Language_Models.pdf", "title": "Beyond Memorization: Violating Privacy Via Inference with Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Robin Staab", "Mark Vero", "Mislav Balunovi\u0107", "Martin Vechev"], "abstract": "Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\\%$ top-1 and $95.8\\%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time ($240\\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for a wider privacy protection.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/beyond-memorization-violating-privacy-via", "bibtex": "@misc{staab2023memorization,\n      title={Beyond Memorization: Violating Privacy Via Inference with Large Language Models}, \n      author={Robin Staab and Mark Vero and Mislav Balunovi\u0107 and Martin Vechev},\n      year={2023},\n      eprint={2310.07298},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "11-10-2023", "categories": ["cs.AI", "cs.LG", "I.2.7"]}, "2303.17564v3": {"paper_id": "2303.17564v3", "abs_url": "https://arxiv.org/abs/2303.17564v3", "pdf_url": "https://arxiv.org/pdf/2303.17564v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2303.17564v3_BloombergGPT_A_Large_Language_Model_for_Finance.pdf", "title": "BloombergGPT: A Large Language Model for Finance", "year": 2023, "paper_venue": null, "authors": ["Shijie Wu", "Ozan Irsoy", "Steven Lu", "Vadim Dabravolski", "Mark Dredze", "Sebastian Gehrmann", "Prabhanjan Kambadur", "David Rosenberg", "Gideon Mann"], "abstract": "The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.", "comments": "Updated to include Training Chronicles (Appendix C)", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/bloomberggpt-a-large-language-model-for", "bibtex": "@misc{wu2023bloomberggpt,\n      title={BloombergGPT: A Large Language Model for Finance}, \n      author={Shijie Wu and Ozan Irsoy and Steven Lu and Vadim Dabravolski and Mark Dredze and Sebastian Gehrmann and Prabhanjan Kambadur and David Rosenberg and Gideon Mann},\n      year={2023},\n      eprint={2303.17564},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "30-03-2023", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-fin.GN"]}, "2310.08678v1": {"paper_id": "2310.08678v1", "abs_url": "https://arxiv.org/abs/2310.08678v1", "pdf_url": "https://arxiv.org/pdf/2310.08678v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.08678v1_Can_GPT_models_be_Financial_Analysts?_An_Evaluation_of_ChatGPT_and_GPT-4_on_mock_CFA_Exams.pdf", "title": "Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams", "year": 2023, "paper_venue": null, "authors": ["Ethan Callanan", "Amarachi Mbakwe", "Antony Papadimitriou", "Yulong Pei", "Mathieu Sibue", "Xiaodan Zhu", "Zhiqiang Ma", "Xiaomo Liu", "Sameena Shah"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on a wide range of Natural Language Processing (NLP) tasks, often matching or even beating state-of-the-art task-specific models. This study aims at assessing the financial reasoning capabilities of LLMs. We leverage mock exam questions of the Chartered Financial Analyst (CFA) Program to conduct a comprehensive evaluation of ChatGPT and GPT-4 in financial analysis, considering Zero-Shot (ZS), Chain-of-Thought (CoT), and Few-Shot (FS) scenarios. We present an in-depth analysis of the models' performance and limitations, and estimate whether they would have a chance at passing the CFA exams. Finally, we outline insights into potential strategies and improvements to enhance the applicability of LLMs in finance. In this perspective, we hope this work paves the way for future studies to continue enhancing LLMs for financial reasoning through rigorous evaluation.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/can-gpt-models-be-financial-analysts-an", "bibtex": "@misc{callanan2023gpt,\n      title={Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams}, \n      author={Ethan Callanan and Amarachi Mbakwe and Antony Papadimitriou and Yulong Pei and Mathieu Sibue and Xiaodan Zhu and Zhiqiang Ma and Xiaomo Liu and Sameena Shah},\n      year={2023},\n      eprint={2310.08678},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "12-10-2023", "categories": ["cs.CL", "cs.AI", "q-fin.GN"]}, "2311.04235v1": {"paper_id": "2311.04235v1", "abs_url": "https://arxiv.org/abs/2311.04235v1", "pdf_url": "https://arxiv.org/pdf/2311.04235v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.04235v1_Can_LLMs_Follow_Simple_Rules?.pdf", "title": "Can LLMs Follow Simple Rules?", "year": 2023, "paper_venue": null, "authors": ["Norman Mu", "Sarah Chen", "Zifan Wang", "Sizhe Chen", "David Karamardian", "Lulwa Aljeraisy", "Dan Hendrycks", "David Wagner"], "abstract": "As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as \"do not generate abusive content\", but these may be circumvented by jailbreaking techniques. Evaluating how well LLMs follow developer-provided rules in the face of adversarial inputs typically requires manual review, which slows down monitoring and methods development. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 15 simple text scenarios in which the model is instructed to obey a set of rules in natural language while interacting with the human user. Each scenario has a concise evaluation program to determine whether the model has broken any rules in a conversation. Through manual exploration of model behavior in our scenarios, we identify 6 categories of attack strategies and collect two suites of test cases: one consisting of unique conversations from manual testing and one that systematically implements strategies from the 6 categories. Across various popular proprietary and open models such as GPT-4 and Llama 2, we find that all models are susceptible to a wide variety of adversarial hand-crafted user inputs, though GPT-4 is the best-performing model. Additionally, we evaluate open models under gradient-based attacks and find significant vulnerabilities. We propose RuLES as a challenging new setting for research into exploring and defending against both manual and automatic attacks on LLMs.", "comments": "Project website: this https URL", "official_code_urls": ["https://github.com/normster/llm_rules"], "pwc_page_url": "https://paperswithcode.com/paper/can-llms-follow-simple-rules", "bibtex": "@misc{mu2023llms,\n      title={Can LLMs Follow Simple Rules?}, \n      author={Norman Mu and Sarah Chen and Zifan Wang and Sizhe Chen and David Karamardian and Lulwa Aljeraisy and Dan Hendrycks and David Wagner},\n      year={2023},\n      eprint={2311.04235},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "06-11-2023", "categories": ["cs.AI", "cs.CL", "cs.LG"]}, "2310.01783v1": {"paper_id": "2310.01783v1", "abs_url": "https://arxiv.org/abs/2310.01783v1", "pdf_url": "https://arxiv.org/pdf/2310.01783v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.01783v1_Can_large_language_models_provide_useful_feedback_on_research_papers?_A_large-scale_empirical_analysis.pdf", "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis", "year": 2023, "paper_venue": null, "authors": ["Weixin Liang", "Yuhui Zhang", "Hancheng Cao", "Binglu Wang", "Daisy Ding", "Xinyu Yang", "Kailas Vodrahalli", "Siyu He", "Daniel Smith", "Yian Yin", "Daniel McFarland", "James Zou"], "abstract": "Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.", "comments": "", "official_code_urls": ["https://github.com/weixin-liang/llm-scientific-feedback"], "pwc_page_url": "https://paperswithcode.com/paper/can-large-language-models-provide-useful", "bibtex": "@misc{liang2023large,\n      title={Can large language models provide useful feedback on research papers? A large-scale empirical analysis}, \n      author={Weixin Liang and Yuhui Zhang and Hancheng Cao and Binglu Wang and Daisy Ding and Xinyu Yang and Kailas Vodrahalli and Siyu He and Daniel Smith and Yian Yin and Daniel McFarland and James Zou},\n      year={2023},\n      eprint={2310.01783},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "03-10-2023", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"]}, "2312.10253v1": {"paper_id": "2312.10253v1", "abs_url": "https://arxiv.org/abs/2312.10253v1", "pdf_url": "https://arxiv.org/pdf/2312.10253v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.10253v1_Catwalk_A_Unified_Language_Model_Evaluation_Framework_for_Many_Datasets.pdf", "title": "Catwalk: A Unified Language Model Evaluation Framework for Many Datasets", "year": 2023, "paper_venue": null, "authors": ["Dirk Groeneveld", "Anas Awadalla", "Iz Beltagy", "Akshita Bhagia", "Ian Magnusson", "Hao Peng", "Oyvind Tafjord", "Pete Walsh", "Kyle Richardson", "Jesse Dodge"], "abstract": ".", "comments": "technical report, work in progress", "official_code_urls": ["https://github.com/allenai/catwalk"], "pwc_page_url": "https://paperswithcode.com/paper/catwalk-a-unified-language-model-evaluation", "bibtex": "@misc{groeneveld2023catwalk,\n      title={Catwalk: A Unified Language Model Evaluation Framework for Many Datasets}, \n      author={Dirk Groeneveld and Anas Awadalla and Iz Beltagy and Akshita Bhagia and Ian Magnusson and Hao Peng and Oyvind Tafjord and Pete Walsh and Kyle Richardson and Jesse Dodge},\n      year={2023},\n      eprint={2312.10253},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "15-12-2023", "categories": ["cs.CL"]}, "2306.00550v1": {"paper_id": "2306.00550v1", "abs_url": "https://arxiv.org/abs/2306.00550v1", "pdf_url": "https://arxiv.org/pdf/2306.00550v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2306.00550v1_Chain-Of-Thought_Prompting_Under_Streaming_Batch_A_Case_Study.pdf", "title": "Chain-Of-Thought Prompting Under Streaming Batch: A Case Study", "year": 2023, "paper_venue": null, "authors": ["Yuxin Tang"], "abstract": "Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities. Chain-of-Thought (CoT) has been proposed as a way of assisting LLMs in performing complex reasoning. However, developing effective prompts can be a challenging and labor-intensive task. Many studies come out of some way to automatically construct CoT from test data. Most of them assume that all test data is visible before testing and only select a small subset to generate rationales, which is an unrealistic assumption. In this paper, we present a case study on how to construct and optimize chain-of-thought prompting using batch data in streaming settings.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/chain-of-thought-prompting-under-streaming", "bibtex": "@misc{tang2023chainofthought,\n      title={Chain-Of-Thought Prompting Under Streaming Batch: A Case Study}, \n      author={Yuxin Tang},\n      year={2023},\n      eprint={2306.00550},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "01-06-2023", "categories": ["cs.LG", "cs.AI", "cs.CL"]}, "2309.11495v2": {"paper_id": "2309.11495v2", "abs_url": "https://arxiv.org/abs/2309.11495v2", "pdf_url": "https://arxiv.org/pdf/2309.11495v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.11495v2_Chain-of-Verification_Reduces_Hallucination_in_Large_Language_Models.pdf", "title": "Chain-of-Verification Reduces Hallucination in Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Shehzaad Dhuliawala", "Mojtaba Komeili", "Jing Xu", "Roberta Raileanu", "Xian Li", "Asli Celikyilmaz", "Jason Weston"], "abstract": "Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/chain-of-verification-reduces-hallucination", "bibtex": "@misc{dhuliawala2023chainofverification,\n      title={Chain-of-Verification Reduces Hallucination in Large Language Models}, \n      author={Shehzaad Dhuliawala and Mojtaba Komeili and Jing Xu and Roberta Raileanu and Xian Li and Asli Celikyilmaz and Jason Weston},\n      year={2023},\n      eprint={2309.11495},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "20-09-2023", "categories": ["cs.CL", "cs.AI"]}, "2312.17080v1": {"paper_id": "2312.17080v1", "abs_url": "https://arxiv.org/abs/2312.17080v1", "pdf_url": "https://arxiv.org/pdf/2312.17080v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.17080v1_Challenge_LLMs_to_Reason_About_Reasoning_A_Benchmark_to_Unveil_Cognitive_Depth_in_LLMs.pdf", "title": "Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs", "year": 2023, "paper_venue": null, "authors": ["Zhongshen Zeng", "Pengguang Chen", "Haiyun Jiang", "Jiaya Jia"], "abstract": "In this work, we introduce a novel evaluation paradigm for Large Language Models, one that challenges them to engage in meta-reasoning. This approach addresses critical shortcomings in existing math problem-solving benchmarks, traditionally used to evaluate the cognitive capabilities of agents. Our paradigm shifts the focus from result-oriented assessments, which often overlook the reasoning process, to a more holistic evaluation that effectively differentiates the cognitive capabilities among models. For example, in our benchmark, GPT-4 demonstrates a performance ten times more accurate than GPT3-5. The significance of this new paradigm lies in its ability to reveal potential cognitive deficiencies in LLMs that current benchmarks, such as GSM8K, fail to uncover due to their saturation and lack of effective differentiation among varying reasoning abilities. Our comprehensive analysis includes several state-of-the-art math models from both open-source and closed-source communities, uncovering fundamental deficiencies in their training and evaluation approaches. This paper not only advocates for a paradigm shift in the assessment of LLMs but also contributes to the ongoing discourse on the trajectory towards Artificial General Intelligence (AGI). By promoting the adoption of meta-reasoning evaluation methods similar to ours, we aim to facilitate a more accurate assessment of the true cognitive abilities of LLMs.", "comments": "Code: this https URL", "official_code_urls": ["https://github.com/dvlab-research/diaggsm8k"], "pwc_page_url": "https://paperswithcode.com/paper/challenge-llms-to-reason-about-reasoning-a", "bibtex": "@misc{zeng2023challenge,\n      title={Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs}, \n      author={Zhongshen Zeng and Pengguang Chen and Haiyun Jiang and Jiaya Jia},\n      year={2023},\n      eprint={2312.17080},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "28-12-2023", "categories": ["cs.CL"]}, "2311.16989v4": {"paper_id": "2311.16989v4", "abs_url": "https://arxiv.org/abs/2311.16989v4", "pdf_url": "https://arxiv.org/pdf/2311.16989v4.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.16989v4_ChatGPTs_One-year_Anniversary_Are_Open-Source_Large_Language_Models_Catching_up?.pdf", "title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?", "year": 2023, "paper_venue": null, "authors": ["Hailin Chen", "Fangkai Jiao", "Xingxuan Li", "Chengwei Qin", "Mathieu Ravaut", "Ruochen Zhao", "Caiming Xiong", "Shafiq Joty"], "abstract": "Upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce. Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work, on the first anniversary of ChatGPT, we provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.", "comments": "version v4, included latest top-performing open-sourced LLMs", "official_code_urls": ["https://github.com/ntunlp/opensource-llms-better-than-openai"], "pwc_page_url": "https://paperswithcode.com/paper/chatgpt-s-one-year-anniversary-are-open", "bibtex": "@misc{chen2024chatgpts,\n      title={ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?}, \n      author={Hailin Chen and Fangkai Jiao and Xingxuan Li and Chengwei Qin and Mathieu Ravaut and Ruochen Zhao and Caiming Xiong and Shafiq Joty},\n      year={2024},\n      eprint={2311.16989},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "28-11-2023", "categories": ["cs.CL"]}, "2310.16146v1": {"paper_id": "2310.16146v1", "abs_url": "https://arxiv.org/abs/2310.16146v1", "pdf_url": "https://arxiv.org/pdf/2310.16146v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.16146v1_Clinfoai_An_Open-Source_Retrieval-Augmented_Large_Language_Model_System_for_Answering_Medical_Questions_using_Scientific_Literature.pdf", "title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature", "year": 2023, "paper_venue": null, "authors": ["Alejandro Lozano", "Scott L Fleming", "Chia-Chun Chiang", "Nigam Shah"], "abstract": "and other publicly available OpenQA systems on PubMedRS-200.", "comments": "Preprint of an article published in Pacific Symposium on Biocomputing copyright 2024 World Scientific Publishing Co., Singapore, this http URL", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/clinfo-ai-an-open-source-retrieval-augmented", "bibtex": "@misc{lozano2023clinfoai,\n      title={Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature}, \n      author={Alejandro Lozano and Scott L Fleming and Chia-Chun Chiang and Nigam Shah},\n      year={2023},\n      eprint={2310.16146},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}", "published_date": "24-10-2023", "categories": ["cs.IR", "cs.AI", "cs.CL"]}, "2309.07430v3": {"paper_id": "2309.07430v3", "abs_url": "https://arxiv.org/abs/2309.07430v3", "pdf_url": "https://arxiv.org/pdf/2309.07430v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.07430v3_Clinical_Text_Summarization_Adapting_Large_Language_Models_Can_Outperform_Human_Experts.pdf", "title": "Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts", "year": 2023, "paper_venue": null, "authors": ["Dave Van Veen", "Cara Van Uden", "Louis Blankemeier", "Jean-Benoit Delbrouck", "Asad Aali", "Christian Bluethgen", "Anuj Pareek", "Malgorzata Polacin", "Eduardo Pontes Reis", "Anna Seehofnerova", "Nidhi Rohatgi", "Poonam Hosamani", "William Collins", "Neera Ahuja", "Curtis P. Langlotz", "Jason Hom", "Sergios Gatidis", "John Pauly", "Akshay S. Chaudhari"], "abstract": "Sifting through vast textual data and summarizing key information from electronic health records (EHR) imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy on a diverse range of clinical summarization tasks has not yet been rigorously demonstrated. In this work, we apply domain adaptation methods to eight LLMs, spanning six datasets and four distinct clinical summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not improve results. Further, in a clinical reader study with ten physicians, we show that summaries from our best-adapted LLMs are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis highlights challenges faced by both LLMs and human experts. Lastly, we correlate traditional quantitative NLP metrics with reader study scores to enhance our understanding of how these metrics align with physician preferences. Our research marks the first evidence of LLMs outperforming human experts in clinical text summarization across multiple tasks. This implies that integrating LLMs into clinical workflows could alleviate documentation burden, empowering clinicians to focus more on personalized patient care and the inherently human aspects of medicine.", "comments": "24 pages, 24 figures. Compared to the original, newer versions include minor edits and supplementary additional experiments that reinforce the initial findings", "official_code_urls": ["https://github.com/stanfordmimi/clin-summ"], "pwc_page_url": "https://paperswithcode.com/paper/clinical-text-summarization-adapting-large", "bibtex": "@misc{vanveen2023clinical,\n      title={Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts}, \n      author={Dave Van Veen and Cara Van Uden and Louis Blankemeier and Jean-Benoit Delbrouck and Asad Aali and Christian Bluethgen and Anuj Pareek and Malgorzata Polacin and Eduardo Pontes Reis and Anna Seehofnerova and Nidhi Rohatgi and Poonam Hosamani and William Collins and Neera Ahuja and Curtis P. Langlotz and Jason Hom and Sergios Gatidis and John Pauly and Akshay S. Chaudhari},\n      year={2023},\n      eprint={2309.07430},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "14-09-2023", "categories": ["cs.CL"]}, "2312.08914v2": {"paper_id": "2312.08914v2", "abs_url": "https://arxiv.org/abs/2312.08914v2", "pdf_url": "https://arxiv.org/pdf/2312.08914v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.08914v2_CogAgent_A_Visual_Language_Model_for_GUI_Agents.pdf", "title": "CogAgent: A Visual Language Model for GUI Agents", "year": 2023, "paper_venue": null, "authors": ["Wenyi Hong", "Weihan Wang", "Qingsong Lv", "Jiazheng Xu", "Wenmeng Yu", "Junhui Ji", "Yan Wang", "Zihan Wang", "Yuxuan Zhang", "Juanzi Li", "Bin Xu", "Yuxiao Dong", "Ming Ding", "Jie Tang"], "abstract": ".", "comments": "27 pages, 19 figures", "official_code_urls": ["https://github.com/thudm/cogvlm"], "pwc_page_url": "https://paperswithcode.com/paper/cogagent-a-visual-language-model-for-gui", "bibtex": "@misc{hong2023cogagent,\n      title={CogAgent: A Visual Language Model for GUI Agents}, \n      author={Wenyi Hong and Weihan Wang and Qingsong Lv and Jiazheng Xu and Wenmeng Yu and Junhui Ji and Yan Wang and Zihan Wang and Yuxuan Zhang and Juanzi Li and Bin Xu and Yuxiao Dong and Ming Ding and Jie Tang},\n      year={2023},\n      eprint={2312.08914},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "14-12-2023", "categories": ["cs.CV"]}, "2302.00763v1": {"paper_id": "2302.00763v1", "abs_url": "https://arxiv.org/abs/2302.00763v1", "pdf_url": "https://arxiv.org/pdf/2302.00763v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2302.00763v1_Collaborating_with_language_models_for_embodied_reasoning.pdf", "title": "Collaborating with language models for embodied reasoning", "year": 2023, "paper_venue": null, "authors": ["Ishita Dasgupta", "Christine Kaeser-Chen", "Kenneth Marino", "Arun Ahuja", "Sheila Babayan", "Felix Hill", "Rob Fergus"], "abstract": "Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.", "comments": "Presented at NeurIPS 2022 Language and Reinforcement Learning Workshop (best paper) and NeurIPS 2022 Foundation Models for Decision Making Workshop. 4 pages main; 14 pages total (including references and appendix); 3 figures", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/collaborating-with-language-models-for", "bibtex": "@misc{dasgupta2023collaborating,\n      title={Collaborating with language models for embodied reasoning}, \n      author={Ishita Dasgupta and Christine Kaeser-Chen and Kenneth Marino and Arun Ahuja and Sheila Babayan and Felix Hill and Rob Fergus},\n      year={2023},\n      eprint={2302.00763},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "01-02-2023", "categories": ["cs.LG", "cs.AI", "cs.CL"]}, "2305.11186v2": {"paper_id": "2305.11186v2", "abs_url": "https://arxiv.org/abs/2305.11186v2", "pdf_url": "https://arxiv.org/pdf/2305.11186v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.11186v2_Compress_Then_Prompt_Improving_Accuracy-Efficiency_Trade-off_of_LLM_Inference_with_Transferable_Prompt.pdf", "title": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt", "year": 2023, "paper_venue": null, "authors": ["Zhaozhuo Xu", "Zirui Liu", "Beidi Chen", "Yuxin Tang", "Jue Wang", "Kaixiong Zhou", "Xia Hu", "Anshumali Shrivastava"], "abstract": "While the numerous parameters in Large Language Models (LLMs) contribute to their superior performance, this massive scale makes them inefficient and memory-hungry. Thus, they are hard to deploy on commodity hardware, such as one single GPU. Given the memory and power constraints of such devices, model compression methods are widely employed to reduce both the model size and inference latency, which essentially trades off model quality in return for improved efficiency. Thus, optimizing this accuracy-efficiency trade-off is crucial for the LLM deployment on commodity hardware. In this paper, we introduce a new perspective to optimize this trade-off by prompting compressed models. Specifically, we first observe that for certain questions, the generation quality of a compressed LLM can be significantly improved by adding carefully designed hard prompts, though this isn't the case for all questions. Based on this observation, we propose a soft prompt learning method where we expose the compressed model to the prompt learning process, aiming to enhance the performance of prompts. Our experimental analysis suggests our soft prompt strategy greatly improves the performance of the 8x compressed LLaMA-7B model (with a joint 4-bit quantization and 50% weight pruning compression), allowing them to match their uncompressed counterparts on popular benchmarks. Also, we demonstrate that these learned prompts can be transferred across various datasets, tasks, and compression levels. Hence with this transferability, we can stitch the soft prompt to a newly compressed model to improve the test-time accuracy in an ``in-situ'' way.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/compress-then-prompt-improving-accuracy", "bibtex": "@misc{xu2023compress,\n      title={Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt}, \n      author={Zhaozhuo Xu and Zirui Liu and Beidi Chen and Yuxin Tang and Jue Wang and Kaixiong Zhou and Xia Hu and Anshumali Shrivastava},\n      year={2023},\n      eprint={2305.11186},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "17-05-2023", "categories": ["cs.CL", "cs.LG"]}, "2310.06201v1": {"paper_id": "2310.06201v1", "abs_url": "https://arxiv.org/abs/2310.06201v1", "pdf_url": "https://arxiv.org/pdf/2310.06201v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.06201v1_Compressing_Context_to_Enhance_Inference_Efficiency_of_Large_Language_Models.pdf", "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Yucheng Li", "Bo Dong", "Chenghua Lin", "Frank Guerin"], "abstract": "Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50\\% reduction in context cost, resulting in a 36\\% reduction in inference memory usage and a 32\\% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.", "comments": "EMNLP 2023. arXiv admin note: substantial text overlap with arXiv:2304.12102 ; text overlap with arXiv:2303.11076 by other authors", "official_code_urls": ["https://github.com/liyucheng09/selective_context"], "pwc_page_url": "https://paperswithcode.com/paper/compressing-context-to-enhance-inference", "bibtex": "@misc{li2023compressing,\n      title={Compressing Context to Enhance Inference Efficiency of Large Language Models}, \n      author={Yucheng Li and Bo Dong and Chenghua Lin and Frank Guerin},\n      year={2023},\n      eprint={2310.06201},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "09-10-2023", "categories": ["cs.CL"]}, "2309.08532v1": {"paper_id": "2309.08532v1", "abs_url": "https://arxiv.org/abs/2309.08532v1", "pdf_url": "https://arxiv.org/pdf/2309.08532v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.08532v1_Connecting_Large_Language_Models_with_Evolutionary_Algorithms_Yields_Powerful_Prompt_Optimizers.pdf", "title": "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers", "year": 2023, "paper_venue": null, "authors": ["Qingyan Guo", "Rui Wang", "Junliang Guo", "Bei Li", "Kaitao Song", "Xu Tan", "Guoqing Liu", "Jiang Bian", "Yujiu Yang"], "abstract": "Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 9 datasets spanning language understanding and generation tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation by up to 25% and 14% respectively. Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.", "comments": "Work in progress", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/connecting-large-language-models-with", "bibtex": "@misc{guo2023connecting,\n      title={Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers}, \n      author={Qingyan Guo and Rui Wang and Junliang Guo and Bei Li and Kaitao Song and Xu Tan and Guoqing Liu and Jiang Bian and Yujiu Yang},\n      year={2023},\n      eprint={2309.08532},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "15-09-2023", "categories": ["cs.CL", "cs.AI"]}, "2311.09277v1": {"paper_id": "2311.09277v1", "abs_url": "https://arxiv.org/abs/2311.09277v1", "pdf_url": "https://arxiv.org/pdf/2311.09277v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.09277v1_Contrastive_Chain-of-Thought_Prompting.pdf", "title": "Contrastive Chain-of-Thought Prompting", "year": 2023, "paper_venue": null, "authors": ["Yew Ken Chia", "Guizhen Chen", "Luu Anh Tuan", "Soujanya Poria", "Lidong Bing"], "abstract": "Despite the success of chain of thought in enhancing language model reasoning, the underlying process remains less well understood. Although logically sound reasoning appears inherently crucial for chain of thought, prior studies surprisingly reveal minimal impact when using invalid demonstrations instead. Furthermore, the conventional chain of thought does not inform language models on what mistakes to avoid, which potentially leads to more errors. Hence, inspired by how humans can learn from both positive and negative examples, we propose contrastive chain of thought to enhance language model reasoning. Compared to the conventional chain of thought, our approach provides both valid and invalid reasoning demonstrations, to guide the model to reason step-by-step while reducing reasoning mistakes. To improve generalization, we introduce an automatic method to construct contrastive demonstrations. Our experiments on reasoning benchmarks demonstrate that contrastive chain of thought can serve as a general enhancement of chain-of-thought prompting.", "comments": "", "official_code_urls": ["https://github.com/damo-nlp-sg/contrastive-cot"], "pwc_page_url": "https://paperswithcode.com/paper/contrastive-chain-of-thought-prompting", "bibtex": "@misc{chia2023contrastive,\n      title={Contrastive Chain-of-Thought Prompting}, \n      author={Yew Ken Chia and Guizhen Chen and Luu Anh Tuan and Soujanya Poria and Lidong Bing},\n      year={2023},\n      eprint={2311.09277},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "15-11-2023", "categories": ["cs.CL"]}, "2310.13639v2": {"paper_id": "2310.13639v2", "abs_url": "https://arxiv.org/abs/2310.13639v2", "pdf_url": "https://arxiv.org/pdf/2310.13639v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.13639v2_Contrastive_Preference_Learning_Learning_from_Human_Feedback_without_RL.pdf", "title": "Contrastive Preference Learning: Learning from Human Feedback without RL", "year": 2023, "paper_venue": null, "authors": ["Joey Hejna", "Rafael Rafailov", "Harshit Sikchi", "Chelsea Finn", "Scott Niekum", "W. Bradley Knox", "Dorsa Sadigh"], "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the regret-based model of human preferences. Using the principle of maximum entropy, we derive Contrastive Preference Learning (CPL), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL. CPL is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs. This enables CPL to elegantly scale to high-dimensional and sequential RLHF problems while being simpler than prior methods.", "comments": "Code released at this https URL . Edited 10/23 only to fix typo in the title", "official_code_urls": ["https://github.com/jhejna/cpl"], "pwc_page_url": "https://paperswithcode.com/paper/contrastive-prefence-learning-learning-from", "bibtex": "@misc{hejna2023contrastive,\n      title={Contrastive Preference Learning: Learning from Human Feedback without RL}, \n      author={Joey Hejna and Rafael Rafailov and Harshit Sikchi and Chelsea Finn and Scott Niekum and W. Bradley Knox and Dorsa Sadigh},\n      year={2023},\n      eprint={2310.13639},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "20-10-2023", "categories": ["cs.LG", "cs.AI"]}, "2310.02374v3": {"paper_id": "2310.02374v3", "abs_url": "https://arxiv.org/abs/2310.02374v3", "pdf_url": "https://arxiv.org/pdf/2310.02374v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.02374v3_Conversational_Health_Agents_A_Personalized_LLM-Powered_Agent_Framework.pdf", "title": "Conversational Health Agents: A Personalized LLM-Powered Agent Framework", "year": 2023, "paper_venue": null, "authors": ["Mahyar Abbasian", "Iman Azimi", "Amir M. Rahmani", "Ramesh Jain"], "abstract": "Conversational Health Agents (CHAs) are interactive systems that provide healthcare services, such as assistance, self-awareness, and diagnosis. Current CHAs, especially those utilizing Large Language Models (LLMs), primarily focus on conversation aspects. However, they offer limited agent capabilities specifically lacking multi-step problem-solving, empathetic conversations, and multimodal data analysis. Our aim is to overcome these limitations. In this paper, we propose an LLM-powered framework to empower CHAs to generate a personalized response for users' healthcare queries. This framework provides critical thinking, knowledge acquisition, and problem-solving abilities by integrating healthcare data sources, enabling multilingual and multimodal conversations, and interacting with various user data analysis tools. We illustrate the framework's proficiency in handling complex healthcare tasks via a case study on stress level estimation, showcasing the agent's cognitive and operational capabilities. Powered by our framework, the CHA can provide appropriate responses, when the user inquires about their stress level. To achieve this, it learns to collect photoplethysmogram signals, converts them into heart rate variability, and interprets them as indicators of stress levels.", "comments": "23 pages, 5 figures, journal paper", "official_code_urls": ["https://github.com/Institute4FutureHealth/CHA"], "pwc_page_url": "https://paperswithcode.com/paper/conversational-health-agents-a-personalized", "bibtex": "@misc{abbasian2023conversational,\n      title={Conversational Health Agents: A Personalized LLM-Powered Agent Framework}, \n      author={Mahyar Abbasian and Iman Azimi and Amir M. Rahmani and Ramesh Jain},\n      year={2023},\n      eprint={2310.02374},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "03-10-2023", "categories": ["cs.CL"]}, "2312.02519v1": {"paper_id": "2312.02519v1", "abs_url": "https://arxiv.org/abs/2312.02519v1", "pdf_url": "https://arxiv.org/pdf/2312.02519v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.02519v1_Creative_Agents_Empowering_Agents_with_Imagination_for_Creative_Tasks.pdf", "title": "Creative Agents: Empowering Agents with Imagination for Creative Tasks", "year": 2023, "paper_venue": null, "authors": ["Chi Zhang", "Penglin Cai", "Yuhui Fu", "Haoqi Yuan", "Zongqing Lu"], "abstract": ").", "comments": "The first two authors contribute equally", "official_code_urls": ["https://github.com/pku-rl/creative-agents"], "pwc_page_url": "https://paperswithcode.com/paper/creative-agents-empowering-agents-with", "bibtex": "@misc{zhang2023creative,\n      title={Creative Agents: Empowering Agents with Imagination for Creative Tasks}, \n      author={Chi Zhang and Penglin Cai and Yuhui Fu and Haoqi Yuan and Zongqing Lu},\n      year={2023},\n      eprint={2312.02519},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "05-12-2023", "categories": ["cs.AI", "cs.LG"]}, "2308.04371v5": {"paper_id": "2308.04371v5", "abs_url": "https://arxiv.org/abs/2308.04371v5", "pdf_url": "https://arxiv.org/pdf/2308.04371v5.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2308.04371v5_Cumulative_Reasoning_with_Large_Language_Models.pdf", "title": "Cumulative Reasoning with Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Yifan Zhang", "Jingqin Yang", "Yang Yuan", "Andrew Chi-Chih Yao"], "abstract": ".", "comments": "", "official_code_urls": ["https://github.com/iiis-ai/cumulative-reasoning"], "pwc_page_url": "https://paperswithcode.com/paper/cumulative-reasoning-with-large-language", "bibtex": "@misc{zhang2023cumulative,\n      title={Cumulative Reasoning with Large Language Models}, \n      author={Yifan Zhang and Jingqin Yang and Yang Yuan and Andrew Chi-Chih Yao},\n      year={2023},\n      eprint={2308.04371},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "08-08-2023", "categories": ["cs.AI"]}, "2312.01700v2": {"paper_id": "2312.01700v2", "abs_url": "https://arxiv.org/abs/2312.01700v2", "pdf_url": "https://arxiv.org/pdf/2312.01700v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.01700v2_Data_Management_For_Large_Language_Models_A_Survey.pdf", "title": "Data Management For Large Language Models: A Survey", "year": 2023, "paper_venue": null, "authors": ["Zige Wang", "Wanjun Zhong", "Yufei Wang", "Qi Zhu", "Fei Mi", "Baojun Wang", "Lifeng Shang", "Xin Jiang", "Qun Liu"], "abstract": ".", "comments": "Work in progress", "official_code_urls": ["https://github.com/zigew/data_management_llm"], "pwc_page_url": "https://paperswithcode.com/paper/data-management-for-large-language-models-a", "bibtex": "@misc{wang2023data,\n      title={Data Management For Large Language Models: A Survey}, \n      author={Zige Wang and Wanjun Zhong and Yufei Wang and Qi Zhu and Fei Mi and Baojun Wang and Lifeng Shang and Xin Jiang and Qun Liu},\n      year={2023},\n      eprint={2312.01700},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "04-12-2023", "categories": ["cs.CL", "cs.AI"]}, "2302.01560v2": {"paper_id": "2302.01560v2", "abs_url": "https://arxiv.org/abs/2302.01560v2", "pdf_url": "https://arxiv.org/pdf/2302.01560v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2302.01560v2_Describe_Explain_Plan_and_Select_Interactive_Planning_with_Large_Language_Models_Enables_Open-World_Multi-Task_Agents.pdf", "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "year": 2023, "paper_venue": null, "authors": ["Zihao Wang", "Shaofei Cai", "Guanzhou Chen", "Anji Liu", "Xiaojian Ma", "Yitao Liang"], "abstract": ".", "comments": "NeurIPS 2023", "official_code_urls": ["https://github.com/craftjarvis/mc-planner"], "pwc_page_url": "https://paperswithcode.com/paper/describe-explain-plan-and-select-interactive", "bibtex": "@misc{wang2023describe,\n      title={Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents}, \n      author={Zihao Wang and Shaofei Cai and Guanzhou Chen and Anji Liu and Xiaojian Ma and Yitao Liang},\n      year={2023},\n      eprint={2302.01560},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "03-02-2023", "categories": ["cs.AI"]}, "2312.08361v1": {"paper_id": "2312.08361v1", "abs_url": "https://arxiv.org/abs/2312.08361v1", "pdf_url": "https://arxiv.org/pdf/2312.08361v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.08361v1_Distributed_Inference_and_Fine-tuning_of_Large_Language_Models_Over_The_Internet.pdf", "title": "Distributed Inference and Fine-tuning of Large Language Models Over The Internet", "year": 2023, "paper_venue": null, "authors": ["Alexander Borzunov", "Max Ryabinin", "Artem Chumachenko", "Dmitry Baranchuk", "Tim Dettmers", "Younes Belkada", "Pavel Samygin", "Colin Raffel"], "abstract": "Large language models (LLMs) are useful in many NLP tasks and become more capable with size, with the best open-source models having over 50 billion parameters. However, using these 50B+ models requires high-end hardware, making them inaccessible to most researchers. In this work, we investigate methods for cost-efficient inference and fine-tuning of LLMs, comparing local and distributed strategies. We observe that a large enough model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network. This could allow running LLM efficiently by pooling together idle compute resources of multiple research groups and volunteers. We address two open problems: (1) how to perform inference and fine-tuning reliably if any device can disconnect abruptly and (2) how to partition LLMs between devices with uneven hardware, joining and leaving at will. In order to do that, we develop special fault-tolerant inference algorithms and load-balancing protocols that automatically assign devices to maximize the total system throughput. We showcase these algorithms in Petals - a decentralized system that runs Llama 2 (70B) and BLOOM (176B) over the Internet up to 10x faster than offloading for interactive generation. We evaluate the performance of our system in simulated conditions and a real-world setup spanning two continents.", "comments": "Accepted to Conference on Neural Information Processing Systems (NeurIPS) 2023. 20 pages, 3 figures", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/distributed-inference-and-fine-tuning-of-1", "bibtex": "@misc{borzunov2023distributed,\n      title={Distributed Inference and Fine-tuning of Large Language Models Over The Internet}, \n      author={Alexander Borzunov and Max Ryabinin and Artem Chumachenko and Dmitry Baranchuk and Tim Dettmers and Younes Belkada and Pavel Samygin and Colin Raffel},\n      year={2023},\n      eprint={2312.08361},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "13-12-2023", "categories": ["cs.LG", "cs.DC"]}, "2401.00908v1": {"paper_id": "2401.00908v1", "abs_url": "https://arxiv.org/abs/2401.00908v1", "pdf_url": "https://arxiv.org/pdf/2401.00908v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.00908v1_DocLLM_A_layout-aware_generative_language_model_for_multimodal_document_understanding.pdf", "title": "DocLLM: A layout-aware generative language model for multimodal document understanding", "year": 2023, "paper_venue": null, "authors": ["Dongsheng Wang", "Natraj Raman", "Mathieu Sibue", "Zhiqiang Ma", "Petr Babkin", "Simerjot Kaur", "Yulong Pei", "Armineh Nourbakhsh", "Xiaomo Liu"], "abstract": "Enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.", "comments": "16 pages, 4 figures", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/docllm-a-layout-aware-generative-language", "bibtex": "@misc{wang2023docllm,\n      title={DocLLM: A layout-aware generative language model for multimodal document understanding}, \n      author={Dongsheng Wang and Natraj Raman and Mathieu Sibue and Zhiqiang Ma and Petr Babkin and Simerjot Kaur and Yulong Pei and Armineh Nourbakhsh and Xiaomo Liu},\n      year={2023},\n      eprint={2401.00908},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "31-12-2023", "categories": ["cs.CL"]}, "2310.19212v1": {"paper_id": "2310.19212v1", "abs_url": "https://arxiv.org/abs/2310.19212v1", "pdf_url": "https://arxiv.org/pdf/2310.19212v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.19212v1_EHRTutor_Enhancing_Patient_Understanding_of_Discharge_Instructions.pdf", "title": "EHRTutor: Enhancing Patient Understanding of Discharge Instructions", "year": 2023, "paper_venue": null, "authors": ["Zihao Zhang", "Zonghai Yao", "Huixue Zhou", "Feiyun ouyang", "Hong Yu"], "abstract": "Large language models have shown success as a tutor in education in various fields. Educating patients about their clinical visits plays a pivotal role in patients' adherence to their treatment plans post-discharge. This paper presents EHRTutor, an innovative multi-component framework leveraging the Large Language Model (LLM) for patient education through conversational question-answering. EHRTutor first formulates questions pertaining to the electronic health record discharge instructions. It then educates the patient through conversation by administering each question as a test. Finally, it generates a summary at the end of the conversation. Evaluation results using LLMs and domain experts have shown a clear preference for EHRTutor over the baseline. Moreover, EHRTutor also offers a framework for generating synthetic patient education dialogues that can be used for future in-house system training.", "comments": "To appear in NeurIPS'23 Workshop on Generative AI for Education (GAIED)", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/ehrtutor-enhancing-patient-understanding-of", "bibtex": "@misc{zhang2023ehrtutor,\n      title={EHRTutor: Enhancing Patient Understanding of Discharge Instructions}, \n      author={Zihao Zhang and Zonghai Yao and Huixue Zhou and Feiyun ouyang and Hong Yu},\n      year={2023},\n      eprint={2310.19212},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "30-10-2023", "categories": ["cs.CL", "cs.AI"]}, "2310.03046v1": {"paper_id": "2310.03046v1", "abs_url": "https://arxiv.org/abs/2310.03046v1", "pdf_url": "https://arxiv.org/pdf/2310.03046v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.03046v1_EcoAssistant_Using_LLM_Assistant_More_Affordably_and_Accurately.pdf", "title": "EcoAssistant: Using LLM Assistant More Affordably and Accurately", "year": 2023, "paper_venue": null, "authors": ["Jieyu Zhang", "Ranjay Krishna", "Ahmed H. Awadallah", "Chi Wang"], "abstract": "Today, users ask Large language models (LLMs) as assistants to answer queries that require external knowledge; they ask about the weather in a specific city, about stock prices, and even about where specific locations are within their neighborhood. These queries require the LLM to produce code that invokes external APIs to answer the user's question, yet LLMs rarely produce correct code on the first try, requiring iterative code refinement upon execution results. In addition, using LLM assistants to support high query volumes can be expensive. In this work, we contribute a framework, EcoAssistant, that enables LLMs to answer code-driven queries more affordably and accurately. EcoAssistant contains three components. First, it allows the LLM assistants to converse with an automatic code executor to iteratively refine code or to produce answers based on the execution results. Second, we use a hierarchy of LLM assistants, which attempts to answer the query with weaker, cheaper LLMs before backing off to stronger, expensive ones. Third, we retrieve solutions from past successful queries as in-context demonstrations to help subsequent queries. Empirically, we show that EcoAssistant offers distinct advantages for affordability and accuracy, surpassing GPT-4 by 10 points of success rate with less than 50% of GPT-4's cost.", "comments": "", "official_code_urls": ["https://github.com/jieyuz2/ecoassistant"], "pwc_page_url": "https://paperswithcode.com/paper/ecoassistant-using-llm-assistant-more", "bibtex": "@misc{zhang2023ecoassistant,\n      title={EcoAssistant: Using LLM Assistant More Affordably and Accurately}, \n      author={Jieyu Zhang and Ranjay Krishna and Ahmed H. Awadallah and Chi Wang},\n      year={2023},\n      eprint={2310.03046},\n      archivePrefix={arXiv},\n      primaryClass={cs.SE}\n}", "published_date": "03-10-2023", "categories": ["cs.SE", "cs.AI"]}, "2303.15772v1": {"paper_id": "2303.15772v1", "abs_url": "https://arxiv.org/abs/2303.15772v1", "pdf_url": "https://arxiv.org/pdf/2303.15772v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2303.15772v1_Ecosystem_Graphs_The_Social_Footprint_of_Foundation_Models.pdf", "title": "Ecosystem Graphs: The Social Footprint of Foundation Models", "year": 2023, "paper_venue": null, "authors": ["Rishi Bommasani", "Dilara Soylu", "Thomas I. Liao", "Kathleen A. Creel", "Percy Liang"], "abstract": ". As of March 16, 2023, we annotate 262 assets (64 datasets, 128 models, 70 applications) from 63 organizations linked by 356 dependencies. We show Ecosystem Graphs functions as a powerful abstraction and interface for achieving the minimum transparency required to address myriad use cases. Therefore, we envision Ecosystem Graphs will be a community-maintained resource that provides value to stakeholders spanning AI researchers, industry professionals, social scientists, auditors and policymakers.", "comments": "Authored by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI). Ecosystem Graphs available at this https URL", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/ecosystem-graphs-the-social-footprint-of", "bibtex": "@misc{bommasani2023ecosystem,\n      title={Ecosystem Graphs: The Social Footprint of Foundation Models}, \n      author={Rishi Bommasani and Dilara Soylu and Thomas I. Liao and Kathleen A. Creel and Percy Liang},\n      year={2023},\n      eprint={2303.15772},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "28-03-2023", "categories": ["cs.LG", "cs.AI", "cs.CY"]}, "2312.07125v1": {"paper_id": "2312.07125v1", "abs_url": "https://arxiv.org/abs/2312.07125v1", "pdf_url": "https://arxiv.org/pdf/2312.07125v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.07125v1_Efficient_Few-Shot_Clinical_Task_Adaptation_with_Large_Language_Models.pdf", "title": "Efficient Few-Shot Clinical Task Adaptation with Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Kaipeng Zheng", "Weiran Huang", "Lichao Sun"], "abstract": "Few-shot learning has been studied to adapt models to tasks with very few samples. It holds profound significance, particularly in clinical tasks, due to the high annotation cost of medical images. Several works have explored few-shot learning on medical images, yet they still require a large number of medical images for pre-training models to gain domain-specific priors. Vision foundation models recently have achieved remarkable success in natural images. Hence, adapting rapidly advancing vision foundation models from natural images to few-shot clinical tasks holds great promise. MedFMC has recently organized a challenge to shed more light on this topic at NeurIPS 2023. In this work, we present our challenge solution. We observe that a simple variant of fine-tuning with partial freezing shows remarkable performance. Empirical evidence demonstrates that this approach could outperform various common fine-tuning methods under limited sample sizes. Additionally, we explore enhanced utilization of semantic supervision to boost performance. We propose a novel approach that contextualizes labels via large language models (LLMs). Our findings reveal that the context generated by LLMs significantly enhances the discrimination of semantic embeddings for similar categories, resulting in a notable performance improvement of 3%-5% in 1-shot settings compared to commonly employed one-hot labels and other semantic supervision methods. Our solution secures the 1st place in the MedFMC challenge.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/efficient-few-shot-clinical-task-adaptation", "bibtex": "@misc{zheng2023efficient,\n      title={Efficient Few-Shot Clinical Task Adaptation with Large Language Models}, \n      author={Kaipeng Zheng and Weiran Huang and Lichao Sun},\n      year={2023},\n      eprint={2312.07125},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "12-12-2023", "categories": ["cs.CV"]}, "2308.01666v1": {"paper_id": "2308.01666v1", "abs_url": "https://arxiv.org/abs/2308.01666v1", "pdf_url": "https://arxiv.org/pdf/2308.01666v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2308.01666v1_Evaluating_ChatGPT_text-mining_of_clinical_records_for_obesity_monitoring.pdf", "title": "Evaluating ChatGPT text-mining of clinical records for obesity monitoring", "year": 2023, "paper_venue": null, "authors": ["Ivo S. Fins", "(1),", "Heather Davies", "(1),", "Sean Farrell", "(2),", "Jose R.Torres", "(3),", "Gina Pinchbeck", "(1),", "Alan D. Radford", "(1),", "Peter-John Noble", "(1) ((1) Small Animal Veterinary Surveillance Network, Institute of Infection, Veterinary and Ecological Sciences, University of Liverpool, Liverpool, UK, (2) Department of Computer Science, Durham University, Durham, UK, (3) Institute for Animal Health and Food Safety, University of Las Palmas de Gran Canaria, Las Palmas, Canary Archipelago, Spain)"], "abstract": "Background: Veterinary clinical narratives remain a largely untapped resource for addressing complex diseases. Here we compare the ability of a large language model (ChatGPT) and a previously developed regular expression (RegexT) to identify overweight body condition scores (BCS) in veterinary narratives. Methods: BCS values were extracted from 4,415 anonymised clinical narratives using either RegexT or by appending the narrative to a prompt sent to ChatGPT coercing the model to return the BCS information. Data were manually reviewed for comparison. Results: The precision of RegexT was higher (100%, 95% CI 94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recall of ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that of RegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering is needed to improve ChatGPT output. Conclusions: Large language models create diverse opportunities and, whilst complex, present an intuitive interface to information but require careful implementation to avoid unpredictable errors.", "comments": "Supplementary Material: The data that support the findings of this study are available in the ancillary files of this submission. 5 pages, 2 figures (textboxes)", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/evaluating-chatgpt-text-mining-of-clinical", "bibtex": "@misc{fins2023evaluating,\n      title={Evaluating ChatGPT text-mining of clinical records for obesity monitoring}, \n      author={Ivo S. Fins and Heather Davies and Sean Farrell and Jose R. Torres and Gina Pinchbeck and Alan D. Radford and Peter-John Noble},\n      year={2023},\n      eprint={2308.01666},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}", "published_date": "03-08-2023", "categories": ["cs.IR", "cs.CL"]}, "2310.19736v3": {"paper_id": "2310.19736v3", "abs_url": "https://arxiv.org/abs/2310.19736v3", "pdf_url": "https://arxiv.org/pdf/2310.19736v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.19736v3_Evaluating_Large_Language_Models_A_Comprehensive_Survey.pdf", "title": "Evaluating Large Language Models: A Comprehensive Survey", "year": 2023, "paper_venue": null, "authors": ["Zishan Guo", "Renren Jin", "Chuang Liu", "Yufei Huang", "Dan Shi", "Supryadi", "Linhao Yu", "Yan Liu", "Jiaxuan Li", "Bojian Xiong", "Deyi Xiong"], "abstract": ".", "comments": "111 pages", "official_code_urls": ["https://github.com/tjunlp-lab/awesome-llms-evaluation-papers"], "pwc_page_url": "https://paperswithcode.com/paper/evaluating-large-language-models-a", "bibtex": "@misc{guo2023evaluating,\n      title={Evaluating Large Language Models: A Comprehensive Survey}, \n      author={Zishan Guo and Renren Jin and Chuang Liu and Yufei Huang and Dan Shi and Supryadi and Linhao Yu and Yan Liu and Jiaxuan Li and Bojian Xiong and Deyi Xiong},\n      year={2023},\n      eprint={2310.19736},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "30-10-2023", "categories": ["cs.CL", "cs.AI"]}, "2312.17025v2": {"paper_id": "2312.17025v2", "abs_url": "https://arxiv.org/abs/2312.17025v2", "pdf_url": "https://arxiv.org/pdf/2312.17025v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.17025v2_Experiential_Co-Learning_of_Software-Developing_Agents.pdf", "title": "Experiential Co-Learning of Software-Developing Agents", "year": 2023, "paper_venue": null, "authors": ["Chen Qian", "Yufan Dang", "Jiahao Li", "Wei Liu", "Weize Chen", "Cheng Yang", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. These agents are now capable of collaborating seamlessly, splitting tasks and enhancing accuracy, thus minimizing the need for human involvement. However, these agents often approach a diverse range of tasks in isolation, without benefiting from past experiences. This isolation can lead to repeated mistakes and inefficient trials in task solving. To this end, this paper introduces Experiential Co-Learning, a novel framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for mutual reasoning. This paradigm, enriched with previous experiences, equips agents to more effectively address unseen tasks.", "comments": "Work in progress", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/experiential-co-learning-of-software", "bibtex": "@misc{qian2023experiential,\n      title={Experiential Co-Learning of Software-Developing Agents}, \n      author={Chen Qian and Yufan Dang and Jiahao Li and Wei Liu and Weize Chen and Cheng Yang and Zhiyuan Liu and Maosong Sun},\n      year={2023},\n      eprint={2312.17025},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "28-12-2023", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"]}, "2305.14688v1": {"paper_id": "2305.14688v1", "abs_url": "https://arxiv.org/abs/2305.14688v1", "pdf_url": "https://arxiv.org/pdf/2305.14688v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.14688v1_ExpertPrompting_Instructing_Large_Language_Models_to_be_Distinguished_Experts.pdf", "title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts", "year": 2023, "paper_venue": null, "authors": ["Benfeng Xu", "An Yang", "Junyang Lin", "Quan Wang", "Chang Zhou", "Yongdong Zhang", "Zhendong Mao"], "abstract": "}.", "comments": "", "official_code_urls": ["https://github.com/ofa-sys/expertllama"], "pwc_page_url": "https://paperswithcode.com/paper/expertprompting-instructing-large-language", "bibtex": "@misc{xu2023expertprompting,\n      title={ExpertPrompting: Instructing Large Language Models to be Distinguished Experts}, \n      author={Benfeng Xu and An Yang and Junyang Lin and Quan Wang and Chang Zhou and Yongdong Zhang and Zhendong Mao},\n      year={2023},\n      eprint={2305.14688},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "24-05-2023", "categories": ["cs.CL", "cs.AI"]}, "2305.04118v3": {"paper_id": "2305.04118v3", "abs_url": "https://arxiv.org/abs/2305.04118v3", "pdf_url": "https://arxiv.org/pdf/2305.04118v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.04118v3_Exploring_Human-Like_Translation_Strategy_with_Large_Language_Models.pdf", "title": "Exploring Human-Like Translation Strategy with Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Zhiwei He", "Tian Liang", "Wenxiang Jiao", "Zhuosheng Zhang", "Yujiu Yang", "Rui Wang", "Zhaopeng Tu", "Shuming Shi", "Xing Wang"], "abstract": ".", "comments": "To be published in TACL (pre-MIT Press publication version)", "official_code_urls": ["https://github.com/zwhe99/MAPS-mt"], "pwc_page_url": "https://paperswithcode.com/paper/exploring-human-like-translation-strategy", "bibtex": "@misc{he2023exploring,\n      title={Exploring Human-Like Translation Strategy with Large Language Models}, \n      author={Zhiwei He and Tian Liang and Wenxiang Jiao and Zhuosheng Zhang and Yujiu Yang and Rui Wang and Zhaopeng Tu and Shuming Shi and Xing Wang},\n      year={2023},\n      eprint={2305.04118},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "06-05-2023", "categories": ["cs.CL"]}, "2310.07225v1": {"paper_id": "2310.07225v1", "abs_url": "https://arxiv.org/abs/2310.07225v1", "pdf_url": "https://arxiv.org/pdf/2310.07225v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.07225v1_Exploring_the_Landscape_of_Large_Language_Models_In_Medical_Question_Answering_Observations_and_Open_Questions.pdf", "title": "Exploring the Landscape of Large Language Models In Medical Question Answering: Observations and Open Questions", "year": 2023, "paper_venue": null, "authors": ["Karolina Korgul", "Andrew M. Bean", "Felix Krones", "Robert McCraith", "Adam Mahdi"], "abstract": "Large Language Models (LLMs) have shown promise in medical question answering by achieving passing scores in standardised exams and have been suggested as tools for supporting healthcare workers. Deploying LLMs into such a high-risk context requires a clear understanding of the limitations of these models. With the rapid development and release of new LLMs, it is especially valuable to identify patterns which exist across models and may, therefore, continue to appear in newer versions. In this paper, we evaluate a wide range of popular LLMs on their knowledge of medical questions in order to better understand their properties as a group. From this comparison, we provide preliminary observations and raise open questions for further research.", "comments": "11 pages, 8 figures", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/exploring-the-landscape-of-large-language", "bibtex": "@misc{korgul2023exploring,\n      title={Exploring the Landscape of Large Language Models In Medical Question Answering: Observations and Open Questions}, \n      author={Karolina Korgul and Andrew M. Bean and Felix Krones and Robert McCraith and Adam Mahdi},\n      year={2023},\n      eprint={2310.07225},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "11-10-2023", "categories": ["cs.CL"]}, "2312.14262v1": {"paper_id": "2312.14262v1", "abs_url": "https://arxiv.org/abs/2312.14262v1", "pdf_url": "https://arxiv.org/pdf/2312.14262v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.14262v1_Exploring_the_intersection_of_Generative_AI_and_Software_Development.pdf", "title": "Exploring the intersection of Generative AI and Software Development", "year": 2023, "paper_venue": null, "authors": ["Filipe Calegario", "Vanilson Bur\u00e9gio", "Francisco Erivaldo", "Daniel Moraes Costa Andrade", "Kailane Felix", "Nathalia Barbosa", "Pedro Lucas da Silva Lucena", "C\u00e9sar Fran\u00e7a"], "abstract": "In the ever-evolving landscape of Artificial Intelligence (AI), the synergy between generative AI and Software Engineering emerges as a transformative frontier. This whitepaper delves into the unexplored realm, elucidating how generative AI techniques can revolutionize software development. Spanning from project management to support and updates, we meticulously map the demands of each development stage and unveil the potential of generative AI in addressing them. Techniques such as zero-shot prompting, self-consistency, and multimodal chain-of-thought are explored, showcasing their unique capabilities in enhancing generative AI models. The significance of vector embeddings, context, plugins, tools, and code assistants is underscored, emphasizing their role in capturing semantic information and amplifying generative AI capabilities. Looking ahead, this intersection promises to elevate productivity, improve code quality, and streamline the software development process. This whitepaper serves as a guide for stakeholders, urging discussions and experiments in the application of generative AI in Software Engineering, fostering innovation and collaboration for a qualitative leap in the efficiency and effectiveness of software development.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/exploring-the-intersection-of-generative-ai", "bibtex": "@misc{calegario2023exploring,\n      title={Exploring the intersection of Generative AI and Software Development}, \n      author={Filipe Calegario and Vanilson Bur\u00e9gio and Francisco Erivaldo and Daniel Moraes Costa Andrade and Kailane Felix and Nathalia Barbosa and Pedro Lucas da Silva Lucena and C\u00e9sar Fran\u00e7a},\n      year={2023},\n      eprint={2312.14262},\n      archivePrefix={arXiv},\n      primaryClass={cs.SE}\n}", "published_date": "21-12-2023", "categories": ["cs.SE", "cs.AI"]}, "2312.09571v1": {"paper_id": "2312.09571v1", "abs_url": "https://arxiv.org/abs/2312.09571v1", "pdf_url": "https://arxiv.org/pdf/2312.09571v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.09571v1_Extending_Context_Window_of_Large_Language_Models_via_Semantic_Compression.pdf", "title": "Extending Context Window of Large Language Models via Semantic Compression", "year": 2023, "paper_venue": null, "authors": ["Weizhi Fei", "Xueyan Niu", "Pingyi Zhou", "Lu Hou", "Bo Bai", "Lei Deng", "Wei Han"], "abstract": "Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts. We propose a novel semantic compression method that enables generalization to texts that are 6-8 times longer, without incurring significant computational costs or requiring fine-tuning. Our proposed framework draws inspiration from source coding in information theory and employs a pre-trained model to reduce the semantic redundancy of long inputs before passing them to the LLMs for downstream tasks. Experimental results demonstrate that our method effectively extends the context window of LLMs across a range of tasks including question answering, summarization, few-shot learning, and information retrieval. Furthermore, the proposed semantic compression method exhibits consistent fluency in text generation while reducing the associated computational overhead.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/extending-context-window-of-large-language-1", "bibtex": "@misc{fei2023extending,\n      title={Extending Context Window of Large Language Models via Semantic Compression}, \n      author={Weizhi Fei and Xueyan Niu and Pingyi Zhou and Lu Hou and Bo Bai and Lei Deng and Wei Han},\n      year={2023},\n      eprint={2312.09571},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "15-12-2023", "categories": ["cs.CL", "cs.IT", "math.IT"]}, "2301.13379v3": {"paper_id": "2301.13379v3", "abs_url": "https://arxiv.org/abs/2301.13379v3", "pdf_url": "https://arxiv.org/pdf/2301.13379v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2301.13379v3_Faithful_Chain-of-Thought_Reasoning.pdf", "title": "Faithful Chain-of-Thought Reasoning", "year": 2023, "paper_venue": null, "authors": ["Qing Lyu", "Shreya Havaldar", "Adam Stein", "Li Zhang", "Delip Rao", "Eric Wong", "Marianna Apidianaki", "Chris Callison-Burch"], "abstract": "While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.", "comments": "IJCNLP-AACL 2023 camera-ready version", "official_code_urls": ["https://github.com/veronica320/faithful-cot"], "pwc_page_url": "https://paperswithcode.com/paper/faithful-chain-of-thought-reasoning", "bibtex": "@misc{lyu2023faithful,\n      title={Faithful Chain-of-Thought Reasoning}, \n      author={Qing Lyu and Shreya Havaldar and Adam Stein and Li Zhang and Delip Rao and Eric Wong and Marianna Apidianaki and Chris Callison-Burch},\n      year={2023},\n      eprint={2301.13379},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "31-01-2023", "categories": ["cs.CL"]}, "2312.10007v1": {"paper_id": "2312.10007v1", "abs_url": "https://arxiv.org/abs/2312.10007v1", "pdf_url": "https://arxiv.org/pdf/2312.10007v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.10007v1_Faithful_Persona-based_Conversational_Dataset_Generation_with_Large_Language_Models.pdf", "title": "Faithful Persona-based Conversational Dataset Generation with Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Pegah Jandaghi", "XiangHai Sheng", "Xinyi Bai", "Jay Pujara", "Hakim Sidahmed"], "abstract": "High-quality conversational datasets are essential for developing AI models that can communicate with users. One way to foster deeper interactions between a chatbot and its user is through personas, aspects of the user's character that provide insights into their personality, motivations, and behaviors. Training Natural Language Processing (NLP) models on a diverse and comprehensive persona-based dataset can lead to conversational models that create a deeper connection with the user, and maintain their engagement. In this paper, we leverage the power of Large Language Models (LLMs) to create a large, high-quality conversational dataset from a seed dataset. We propose a Generator-Critic architecture framework to expand the initial dataset, while improving the quality of its conversations. The Generator is an LLM prompted to output conversations. The Critic consists of a mixture of expert LLMs that control the quality of the generated conversations. These experts select the best generated conversations, which we then use to improve the Generator. We release Synthetic-Persona-Chat, consisting of 20k conversations seeded from Persona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our generation framework on different dimensions through extensive experiments, and observe that the losing rate of Synthetic-Persona-Chat against Persona-Chat during Turing test decreases from 17.2% to 8.8% over three iterations.", "comments": "", "official_code_urls": ["https://github.com/google-research-datasets/Synthetic-Persona-Chat"], "pwc_page_url": "https://paperswithcode.com/paper/faithful-persona-based-conversational-dataset", "bibtex": "@misc{jandaghi2023faithful,\n      title={Faithful Persona-based Conversational Dataset Generation with Large Language Models}, \n      author={Pegah Jandaghi and XiangHai Sheng and Xinyi Bai and Jay Pujara and Hakim Sidahmed},\n      year={2023},\n      eprint={2312.10007},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "15-12-2023", "categories": ["cs.CL", "cs.LG"]}, "2311.08263v1": {"paper_id": "2311.08263v1", "abs_url": "https://arxiv.org/abs/2311.08263v1", "pdf_url": "https://arxiv.org/pdf/2311.08263v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.08263v1_Fast_Chain-of-Thought_A_Glance_of_Future_from_Parallel_Decoding_Leads_to_Answers_Faster.pdf", "title": "Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers Faster", "year": 2023, "paper_venue": null, "authors": ["Hongxuan Zhang", "Zhining Liu", "Jiaqi Zheng", "Chenyi Zhuang", "Jinjie Gu", "Guihai Chen"], "abstract": "In this work, we propose FastCoT, a model-agnostic framework based on parallel decoding without any further training of an auxiliary model or modification to the LLM itself. FastCoT uses a size-varying context window whose size changes with position to conduct parallel decoding and auto-regressive decoding simultaneously, thus fully utilizing GPU computation resources. In FastCoT, the parallel decoding part provides the LLM with a quick glance of the future composed of approximate tokens, which could lead to faster answers compared to regular autoregressive decoding used by causal transformers. We also provide an implementation of parallel decoding within LLM, which supports KV-cache generation and batch processing. Through extensive experiments, we demonstrate that FastCoT saves inference time by nearly 20% with only a negligible performance drop compared to the regular approach. Additionally, we show that the context window size exhibits considerable robustness for different tasks.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/fast-chain-of-thought-a-glance-of-future-from", "bibtex": "@misc{zhang2023fast,\n      title={Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers Faster}, \n      author={Hongxuan Zhang and Zhining Liu and Jiaqi Zheng and Chenyi Zhuang and Jinjie Gu and Guihai Chen},\n      year={2023},\n      eprint={2311.08263},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "14-11-2023", "categories": ["cs.CL"]}, "2303.04129v1": {"paper_id": "2303.04129v1", "abs_url": "https://arxiv.org/abs/2303.04129v1", "pdf_url": "https://arxiv.org/pdf/2303.04129v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2303.04129v1_Foundation_Models_for_Decision_Making_Problems_Methods_and_Opportunities.pdf", "title": "Foundation Models for Decision Making: Problems, Methods, and Opportunities", "year": 2023, "paper_venue": null, "authors": ["Sherry Yang", "Ofir Nachum", "Yilun Du", "Jason Wei", "Pieter Abbeel", "Dale Schuurmans"], "abstract": "Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/foundation-models-for-decision-making", "bibtex": "@misc{yang2023foundation,\n      title={Foundation Models for Decision Making: Problems, Methods, and Opportunities}, \n      author={Sherry Yang and Ofir Nachum and Yilun Du and Jason Wei and Pieter Abbeel and Dale Schuurmans},\n      year={2023},\n      eprint={2303.04129},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "07-03-2023", "categories": ["cs.AI", "cs.LG"]}, "2312.03014v1": {"paper_id": "2312.03014v1", "abs_url": "https://arxiv.org/abs/2312.03014v1", "pdf_url": "https://arxiv.org/pdf/2312.03014v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.03014v1_Foundation_Models_for_Weather_and_Climate_Data_Understanding_A_Comprehensive_Survey.pdf", "title": "Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey", "year": 2023, "paper_venue": null, "authors": ["Shengchao Chen", "Guodong Long", "Jing Jiang", "Dikai Liu", "Chengqi Zhang"], "abstract": "As artificial intelligence (AI) continues to rapidly evolve, the realm of Earth and atmospheric sciences is increasingly adopting data-driven models, powered by progressive developments in deep learning (DL). Specifically, DL techniques are extensively utilized to decode the chaotic and nonlinear aspects of Earth systems, and to address climate challenges via understanding weather and climate data. Cutting-edge performance on specific tasks within narrower spatio-temporal scales has been achieved recently through DL. The rise of large models, specifically large language models (LLMs), has enabled fine-tuning processes that yield remarkable outcomes across various downstream tasks, thereby propelling the advancement of general AI. However, we are still navigating the initial stages of crafting general AI for weather and climate. In this survey, we offer an exhaustive, timely overview of state-of-the-art AI methodologies specifically engineered for weather and climate data, with a special focus on time series and text data. Our primary coverage encompasses four critical aspects: types of weather and climate data, principal model architectures, model scopes and applications, and datasets for weather and climate. Furthermore, in relation to the creation and application of foundation models for weather and climate data understanding, we delve into the field's prevailing challenges, offer crucial insights, and propose detailed avenues for future research. This comprehensive approach equips practitioners with the requisite knowledge to make substantial progress in this domain. Our survey encapsulates the most recent breakthroughs in research on large, data-driven models for weather and climate data understanding, emphasizing robust foundations, current advancements, practical applications, crucial resources, and prospective research opportunities.", "comments": "Ongoing work. Survey Paper. 35 pages, 2 figures, 4 tables. The first work to comprehensively and systematically summarize DL-based weather and climate data understanding, paving the way for the development of weather and climate foundation models", "official_code_urls": ["https://github.com/shengchaochen82/awesome-large-models-for-weather-and-climate"], "pwc_page_url": "https://paperswithcode.com/paper/foundation-models-for-weather-and-climate", "bibtex": "@misc{chen2023foundation,\n      title={Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey}, \n      author={Shengchao Chen and Guodong Long and Jing Jiang and Dikai Liu and Chengqi Zhang},\n      year={2023},\n      eprint={2312.03014},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "05-12-2023", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.ao-ph"]}, "2310.03214v2": {"paper_id": "2310.03214v2", "abs_url": "https://arxiv.org/abs/2310.03214v2", "pdf_url": "https://arxiv.org/pdf/2310.03214v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.03214v2_FreshLLMs_Refreshing_Large_Language_Models_with_Search_Engine_Augmentation.pdf", "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation", "year": 2023, "paper_venue": null, "authors": ["Tu Vu", "Mohit Iyyer", "Xuezhi Wang", "Noah Constant", "Jerry Wei", "Jason Wei", "Chris Tar", "Yun-Hsuan Sung", "Denny Zhou", "Quoc Le", "Thang Luong"], "abstract": "and commit to updating it at regular intervals.", "comments": "Preprint, 26 pages, 10 figures, 5 tables; Added FreshEval", "official_code_urls": ["https://github.com/freshllms/freshqa"], "pwc_page_url": "https://paperswithcode.com/paper/freshllms-refreshing-large-language-models", "bibtex": "@misc{vu2023freshllms,\n      title={FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation}, \n      author={Tu Vu and Mohit Iyyer and Xuezhi Wang and Noah Constant and Jerry Wei and Jason Wei and Chris Tar and Yun-Hsuan Sung and Denny Zhou and Quoc Le and Thang Luong},\n      year={2023},\n      eprint={2310.03214},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "05-10-2023", "categories": ["cs.CL"]}, "2311.13063v2": {"paper_id": "2311.13063v2", "abs_url": "https://arxiv.org/abs/2311.13063v2", "pdf_url": "https://arxiv.org/pdf/2311.13063v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.13063v2_From_Classification_to_Clinical_Insights_Towards_Analyzing_and_Reasoning_About_Mobile_and_Behavioral_Health_Data_With_Large_Language_Models.pdf", "title": "From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Zachary Englhardt", "Chengqian Ma", "Margaret E. Morris", "Xuhai \"Orson\" Xu", "Chun-Cheng Chang", "Lianhui Qin", "Daniel McDuff", "Xin Liu", "Shwetak Patel", "Vikram Iyer"], "abstract": "Passively collected behavioral health data from ubiquitous sensors holds significant promise to provide mental health professionals insights from patient's daily lives; however, developing analysis tools to use this data in clinical practice requires addressing challenges of generalization across devices and weak or ambiguous correlations between the measured signals and an individual's mental health. To address these challenges, we take a novel approach that leverages large language models (LLMs) to synthesize clinically useful insights from multi-sensor data. We develop chain of thought prompting methods that use LLMs to generate reasoning about how trends in data such as step count and sleep relate to conditions like depression and anxiety. We first demonstrate binary depression classification with LLMs achieving accuracies of 61.1% which exceed the state of the art. While it is not robust for clinical use, this leads us to our key finding: even more impactful and valued than classification is a new human-AI collaboration approach in which clinician experts interactively query these tools and combine their domain expertise and context about the patient with AI generated reasoning to support clinical decision-making. We find models like GPT-4 correctly reference numerical data 75% of the time, and clinician participants express strong interest in using this approach to interpret self-tracking data.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/from-classification-to-clinical-insights", "bibtex": "@misc{englhardt2023classification,\n      title={From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models}, \n      author={Zachary Englhardt and Chengqian Ma and Margaret E. Morris and Xuhai \"Orson\" Xu and Chun-Cheng Chang and Lianhui Qin and Daniel McDuff and Xin Liu and Shwetak Patel and Vikram Iyer},\n      year={2023},\n      eprint={2311.13063},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "21-11-2023", "categories": ["cs.AI"]}, "2309.04269v1": {"paper_id": "2309.04269v1", "abs_url": "https://arxiv.org/abs/2309.04269v1", "pdf_url": "https://arxiv.org/pdf/2309.04269v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.04269v1_From_Sparse_to_Dense_GPT-4_Summarization_with_Chain_of_Density_Prompting.pdf", "title": "From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting", "year": 2023, "paper_venue": null, "authors": ["Griffin Adams", "Alexander Fabbri", "Faisal Ladhak", "Eric Lehman", "No\u00e9mie Elhadad"], "abstract": ").", "comments": "preprint", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/from-sparse-to-dense-gpt-4-summarization-with", "bibtex": "@misc{adams2023sparse,\n      title={From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting}, \n      author={Griffin Adams and Alexander Fabbri and Faisal Ladhak and Eric Lehman and No\u00e9mie Elhadad},\n      year={2023},\n      eprint={2309.04269},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "08-09-2023", "categories": ["cs.CL"]}, "2311.10691v1": {"paper_id": "2311.10691v1", "abs_url": "https://arxiv.org/abs/2311.10691v1", "pdf_url": "https://arxiv.org/pdf/2311.10691v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.10691v1_Generalized_products_and_Lorentzian_length_spaces.pdf", "title": "Generalized products and Lorentzian length spaces", "year": 2023, "paper_venue": null, "authors": ["Elefterios Soultanis"], "abstract": "The generalized Lorentzian product naturally has a Lorentzian length structure but can fail the push-up condition in general. We recover the push-up property under a log-Lipschitz condition on the time variable and establish sufficient conditions for global hyperbolicity. Moreover we formulate time-like Ricci curvature bounds without push-up and regularity assumptions, and obtain a partial rigidity of the splitting under a strong energy condition.", "comments": "31 pages. Comments are welcome!", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/generalized-products-and-lorentzian-length", "bibtex": "@misc{soultanis2023generalized,\n      title={Generalized products and Lorentzian length spaces}, \n      author={Elefterios Soultanis},\n      year={2023},\n      eprint={2311.10691},\n      archivePrefix={arXiv},\n      primaryClass={math.DG}\n}", "published_date": "17-11-2023", "categories": ["math.DG", "math-ph", "math.MG", "math.MP"]}, "2304.03442v2": {"paper_id": "2304.03442v2", "abs_url": "https://arxiv.org/abs/2304.03442v2", "pdf_url": "https://arxiv.org/pdf/2304.03442v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2304.03442v2_Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf", "title": "Generative Agents: Interactive Simulacra of Human Behavior", "year": 2023, "paper_venue": null, "authors": ["Joon Sung Park", "Joseph C. O'Brien", "Carrie J. Cai", "Meredith Ringel Morris", "Percy Liang", "Michael S. Bernstein"], "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/generative-agents-interactive-simulacra-of", "bibtex": "@misc{park2023generative,\n      title={Generative Agents: Interactive Simulacra of Human Behavior}, \n      author={Joon Sung Park and Joseph C. O'Brien and Carrie J. Cai and Meredith Ringel Morris and Percy Liang and Michael S. Bernstein},\n      year={2023},\n      eprint={2304.03442},\n      archivePrefix={arXiv},\n      primaryClass={cs.HC}\n}", "published_date": "07-04-2023", "categories": ["cs.HC", "cs.AI", "cs.LG"]}, "2312.13286v1": {"paper_id": "2312.13286v1", "abs_url": "https://arxiv.org/abs/2312.13286v1", "pdf_url": "https://arxiv.org/pdf/2312.13286v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.13286v1_Generative_Multimodal_Models_are_In-Context_Learners.pdf", "title": "Generative Multimodal Models are In-Context Learners", "year": 2023, "paper_venue": null, "authors": ["Quan Sun", "Yufeng Cui", "Xiaosong Zhang", "Fan Zhang", "Qiying Yu", "Zhengxiong Luo", "Yueze Wang", "Yongming Rao", "Jingjing Liu", "Tiejun Huang", "Xinlong Wang"], "abstract": "The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research.", "comments": "Project page: this https URL", "official_code_urls": ["https://github.com/baaivision/emu"], "pwc_page_url": "https://paperswithcode.com/paper/generative-multimodal-models-are-in-context", "bibtex": "@misc{sun2023generative,\n      title={Generative Multimodal Models are In-Context Learners}, \n      author={Quan Sun and Yufeng Cui and Xiaosong Zhang and Fan Zhang and Qiying Yu and Zhengxiong Luo and Yueze Wang and Yongming Rao and Jingjing Liu and Tiejun Huang and Xinlong Wang},\n      year={2023},\n      eprint={2312.13286},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "20-12-2023", "categories": ["cs.CV"]}, "2302.08043v3": {"paper_id": "2302.08043v3", "abs_url": "https://arxiv.org/abs/2302.08043v3", "pdf_url": "https://arxiv.org/pdf/2302.08043v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2302.08043v3_GraphPrompt_Unifying_Pre-Training_and_Downstream_Tasks_for_Graph_Neural_Networks.pdf", "title": "GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks", "year": 2023, "paper_venue": null, "authors": ["Zemin Liu", "Xingtong Yu", "Yuan Fang", "Xinming Zhang"], "abstract": "Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks(GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily rely on a large amount of task-specific supervision. To reduce labeling requirement, the \"pre-train, fine-tune\" and \"pre-train, prompt\" paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-train model in a task-specific manner. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt.", "comments": "WWW23 research track", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/graphprompt-unifying-pre-training-and", "bibtex": "@misc{liu2023graphprompt,\n      title={GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks}, \n      author={Zemin Liu and Xingtong Yu and Yuan Fang and Xinming Zhang},\n      year={2023},\n      eprint={2302.08043},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "16-02-2023", "categories": ["cs.LG", "cs.CL"]}, "2302.11520v4": {"paper_id": "2302.11520v4", "abs_url": "https://arxiv.org/abs/2302.11520v4", "pdf_url": "https://arxiv.org/pdf/2302.11520v4.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2302.11520v4_Guiding_Large_Language_Models_via_Directional_Stimulus_Prompting.pdf", "title": "Guiding Large Language Models via Directional Stimulus Prompting", "year": 2023, "paper_venue": null, "authors": ["Zekun Li", "Baolin Peng", "Pengcheng He", "Michel Galley", "Jianfeng Gao", "Xifeng Yan"], "abstract": "}.", "comments": "Accepted by NeurIPS2023. The code and data are available at this https URL", "official_code_urls": [], "pwc_page_url": "", "bibtex": "@misc{li2023guiding,\n      title={Guiding Large Language Models via Directional Stimulus Prompting}, \n      author={Zekun Li and Baolin Peng and Pengcheng He and Michel Galley and Jianfeng Gao and Xifeng Yan},\n      year={2023},\n      eprint={2302.11520},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "22-02-2023", "categories": ["cs.CL"]}, "2302.06692v2": {"paper_id": "2302.06692v2", "abs_url": "https://arxiv.org/abs/2302.06692v2", "pdf_url": "https://arxiv.org/pdf/2302.06692v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2302.06692v2_Guiding_Pretraining_in_Reinforcement_Learning_with_Large_Language_Models.pdf", "title": "Guiding Pretraining in Reinforcement Learning with Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Yuqing Du", "Olivia Watkins", "Zihan Wang", "C\u00e9dric Colas", "Trevor Darrell", "Pieter Abbeel", "Abhishek Gupta", "Jacob Andreas"], "abstract": ".", "comments": "ICML 2023", "official_code_urls": ["https://github.com/yuqingd/ellm"], "pwc_page_url": "https://paperswithcode.com/paper/guiding-pretraining-in-reinforcement-learning", "bibtex": "@misc{du2023guiding,\n      title={Guiding Pretraining in Reinforcement Learning with Large Language Models}, \n      author={Yuqing Du and Olivia Watkins and Zihan Wang and C\u00e9dric Colas and Trevor Darrell and Pieter Abbeel and Abhishek Gupta and Jacob Andreas},\n      year={2023},\n      eprint={2302.06692},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "13-02-2023", "categories": ["cs.LG", "cs.AI", "cs.CL"]}, "2304.13712v2": {"paper_id": "2304.13712v2", "abs_url": "https://arxiv.org/abs/2304.13712v2", "pdf_url": "https://arxiv.org/pdf/2304.13712v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2304.13712v2_Harnessing_the_Power_of_LLMs_in_Practice_A_Survey_on_ChatGPT_and_Beyond.pdf", "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond", "year": 2023, "paper_venue": null, "authors": ["Jingfeng Yang", "Hongye Jin", "Ruixiang Tang", "Xiaotian Han", "Qizhang Feng", "Haoming Jiang", "Bing Yin", "Xia Hu"], "abstract": "}.", "comments": "", "official_code_urls": ["https://github.com/mooler0410/llmspracticalguide"], "pwc_page_url": "https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a", "bibtex": "@misc{yang2023harnessing,\n      title={Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond}, \n      author={Jingfeng Yang and Hongye Jin and Ruixiang Tang and Xiaotian Han and Qizhang Feng and Haoming Jiang and Bing Yin and Xia Hu},\n      year={2023},\n      eprint={2304.13712},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "26-04-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2310.03051v1": {"paper_id": "2310.03051v1", "abs_url": "https://arxiv.org/abs/2310.03051v1", "pdf_url": "https://arxiv.org/pdf/2310.03051v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.03051v1_How_FaR_Are_Large_Language_Models_From_Agents_with_Theory-of-Mind?.pdf", "title": "How FaR Are Large Language Models From Agents with Theory-of-Mind?", "year": 2023, "paper_venue": null, "authors": ["Pei Zhou", "Aman Madaan", "Srividya Pranavi Potharaju", "Aditya Gupta", "Kevin R. McKee", "Ari Holtzman", "Jay Pujara", "Xiang Ren", "Swaroop Mishra", "Aida Nematzadeh", "Shyam Upadhyay", "Manaal Faruqui"], "abstract": "\"Thinking is for Doing.\" Humans can infer other people's mental states from observations--an ability called Theory-of-Mind (ToM)--and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as ToMi ask models questions to make inferences about beliefs of characters in a story, but do not test whether models can then use these inferences to guide their actions. We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action. Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions. FaR boosts GPT-4's performance from 50% to 71% on T4D, outperforming other prompting methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures and scenarios that also require ToM inferences to choose an action, consistently outperforming other methods including few-shot in-context learning.", "comments": "Preprint, 18 pages, 6 figures, 6 tables", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/how-far-are-large-language-models-from-agents", "bibtex": "@misc{zhou2023far,\n      title={How FaR Are Large Language Models From Agents with Theory-of-Mind?}, \n      author={Pei Zhou and Aman Madaan and Srividya Pranavi Potharaju and Aditya Gupta and Kevin R. McKee and Ari Holtzman and Jay Pujara and Xiang Ren and Swaroop Mishra and Aida Nematzadeh and Shyam Upadhyay and Manaal Faruqui},\n      year={2023},\n      eprint={2310.03051},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "04-10-2023", "categories": ["cs.CL", "cs.AI"]}, "2303.17580v4": {"paper_id": "2303.17580v4", "abs_url": "https://arxiv.org/abs/2303.17580v4", "pdf_url": "https://arxiv.org/pdf/2303.17580v4.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2303.17580v4_HuggingGPT_Solving_AI_Tasks_with_ChatGPT_and_its_Friends_in_Hugging_Face.pdf", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face", "year": 2023, "paper_venue": null, "authors": ["Yongliang Shen", "Kaitao Song", "Xu Tan", "Dongsheng Li", "Weiming Lu", "Yueting Zhuang"], "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.", "comments": "", "official_code_urls": ["https://github.com/microsoft/JARVIS"], "pwc_page_url": "https://paperswithcode.com/paper/hugginggpt-solving-ai-tasks-with-chatgpt-and", "bibtex": "@misc{shen2023hugginggpt,\n      title={HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face}, \n      author={Yongliang Shen and Kaitao Song and Xu Tan and Dongsheng Li and Weiming Lu and Yueting Zhuang},\n      year={2023},\n      eprint={2303.17580},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "30-03-2023", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"]}, "2311.11797v1": {"paper_id": "2311.11797v1", "abs_url": "https://arxiv.org/abs/2311.11797v1", "pdf_url": "https://arxiv.org/pdf/2311.11797v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.11797v1_Igniting_Language_Intelligence_The_Hitchhikers_Guide_From_Chain-of-Thought_Reasoning_to_Language_Agents.pdf", "title": "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents", "year": 2023, "paper_venue": null, "authors": ["Zhuosheng Zhang", "Yao Yao", "Aston Zhang", "Xiangru Tang", "Xinbei Ma", "Zhiwei He", "Yiming Wang", "Mark Gerstein", "Rui Wang", "Gongshen Liu", "Hai Zhao"], "abstract": ".", "comments": "", "official_code_urls": ["https://github.com/zoeyyao27/cot-igniting-agent"], "pwc_page_url": "https://paperswithcode.com/paper/igniting-language-intelligence-the-hitchhiker", "bibtex": "@misc{zhang2023igniting,\n      title={Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents}, \n      author={Zhuosheng Zhang and Yao Yao and Aston Zhang and Xiangru Tang and Xinbei Ma and Zhiwei He and Yiming Wang and Mark Gerstein and Rui Wang and Gongshen Liu and Hai Zhao},\n      year={2023},\n      eprint={2311.11797},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "20-11-2023", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.MA"]}, "2401.00368v1": {"paper_id": "2401.00368v1", "abs_url": "https://arxiv.org/abs/2401.00368v1", "pdf_url": "https://arxiv.org/pdf/2401.00368v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.00368v1_Improving_Text_Embeddings_with_Large_Language_Models.pdf", "title": "Improving Text Embeddings with Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Liang Wang", "Nan Yang", "Xiaolong Huang", "Linjun Yang", "Rangan Majumder", "Furu Wei"], "abstract": "In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across nearly 100 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.", "comments": "15 pages, 8 tables", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/improving-text-embeddings-with-large-language", "bibtex": "@misc{wang2023improving,\n      title={Improving Text Embeddings with Large Language Models}, \n      author={Liang Wang and Nan Yang and Xiaolong Huang and Linjun Yang and Rangan Majumder and Furu Wei},\n      year={2023},\n      eprint={2401.00368},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "31-12-2023", "categories": ["cs.CL", "cs.IR"]}, "2309.09444v1": {"paper_id": "2309.09444v1", "abs_url": "https://arxiv.org/abs/2309.09444v1", "pdf_url": "https://arxiv.org/pdf/2309.09444v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.09444v1_Investigating_Zero-_and_Few-shot_Generalization_in_Fact_Verification.pdf", "title": "Investigating Zero- and Few-shot Generalization in Fact Verification", "year": 2023, "paper_venue": null, "authors": ["Liangming Pan", "Yunxiang Zhang", "Min-Yen Kan"], "abstract": "In this paper, we explore zero- and few-shot generalization for fact verification (FV), which aims to generalize the FV model trained on well-resourced domains (e.g., Wikipedia) to low-resourced domains that lack human annotations. To this end, we first construct a benchmark dataset collection which contains 11 FV datasets representing 6 domains. We conduct an empirical analysis of generalization across these FV datasets, finding that current models generalize poorly. Our analysis reveals that several factors affect generalization, including dataset size, length of evidence, and the type of claims. Finally, we show that two directions of work improve generalization: 1) incorporating domain knowledge via pretraining on specialized domains, and 2) automatically generating training data via claim generation.", "comments": "AACL-IJCNLP 2023 (main conference, long paper)", "official_code_urls": ["https://github.com/teacherpeterpan/fact-checking-generalization"], "pwc_page_url": "https://paperswithcode.com/paper/investigating-zero-and-few-shot-1", "bibtex": "@misc{pan2023investigating,\n      title={Investigating Zero- and Few-shot Generalization in Fact Verification}, \n      author={Liangming Pan and Yunxiang Zhang and Min-Yen Kan},\n      year={2023},\n      eprint={2309.09444},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "18-09-2023", "categories": ["cs.CL"]}, "2305.13860v1": {"paper_id": "2305.13860v1", "abs_url": "https://arxiv.org/abs/2305.13860v1", "pdf_url": "https://arxiv.org/pdf/2305.13860v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.13860v1_Jailbreaking_ChatGPT_via_Prompt_Engineering_An_Empirical_Study.pdf", "title": "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study", "year": 2023, "paper_venue": null, "authors": ["Yi Liu", "Gelei Deng", "Zhengzi Xu", "Yuekang Li", "Yaowen Zheng", "Ying Zhang", "Lida Zhao", "Tianwei Zhang", "Yang Liu"], "abstract": "Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in jailbreaking LLMs and discusses the challenges of robust jailbreak prompt generation and prevention.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/jailbreaking-chatgpt-via-prompt-engineering", "bibtex": "@misc{liu2023jailbreaking,\n      title={Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study}, \n      author={Yi Liu and Gelei Deng and Zhengzi Xu and Yuekang Li and Yaowen Zheng and Ying Zhang and Lida Zhao and Tianwei Zhang and Yang Liu},\n      year={2023},\n      eprint={2305.13860},\n      archivePrefix={arXiv},\n      primaryClass={cs.SE}\n}", "published_date": "23-05-2023", "categories": ["cs.SE", "cs.AI", "cs.CL"]}, "2310.17631v1": {"paper_id": "2310.17631v1", "abs_url": "https://arxiv.org/abs/2310.17631v1", "pdf_url": "https://arxiv.org/pdf/2310.17631v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.17631v1_JudgeLM_Fine-tuned_Large_Language_Models_are_Scalable_Judges.pdf", "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges", "year": 2023, "paper_venue": null, "authors": ["Lianghui Zhu", "Xinggang Wang", "Xinlong Wang"], "abstract": "Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, and multi-turn chat.", "comments": "30 pages, 23 figures", "official_code_urls": ["https://github.com/baaivision/judgelm"], "pwc_page_url": "https://paperswithcode.com/paper/judgelm-fine-tuned-large-language-models-are", "bibtex": "@misc{zhu2023judgelm,\n      title={JudgeLM: Fine-tuned Large Language Models are Scalable Judges}, \n      author={Lianghui Zhu and Xinggang Wang and Xinlong Wang},\n      year={2023},\n      eprint={2310.17631},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "26-10-2023", "categories": ["cs.CL", "cs.AI"]}, "2312.04889v3": {"paper_id": "2312.04889v3", "abs_url": "https://arxiv.org/abs/2312.04889v3", "pdf_url": "https://arxiv.org/pdf/2312.04889v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.04889v3_KwaiAgents_Generalized_Information-seeking_Agent_System_with_Large_Language_Models.pdf", "title": "KwaiAgents: Generalized Information-seeking Agent System with Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Haojie Pan", "Zepeng Zhai", "Hao Yuan", "Yaojia Lv", "Ruiji Fu", "Ming Liu", "Zhongyuan Wang", "Bing Qin"], "abstract": "Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update and retrieve information from its internal memory, plan and execute actions using a time-aware search-browse toolkit, and ultimately provide a comprehensive response. We further investigate the system's performance when powered by LLMs less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework, designed to ensure even an open-sourced 7B or 13B model performs well among many agent systems. We exploit both benchmark and human evaluations to systematically validate these capabilities. Extensive experiments show the superiority of our agent system compared to other autonomous agents and highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.", "comments": "", "official_code_urls": ["https://github.com/kwaikeg/kwaiagents"], "pwc_page_url": "https://paperswithcode.com/paper/kwaiagents-generalized-information-seeking", "bibtex": "@misc{pan2024kwaiagents,\n      title={KwaiAgents: Generalized Information-seeking Agent System with Large Language Models}, \n      author={Haojie Pan and Zepeng Zhai and Hao Yuan and Yaojia Lv and Ruiji Fu and Ming Liu and Zhongyuan Wang and Bing Qin},\n      year={2024},\n      eprint={2312.04889},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "08-12-2023", "categories": ["cs.AI", "cs.CL", "cs.LG"]}, "2312.17653v1": {"paper_id": "2312.17653v1", "abs_url": "https://arxiv.org/abs/2312.17653v1", "pdf_url": "https://arxiv.org/pdf/2312.17653v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.17653v1_LARP_Language-Agent_Role_Play_for_Open-World_Games.pdf", "title": "LARP: Language-Agent Role Play for Open-World Games", "year": 2023, "paper_venue": null, "authors": ["Ming Yan", "Ruihao Li", "Hao Zhang", "Hao Wang", "Zhilan Yang", "Ji Yan"], "abstract": ".", "comments": "12 pages, 4 figures", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/larp-language-agent-role-play-for-open-world", "bibtex": "@misc{yan2023larp,\n      title={LARP: Language-Agent Role Play for Open-World Games}, \n      author={Ming Yan and Ruihao Li and Hao Zhang and Hao Wang and Zhilan Yang and Ji Yan},\n      year={2023},\n      eprint={2312.17653},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "24-12-2023", "categories": ["cs.AI"]}, "2305.11206v1": {"paper_id": "2305.11206v1", "abs_url": "https://arxiv.org/abs/2305.11206v1", "pdf_url": "https://arxiv.org/pdf/2305.11206v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.11206v1_LIMA_Less_Is_More_for_Alignment.pdf", "title": "LIMA: Less Is More for Alignment", "year": 2023, "paper_venue": null, "authors": ["Chunting Zhou", "Pengfei Liu", "Puxin Xu", "Srini Iyer", "Jiao Sun", "Yuning Mao", "Xuezhe Ma", "Avia Efrat", "Ping Yu", "Lili Yu", "Susan Zhang", "Gargi Ghosh", "Mike Lewis", "Luke Zettlemoyer", "Omer Levy"], "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/lima-less-is-more-for-alignment", "bibtex": "@misc{zhou2023lima,\n      title={LIMA: Less Is More for Alignment}, \n      author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},\n      year={2023},\n      eprint={2305.11206},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "18-05-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2312.11514v2": {"paper_id": "2312.11514v2", "abs_url": "https://arxiv.org/abs/2312.11514v2", "pdf_url": "https://arxiv.org/pdf/2312.11514v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.11514v2_LLM_in_a_flash_Efficient_Large_Language_Model_Inference_with_Limited_Memory.pdf", "title": "LLM in a flash: Efficient Large Language Model Inference with Limited Memory", "year": 2023, "paper_venue": null, "authors": ["Keivan Alizadeh", "Iman Mirzadeh", "Dmitry Belenko", "Karen Khatamifard", "Minsik Cho", "Carlo C Del Mundo", "Mohammad Rastegari", "Mehrdad Farajtabar"], "abstract": "Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, \"windowing\" strategically reduces data transfer by reusing previously activated neurons, and second, \"row-column bundling\", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.", "comments": "preprint", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/llm-in-a-flash-efficient-large-language-model", "bibtex": "@misc{alizadeh2024llm,\n      title={LLM in a flash: Efficient Large Language Model Inference with Limited Memory}, \n      author={Keivan Alizadeh and Iman Mirzadeh and Dmitry Belenko and Karen Khatamifard and Minsik Cho and Carlo C Del Mundo and Mohammad Rastegari and Mehrdad Farajtabar},\n      year={2024},\n      eprint={2312.11514},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "12-12-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2312.06550v1": {"paper_id": "2312.06550v1", "abs_url": "https://arxiv.org/abs/2312.06550v1", "pdf_url": "https://arxiv.org/pdf/2312.06550v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.06550v1_LLM360_Towards_Fully_Transparent_Open-Source_LLMs.pdf", "title": "LLM360: Towards Fully Transparent Open-Source LLMs", "year": 2023, "paper_venue": null, "authors": ["Zhengzhong Liu", "Aurick Qiao", "Willie Neiswanger", "Hongyi Wang", "Bowen Tan", "Tianhua Tao", "Junbo Li", "Yuqi Wang", "Suqi Sun", "Omkar Pangarkar", "Richard Fan", "Yi Gu", "Victor Miller", "Yonghao Zhuang", "Guowei He", "Haonan Li", "Fajri Koto", "Liping Tang", "Nikhil Ranjan", "Zhiqiang Shen", "Xuguang Ren", "Roberto Iriondo", "Cun Mu", "Zhiting Hu", "Mark Schulze", "Preslav Nakov", "Tim Baldwin", "Eric P. Xing"], "abstract": "). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.", "comments": "", "official_code_urls": ["https://github.com/llm360/analysis360"], "pwc_page_url": "https://paperswithcode.com/paper/llm360-towards-fully-transparent-open-source", "bibtex": "@misc{liu2023llm360,\n      title={LLM360: Towards Fully Transparent Open-Source LLMs}, \n      author={Zhengzhong Liu and Aurick Qiao and Willie Neiswanger and Hongyi Wang and Bowen Tan and Tianhua Tao and Junbo Li and Yuqi Wang and Suqi Sun and Omkar Pangarkar and Richard Fan and Yi Gu and Victor Miller and Yonghao Zhuang and Guowei He and Haonan Li and Fajri Koto and Liping Tang and Nikhil Ranjan and Zhiqiang Shen and Xuguang Ren and Roberto Iriondo and Cun Mu and Zhiting Hu and Mark Schulze and Preslav Nakov and Tim Baldwin and Eric P. Xing},\n      year={2023},\n      eprint={2312.06550},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "11-12-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2309.09708v2": {"paper_id": "2309.09708v2", "abs_url": "https://arxiv.org/abs/2309.09708v2", "pdf_url": "https://arxiv.org/pdf/2309.09708v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.09708v2_LLM4Jobs_Unsupervised_occupation_extraction_and_standardization_leveraging_Large_Language_Models.pdf", "title": "LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Nan Li", "Bo Kang", "Tijl De Bie"], "abstract": "Automated occupation extraction and standardization from free-text job postings and resumes are crucial for applications like job recommendation and labor market policy formation. This paper introduces LLM4Jobs, a novel unsupervised methodology that taps into the capabilities of large language models (LLMs) for occupation coding. LLM4Jobs uniquely harnesses both the natural language understanding and generation capacities of LLMs. Evaluated on rigorous experimentation on synthetic and real-world datasets, we demonstrate that LLM4Jobs consistently surpasses unsupervised state-of-the-art benchmarks, demonstrating its versatility across diverse datasets and granularities. As a side result of our work, we present both synthetic and real-world datasets, which may be instrumental for subsequent research in this domain. Overall, this investigation highlights the promise of contemporary LLMs for the intricate task of occupation extraction and standardization, laying the foundation for a robust and adaptable framework relevant to both research and industrial contexts.", "comments": "", "official_code_urls": ["https://github.com/aida-ugent/skillgpt"], "pwc_page_url": "https://paperswithcode.com/paper/llm4jobs-unsupervised-occupation-extraction", "bibtex": "@misc{li2023llm4jobs,\n      title={LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models}, \n      author={Nan Li and Bo Kang and Tijl De Bie},\n      year={2023},\n      eprint={2309.09708},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "18-09-2023", "categories": ["cs.CL", "cs.AI"]}, "2312.07398v2": {"paper_id": "2312.07398v2", "abs_url": "https://arxiv.org/abs/2312.07398v2", "pdf_url": "https://arxiv.org/pdf/2312.07398v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.07398v2_LLMEval_A_Preliminary_Study_on_How_to_Evaluate_Large_Language_Models.pdf", "title": "LLMEval: A Preliminary Study on How to Evaluate Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Yue Zhang", "Ming Zhang", "Haipeng Yuan", "Shichun Liu", "Yongyao Shi", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "abstract": ".", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/llmeval-a-preliminary-study-on-how-to", "bibtex": "@misc{zhang2023llmeval,\n      title={LLMEval: A Preliminary Study on How to Evaluate Large Language Models}, \n      author={Yue Zhang and Ming Zhang and Haipeng Yuan and Shichun Liu and Yongyao Shi and Tao Gui and Qi Zhang and Xuanjing Huang},\n      year={2023},\n      eprint={2312.07398},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "12-12-2023", "categories": ["cs.AI", "cs.CL"]}, "2310.05736v2": {"paper_id": "2310.05736v2", "abs_url": "https://arxiv.org/abs/2310.05736v2", "pdf_url": "https://arxiv.org/pdf/2310.05736v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.05736v2_LLMLingua_Compressing_Prompts_for_Accelerated_Inference_of_Large_Language_Models.pdf", "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Huiqiang Jiang", "Qianhui Wu", "Chin-Yew Lin", "Yuqing Yang", "Lili Qiu"], "abstract": ".", "comments": "Accepted at EMNLP 2023", "official_code_urls": ["https://github.com/microsoft/LLMLingua"], "pwc_page_url": "https://paperswithcode.com/paper/llmlingua-compressing-prompts-for-accelerated", "bibtex": "@misc{jiang2023llmlingua,\n      title={LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models}, \n      author={Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu},\n      year={2023},\n      eprint={2310.05736},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "09-10-2023", "categories": ["cs.CL", "cs.LG"]}, "2303.16199v2": {"paper_id": "2303.16199v2", "abs_url": "https://arxiv.org/abs/2303.16199v2", "pdf_url": "https://arxiv.org/pdf/2303.16199v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2303.16199v2_LLaMA-Adapter_Efficient_Fine-tuning_of_Language_Models_with_Zero-init_Attention.pdf", "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention", "year": 2023, "paper_venue": null, "authors": ["Renrui Zhang", "Jiaming Han", "Chris Liu", "Peng Gao", "Aojun Zhou", "Xiangfei Hu", "Shilin Yan", "Pan Lu", "Hongsheng Li", "Yu Qiao"], "abstract": ".", "comments": "Code is available at this https URL", "official_code_urls": ["https://github.com/opengvlab/llama-adapter", "https://github.com/zrrskywalker/llama-adapter"], "pwc_page_url": "https://paperswithcode.com/paper/llama-adapter-efficient-fine-tuning-of", "bibtex": "@misc{zhang2023llamaadapter,\n      title={LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention}, \n      author={Renrui Zhang and Jiaming Han and Chris Liu and Peng Gao and Aojun Zhou and Xiangfei Hu and Shilin Yan and Pan Lu and Hongsheng Li and Yu Qiao},\n      year={2023},\n      eprint={2303.16199},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "28-03-2023", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"]}, "2302.13971v1": {"paper_id": "2302.13971v1", "abs_url": "https://arxiv.org/abs/2302.13971v1", "pdf_url": "https://arxiv.org/pdf/2302.13971v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2302.13971v1_LLaMA_Open_and_Efficient_Foundation_Language_Models.pdf", "title": "LLaMA: Open and Efficient Foundation Language Models", "year": 2023, "paper_venue": null, "authors": ["Hugo Touvron", "Thibaut Lavril", "Gautier Izacard", "Xavier Martinet", "Marie-Anne Lachaux", "Timoth\u00e9e Lacroix", "Baptiste Rozi\u00e8re", "Naman Goyal", "Eric Hambro", "Faisal Azhar", "Aurelien Rodriguez", "Armand Joulin", "Edouard Grave", "Guillaume Lample"], "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.", "comments": "", "official_code_urls": ["https://github.com/facebookresearch/llama"], "pwc_page_url": "https://paperswithcode.com/paper/llama-open-and-efficient-foundation-language-1", "bibtex": "@misc{touvron2023llama,\n      title={LLaMA: Open and Efficient Foundation Language Models}, \n      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth\u00e9e Lacroix and Baptiste Rozi\u00e8re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},\n      year={2023},\n      eprint={2302.13971},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "27-02-2023", "categories": ["cs.CL"]}, "2311.05437v1": {"paper_id": "2311.05437v1", "abs_url": "https://arxiv.org/abs/2311.05437v1", "pdf_url": "https://arxiv.org/pdf/2311.05437v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.05437v1_LLaVA-Plus_Learning_to_Use_Tools_for_Creating_Multimodal_Agents.pdf", "title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents", "year": 2023, "paper_venue": null, "authors": ["Shilong Liu", "Hao Cheng", "Haotian Liu", "Hao Zhang", "Feng Li", "Tianhe Ren", "Xueyan Zou", "Jianwei Yang", "Hang Su", "Jun Zhu", "Lei Zhang", "Jianfeng Gao", "Chunyuan Li"], "abstract": "LLaVA-Plus is a general-purpose multimodal assistant that expands the capabilities of large multimodal models. It maintains a skill repository of pre-trained vision and vision-language models and can activate relevant tools based on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained on multimodal instruction-following data to acquire the ability to use tools, covering visual understanding, generation, external knowledge retrieval, and compositions. Empirical results show that LLaVA-Plus outperforms LLaVA in existing capabilities and exhibits new ones. It is distinct in that the image query is directly grounded and actively engaged throughout the entire human-AI interaction sessions, significantly improving tool use performance and enabling new scenarios.", "comments": "25 pages, 25M file size. Project Page: this https URL", "official_code_urls": ["https://github.com/LLaVA-VL/LLaVA-Plus-Codebase"], "pwc_page_url": "https://paperswithcode.com/paper/llava-plus-learning-to-use-tools-for-creating", "bibtex": "@misc{liu2023llavaplus,\n      title={LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents}, \n      author={Shilong Liu and Hao Cheng and Haotian Liu and Hao Zhang and Feng Li and Tianhe Ren and Xueyan Zou and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang and Jianfeng Gao and Chunyuan Li},\n      year={2023},\n      eprint={2311.05437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "09-11-2023", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"]}, "2302.14045v2": {"paper_id": "2302.14045v2", "abs_url": "https://arxiv.org/abs/2302.14045v2", "pdf_url": "https://arxiv.org/pdf/2302.14045v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2302.14045v2_Language_Is_Not_All_You_Need_Aligning_Perception_with_Language_Models.pdf", "title": "Language Is Not All You Need: Aligning Perception with Language Models", "year": 2023, "paper_venue": null, "authors": ["Shaohan Huang", "Li Dong", "Wenhui Wang", "Yaru Hao", "Saksham Singhal", "Shuming Ma", "Tengchao Lv", "Lei Cui", "Owais Khan Mohammed", "Barun Patra", "Qiang Liu", "Kriti Aggarwal", "Zewen Chi", "Johan Bjorck", "Vishrav Chaudhary", "Subhojit Som", "Xia Song", "Furu Wei"], "abstract": "A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.", "comments": "", "official_code_urls": ["https://github.com/microsoft/unilm"], "pwc_page_url": "https://paperswithcode.com/paper/language-is-not-all-you-need-aligning-1", "bibtex": "@misc{huang2023language,\n      title={Language Is Not All You Need: Aligning Perception with Language Models}, \n      author={Shaohan Huang and Li Dong and Wenhui Wang and Yaru Hao and Saksham Singhal and Shuming Ma and Tengchao Lv and Lei Cui and Owais Khan Mohammed and Barun Patra and Qiang Liu and Kriti Aggarwal and Zewen Chi and Johan Bjorck and Vishrav Chaudhary and Subhojit Som and Xia Song and Furu Wei},\n      year={2023},\n      eprint={2302.14045},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "27-02-2023", "categories": ["cs.CL", "cs.CV"]}, "2311.03099v1": {"paper_id": "2311.03099v1", "abs_url": "https://arxiv.org/abs/2311.03099v1", "pdf_url": "https://arxiv.org/pdf/2311.03099v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.03099v1_Language_Models_are_Super_Mario_Absorbing_Abilities_from_Homologous_Models_as_a_Free_Lunch.pdf", "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch", "year": 2023, "paper_venue": null, "authors": ["Le Yu", "Bowen Yu", "Haiyang Yu", "Fei Huang", "Yongbin Li"], "abstract": ".", "comments": "24 pages, 21 figures", "official_code_urls": ["https://github.com/yule-buaa/mergelm"], "pwc_page_url": "https://paperswithcode.com/paper/language-models-are-super-mario-absorbing", "bibtex": "@misc{yu2023language,\n      title={Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch}, \n      author={Le Yu and Bowen Yu and Haiyang Yu and Fei Huang and Yongbin Li},\n      year={2023},\n      eprint={2311.03099},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "06-11-2023", "categories": ["cs.CL", "cs.LG"]}, "2309.15025v1": {"paper_id": "2309.15025v1", "abs_url": "https://arxiv.org/abs/2309.15025v1", "pdf_url": "https://arxiv.org/pdf/2309.15025v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.15025v1_Large_Language_Model_Alignment_A_Survey.pdf", "title": "Large Language Model Alignment: A Survey", "year": 2023, "paper_venue": null, "authors": ["Tianhao Shen", "Renren Jin", "Yufei Huang", "Chuang Liu", "Weilong Dong", "Zishan Guo", "Xinwei Wu", "Yan Liu", "Deyi Xiong"], "abstract": "Our aspiration for this survey extends beyond merely spurring research interests in this realm. We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs.", "comments": "76 pages", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/large-language-model-alignment-a-survey", "bibtex": "@misc{shen2023large,\n      title={Large Language Model Alignment: A Survey}, \n      author={Tianhao Shen and Renren Jin and Yufei Huang and Chuang Liu and Weilong Dong and Zishan Guo and Xinwei Wu and Yan Liu and Deyi Xiong},\n      year={2023},\n      eprint={2309.15025},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "26-09-2023", "categories": ["cs.CL", "cs.AI"]}, "2310.07820v1": {"paper_id": "2310.07820v1", "abs_url": "https://arxiv.org/abs/2310.07820v1", "pdf_url": "https://arxiv.org/pdf/2310.07820v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.07820v1_Large_Language_Models_Are_Zero-Shot_Time_Series_Forecasters.pdf", "title": "Large Language Models Are Zero-Shot Time Series Forecasters", "year": 2023, "paper_venue": null, "authors": ["Nate Gruver", "Marc Finzi", "Shikai Qiu", "Andrew Gordon Wilson"], "abstract": "By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.", "comments": "NeurIPS 2023. Code available at: this https URL", "official_code_urls": ["https://github.com/ngruver/llmtime"], "pwc_page_url": "https://paperswithcode.com/paper/large-language-models-are-zero-shot-time-1", "bibtex": "@misc{gruver2023large,\n      title={Large Language Models Are Zero-Shot Time Series Forecasters}, \n      author={Nate Gruver and Marc Finzi and Shikai Qiu and Andrew Gordon Wilson},\n      year={2023},\n      eprint={2310.07820},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "11-10-2023", "categories": ["cs.LG"]}, "2302.00093v3": {"paper_id": "2302.00093v3", "abs_url": "https://arxiv.org/abs/2302.00093v3", "pdf_url": "https://arxiv.org/pdf/2302.00093v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2302.00093v3_Large_Language_Models_Can_Be_Easily_Distracted_by_Irrelevant_Context.pdf", "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context", "year": 2023, "paper_venue": null, "authors": ["Freda Shi", "Xinyun Chen", "Kanishka Misra", "Nathan Scales", "David Dohan", "Ed Chi", "Nathanael Sch\u00e4rli", "Denny Zhou"], "abstract": "Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.", "comments": "Published in ICML 2023", "official_code_urls": ["https://github.com/google-research-datasets/gsm-ic"], "pwc_page_url": "https://paperswithcode.com/paper/large-language-models-can-be-easily", "bibtex": "@misc{shi2023large,\n      title={Large Language Models Can Be Easily Distracted by Irrelevant Context}, \n      author={Freda Shi and Xinyun Chen and Kanishka Misra and Nathan Scales and David Dohan and Ed Chi and Nathanael Sch\u00e4rli and Denny Zhou},\n      year={2023},\n      eprint={2302.00093},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "31-01-2023", "categories": ["cs.CL", "cs.AI"]}, "2310.01798v1": {"paper_id": "2310.01798v1", "abs_url": "https://arxiv.org/abs/2310.01798v1", "pdf_url": "https://arxiv.org/pdf/2310.01798v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.01798v1_Large_Language_Models_Cannot_Self-Correct_Reasoning_Yet.pdf", "title": "Large Language Models Cannot Self-Correct Reasoning Yet", "year": 2023, "paper_venue": null, "authors": ["Jie Huang", "Xinyun Chen", "Swaroop Mishra", "Huaixiu Steven Zheng", "Adams Wei Yu", "Xinying Song", "Denny Zhou"], "abstract": "Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/large-language-models-cannot-self-correct", "bibtex": "@misc{huang2023large,\n      title={Large Language Models Cannot Self-Correct Reasoning Yet}, \n      author={Jie Huang and Xinyun Chen and Swaroop Mishra and Huaixiu Steven Zheng and Adams Wei Yu and Xinying Song and Denny Zhou},\n      year={2023},\n      eprint={2310.01798},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "03-10-2023", "categories": ["cs.CL", "cs.AI"]}, "2311.01918v1": {"paper_id": "2311.01918v1", "abs_url": "https://arxiv.org/abs/2311.01918v1", "pdf_url": "https://arxiv.org/pdf/2311.01918v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.01918v1_Large_Language_Models_Illuminate_a_Progressive_Pathway_to_Artificial_Healthcare_Assistant_A_Review.pdf", "title": "Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review", "year": 2023, "paper_venue": null, "authors": ["Mingze Yuan", "Peng Bao", "Jiajia Yuan", "Yunhao Shen", "Zifan Chen", "Yi Xie", "Jie Zhao", "Yang Chen", "Li Zhang", "Lin Shen", "Bin Dong"], "abstract": "for an accompanying GitHub repository containing latest papers.", "comments": "24 pages, 1 figure, 3 tables", "official_code_urls": ["https://github.com/mingze-yuan/awesome-llm-healthcare"], "pwc_page_url": "https://paperswithcode.com/paper/large-language-models-illuminate-a", "bibtex": "@misc{yuan2023large,\n      title={Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review}, \n      author={Mingze Yuan and Peng Bao and Jiajia Yuan and Yunhao Shen and Zifan Chen and Yi Xie and Jie Zhao and Yang Chen and Li Zhang and Lin Shen and Bin Dong},\n      year={2023},\n      eprint={2311.01918},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "03-11-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2307.11760v7": {"paper_id": "2307.11760v7", "abs_url": "https://arxiv.org/abs/2307.11760v7", "pdf_url": "https://arxiv.org/pdf/2307.11760v7.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2307.11760v7_Large_Language_Models_Understand_and_Can_be_Enhanced_by_Emotional_Stimuli.pdf", "title": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli", "year": 2023, "paper_venue": null, "authors": ["Cheng Li", "Jindong Wang", "Yixuan Zhang", "Kaijie Zhu", "Wenxin Hou", "Jianxun Lian", "Fang Luo", "Qiang Yang", "Xing Xie"], "abstract": "Emotional intelligence significantly impacts our daily behaviors and interactions. Although Large Language Models (LLMs) are increasingly viewed as a stride toward artificial general intelligence, exhibiting impressive performance in numerous tasks, it is still uncertain if LLMs can genuinely grasp psychological emotional stimuli. Understanding and responding to emotional cues gives humans a distinct advantage in problem-solving. In this paper, we take the first step towards exploring the ability of LLMs to understand emotional stimuli. To this end, we first conduct automatic experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna, Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative applications that represent comprehensive evaluation scenarios. Our automatic experiments show that LLMs have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which we call \"EmotionPrompt\" that combines the original prompt with emotional stimuli), e.g., 8.00% relative performance improvement in Instruction Induction and 115% in BIG-Bench. In addition to those deterministic tasks that can be automatically evaluated using existing metrics, we conducted a human study with 106 participants to assess the quality of generative tasks using both vanilla and emotional prompts. Our human study results demonstrate that EmotionPrompt significantly boosts the performance of generative tasks (10.9% average improvement in terms of performance, truthfulness, and responsibility metrics). We provide an in-depth discussion regarding why EmotionPrompt works for LLMs and the factors that may influence its performance. We posit that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs interaction.", "comments": "Technical report; updated the std error for human study; short version (v1) was accepted by LLM@IJCAI'23; 32 pages; more work: this https URL", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/emotionprompt-leveraging-psychology-for-large", "bibtex": "@misc{li2023large,\n      title={Large Language Models Understand and Can be Enhanced by Emotional Stimuli}, \n      author={Cheng Li and Jindong Wang and Yixuan Zhang and Kaijie Zhu and Wenxin Hou and Jianxun Lian and Fang Luo and Qiang Yang and Xing Xie},\n      year={2023},\n      eprint={2307.11760},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "14-07-2023", "categories": ["cs.CL", "cs.AI", "cs.HC"]}, "2301.13808v3": {"paper_id": "2301.13808v3", "abs_url": "https://arxiv.org/abs/2301.13808v3", "pdf_url": "https://arxiv.org/pdf/2301.13808v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2301.13808v3_Large_Language_Models_are_Versatile_Decomposers_Decompose_Evidence_and_Questions_for_Table-based_Reasoning.pdf", "title": "Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning", "year": 2023, "paper_venue": null, "authors": ["Yunhu Ye", "Binyuan Hui", "Min Yang", "Binhua Li", "Fei Huang", "Yongbin Li"], "abstract": "Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition, most existing methods struggle to reason over complex questions since the required information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning; and (ii) decompose complex questions into simpler sub-questions for text reasoning. Specifically, we first use the LLMs to break down the evidence (tables) involved in the current question, retaining the relevant evidence and excluding the remaining irrelevant evidence from the huge table. In addition, we propose a \"parsing-execution-filling\" strategy to alleviate the hallucination dilemma of the chain of thought by decoupling logic and numerical computation in each step. Extensive experiments show that our method can effectively leverage decomposed evidence and questions and outperforms the strong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably, our model outperforms human performance for the first time on the TabFact dataset.", "comments": "SIGIR 2023", "official_code_urls": ["https://github.com/alibabaresearch/damo-convai"], "pwc_page_url": "https://paperswithcode.com/paper/large-language-models-are-versatile", "bibtex": "@misc{ye2023large,\n      title={Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning}, \n      author={Yunhu Ye and Binyuan Hui and Min Yang and Binhua Li and Fei Huang and Yongbin Li},\n      year={2023},\n      eprint={2301.13808},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "31-01-2023", "categories": ["cs.CL"]}, "2310.19046v2": {"paper_id": "2310.19046v2", "abs_url": "https://arxiv.org/abs/2310.19046v2", "pdf_url": "https://arxiv.org/pdf/2310.19046v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.19046v2_Large_Language_Models_as_Evolutionary_Optimizers.pdf", "title": "Large Language Models as Evolutionary Optimizers", "year": 2023, "paper_venue": null, "authors": ["Shengcai Liu", "Caishun Chen", "Xinghua Qu", "Ke Tang", "Yew-Soon Ong"], "abstract": "Evolutionary algorithms (EAs) have achieved remarkable success in tackling complex combinatorial optimization problems. However, EAs often demand carefully-designed operators with the aid of domain expertise to achieve satisfactory performance. In this work, we present the first study on large language models (LLMs) as evolutionary combinatorial optimizers. The main advantage is that it requires minimal domain knowledge and human efforts, as well as no additional training of the model. This approach is referred to as LLM-driven EA (LMEA). Specifically, in each generation of the evolutionary search, LMEA instructs the LLM to select parent solutions from current population, and perform crossover and mutation to generate offspring solutions. Then, LMEA evaluates these new solutions and include them into the population for the next generation. LMEA is equipped with a self-adaptation mechanism that controls the temperature of the LLM. This enables it to balance between exploration and exploitation and prevents the search from getting stuck in local optima. We investigate the power of LMEA on the classical traveling salesman problems (TSPs) widely used in combinatorial optimization research. Notably, the results show that LMEA performs competitively to traditional heuristics in finding high-quality solutions on TSP instances with up to 20 nodes. Additionally, we also study the effectiveness of LLM-driven crossover/mutation and the self-adaptation mechanism in evolutionary search. In summary, our results reveal the great potentials of LLMs as evolutionary optimizers for solving combinatorial problems. We hope our research shall inspire future explorations on LLM-driven EAs for complex optimization challenges.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/large-language-models-as-evolutionary", "bibtex": "@misc{liu2023large,\n      title={Large Language Models as Evolutionary Optimizers}, \n      author={Shengcai Liu and Caishun Chen and Xinghua Qu and Ke Tang and Yew-Soon Ong},\n      year={2023},\n      eprint={2310.19046},\n      archivePrefix={arXiv},\n      primaryClass={cs.NE}\n}", "published_date": "29-10-2023", "categories": ["cs.NE"]}, "2309.03409v2": {"paper_id": "2309.03409v2", "abs_url": "https://arxiv.org/abs/2309.03409v2", "pdf_url": "https://arxiv.org/pdf/2309.03409v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.03409v2_Large_Language_Models_as_Optimizers.pdf", "title": "Large Language Models as Optimizers", "year": 2023, "paper_venue": null, "authors": ["Chengrun Yang", "Xuezhi Wang", "Yifeng Lu", "Hanxiao Liu", "Quoc V. Le", "Denny Zhou", "Xinyun Chen"], "abstract": ".", "comments": "42 pages, 26 figures, 15 tables. Code at this https URL", "official_code_urls": ["https://github.com/google-deepmind/opro"], "pwc_page_url": "https://paperswithcode.com/paper/large-language-models-as-optimizers", "bibtex": "@misc{yang2023large,\n      title={Large Language Models as Optimizers}, \n      author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V. Le and Denny Zhou and Xinyun Chen},\n      year={2023},\n      eprint={2309.03409},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "07-09-2023", "categories": ["cs.LG", "cs.AI", "cs.CL"]}, "2312.17617v1": {"paper_id": "2312.17617v1", "abs_url": "https://arxiv.org/abs/2312.17617v1", "pdf_url": "https://arxiv.org/pdf/2312.17617v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.17617v1_Large_Language_Models_for_Generative_Information_Extraction_A_Survey.pdf", "title": "Large Language Models for Generative Information Extraction: A Survey", "year": 2023, "paper_venue": null, "authors": ["Derong Xu", "Wei Chen", "Wenjun Peng", "Chao Zhang", "Tong Xu", "Xiangyu Zhao", "Xian Wu", "Yefeng Zheng", "Enhong Chen"], "abstract": "}.", "comments": "", "official_code_urls": ["https://github.com/quqxui/awesome-llm4ie-papers"], "pwc_page_url": "https://paperswithcode.com/paper/large-language-models-for-generative-1", "bibtex": "@misc{xu2023large,\n      title={Large Language Models for Generative Information Extraction: A Survey}, \n      author={Derong Xu and Wei Chen and Wenjun Peng and Chao Zhang and Tong Xu and Xiangyu Zhao and Xian Wu and Yefeng Zheng and Enhong Chen},\n      year={2023},\n      eprint={2312.17617},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "29-12-2023", "categories": ["cs.CL"]}, "2310.03533v4": {"paper_id": "2310.03533v4", "abs_url": "https://arxiv.org/abs/2310.03533v4", "pdf_url": "https://arxiv.org/pdf/2310.03533v4.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.03533v4_Large_Language_Models_for_Software_Engineering_Survey_and_Open_Problems.pdf", "title": "Large Language Models for Software Engineering: Survey and Open Problems", "year": 2023, "paper_venue": null, "authors": ["Angela Fan", "Beliz Gokkaya", "Mark Harman", "Mitya Lyubarskiy", "Shubho Sengupta", "Shin Yoo", "Jie M. Zhang"], "abstract": "This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/large-language-models-for-software-1", "bibtex": "@misc{fan2023large,\n      title={Large Language Models for Software Engineering: Survey and Open Problems}, \n      author={Angela Fan and Beliz Gokkaya and Mark Harman and Mitya Lyubarskiy and Shubho Sengupta and Shin Yoo and Jie M. Zhang},\n      year={2023},\n      eprint={2310.03533},\n      archivePrefix={arXiv},\n      primaryClass={cs.SE}\n}", "published_date": "05-10-2023", "categories": ["cs.SE"]}, "2303.07142v3": {"paper_id": "2303.07142v3", "abs_url": "https://arxiv.org/abs/2303.07142v3", "pdf_url": "https://arxiv.org/pdf/2303.07142v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2303.07142v3_Large_Language_Models_in_the_Workplace_A_Case_Study_on_Prompt_Engineering_for_Job_Type_Classification.pdf", "title": "Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification", "year": 2023, "paper_venue": null, "authors": ["Benjamin Clavi\u00e9", "Alexandru Ciceu", "Frederick Naylor", "Guillaume Souli\u00e9", "Thomas Brightwell"], "abstract": "This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language job posting is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance. Our results show that, with a well-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all other models, achieving a 6% increase in Precision@95% Recall compared to the best supervised approach. Furthermore, we observe that the wording of the prompt is a critical factor in eliciting the appropriate \"reasoning\" in the model, and that seemingly minor aspects of the prompt significantly affect the model's performance.", "comments": "Accepted as a long paper at NLDB 2023", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/large-language-models-in-the-workplace-a-case", "bibtex": "@misc{clavi\u00e92023large,\n      title={Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification}, \n      author={Benjamin Clavi\u00e9 and Alexandru Ciceu and Frederick Naylor and Guillaume Souli\u00e9 and Thomas Brightwell},\n      year={2023},\n      eprint={2303.07142},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "13-03-2023", "categories": ["cs.CL"]}, "2312.05409v1": {"paper_id": "2312.05409v1", "abs_url": "https://arxiv.org/abs/2312.05409v1", "pdf_url": "https://arxiv.org/pdf/2312.05409v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.05409v1_Large-scale_Training_of_Foundation_Models_for_Wearable_Biosignals.pdf", "title": "Large-scale Training of Foundation Models for Wearable Biosignals", "year": 2023, "paper_venue": null, "authors": ["Salar Abbaspourazad", "Oussama Elachqar", "Andrew C. Miller", "Saba Emrani", "Udhyakumar Nallasamy", "Ian Shapiro"], "abstract": "Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for developing neural network models for biosignals. To address this challenge, we have employed self-supervised learning using the unlabeled sensor data collected under informed consent from the large longitudinal Apple Heart and Movement Study (AHMS) to train foundation models for two common biosignals: photoplethysmography (PPG) and electrocardiogram (ECG) recorded on Apple Watch. We curated PPG and ECG datasets from AHMS that include data from ~141K participants spanning ~3 years. Our self-supervised learning framework includes participant level positive pair selection, stochastic augmentation module and a regularized contrastive loss optimized with momentum training, and generalizes well to both PPG and ECG modalities. We show that the pre-trained foundation models readily encode information regarding participants' demographics and health conditions. To the best of our knowledge, this is the first study that builds foundation models using large-scale PPG and ECG data collected via wearable consumer devices $\\unicode{x2013}$ prior works have commonly used smaller-size datasets collected in clinical and experimental settings. We believe PPG and ECG foundation models can enhance future wearable devices by reducing the reliance on labeled data and hold the potential to help the users improve their health.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/large-scale-training-of-foundation-models-for", "bibtex": "@misc{abbaspourazad2023largescale,\n      title={Large-scale Training of Foundation Models for Wearable Biosignals}, \n      author={Salar Abbaspourazad and Oussama Elachqar and Andrew C. Miller and Saba Emrani and Udhyakumar Nallasamy and Ian Shapiro},\n      year={2023},\n      eprint={2312.05409},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "08-12-2023", "categories": ["cs.LG", "cs.AI", "eess.SP"]}, "2310.20689v2": {"paper_id": "2310.20689v2", "abs_url": "https://arxiv.org/abs/2310.20689v2", "pdf_url": "https://arxiv.org/pdf/2310.20689v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.20689v2_Learning_From_Mistakes_Makes_LLM_Better_Reasoner.pdf", "title": "Learning From Mistakes Makes LLM Better Reasoner", "year": 2023, "paper_venue": null, "authors": ["Shengnan An", "Zexiong Ma", "Zeqi Lin", "Nanning Zheng", "Jian-Guang Lou", "Weizhu Chen"], "abstract": ".", "comments": "14 pages, 4 figures", "official_code_urls": ["https://github.com/microsoft/lema", "https://github.com/microsoft/codet"], "pwc_page_url": "https://paperswithcode.com/paper/learning-from-mistakes-makes-llm-better", "bibtex": "@misc{an2023learning,\n      title={Learning From Mistakes Makes LLM Better Reasoner}, \n      author={Shengnan An and Zexiong Ma and Zeqi Lin and Nanning Zheng and Jian-Guang Lou and Weizhu Chen},\n      year={2023},\n      eprint={2310.20689},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "31-10-2023", "categories": ["cs.CL", "cs.AI"]}, "2305.19339v1": {"paper_id": "2305.19339v1", "abs_url": "https://arxiv.org/abs/2305.19339v1", "pdf_url": "https://arxiv.org/pdf/2305.19339v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.19339v1_Less_Likely_Brainstorming_Using_Language_Models_to_Generate_Alternative_Hypotheses.pdf", "title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses", "year": 2023, "paper_venue": null, "authors": ["Liyan Tang", "Yifan Peng", "Yanshan Wang", "Ying Ding", "Greg Durrett", "Justin F. Rousseau"], "abstract": "A human decision-maker benefits the most from an AI assistant that corrects for their biases. For problems such as generating interpretation of a radiology report given findings, a system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user. To alleviate biases in human decision-making, it is worth considering a broad differential diagnosis, going beyond the most likely options. We introduce a new task, \"less likely brainstorming,\" that asks a model to generate outputs that humans think are relevant but less likely to happen. We explore the task in two settings: a brain MRI interpretation generation setting and an everyday commonsense reasoning setting. We found that a baseline approach of training with less likely hypotheses as targets generates outputs that humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective. To tackle this problem, we propose a controlled text generation method that uses a novel contrastive learning strategy to encourage models to differentiate between generating likely and less likely outputs according to humans. We compare our method with several state-of-the-art controlled text generation models via automatic and human evaluations and show that our models' capability of generating less likely outputs is improved.", "comments": "Accepted to ACL (Findings) 2023", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/less-likely-brainstorming-using-language", "bibtex": "@misc{tang2023likely,\n      title={Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses}, \n      author={Liyan Tang and Yifan Peng and Yanshan Wang and Ying Ding and Greg Durrett and Justin F. Rousseau},\n      year={2023},\n      eprint={2305.19339},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "30-05-2023", "categories": ["cs.CL", "cs.AI"]}, "2307.09288v2": {"paper_id": "2307.09288v2", "abs_url": "https://arxiv.org/abs/2307.09288v2", "pdf_url": "https://arxiv.org/pdf/2307.09288v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2307.09288v2_Llama_2_Open_Foundation_and_Fine-Tuned_Chat_Models.pdf", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "year": 2023, "paper_venue": null, "authors": ["Hugo Touvron", "Louis Martin", "Kevin Stone", "Peter Albert", "Amjad Almahairi", "Yasmine Babaei", "Nikolay Bashlykov", "Soumya Batra", "Prajjwal Bhargava", "Shruti Bhosale", "Dan Bikel", "Lukas Blecher", "Cristian Canton Ferrer", "Moya Chen", "Guillem Cucurull", "David Esiobu", "Jude Fernandes", "Jeremy Fu", "Wenyin Fu", "Brian Fuller", "Cynthia Gao", "Vedanuj Goswami", "Naman Goyal", "Anthony Hartshorn", "Saghar Hosseini", "Rui Hou", "Hakan Inan", "Marcin Kardas", "Viktor Kerkez", "Madian Khabsa", "Isabel Kloumann", "Artem Korenev", "Punit Singh Koura", "Marie-Anne Lachaux", "Thibaut Lavril", "Jenya Lee", "Diana Liskovich", "Yinghai Lu", "Yuning Mao", "Xavier Martinet", "Todor Mihaylov", "Pushkar Mishra", "Igor Molybog", "Yixin Nie", "Andrew Poulton", "Jeremy Reizenstein", "Rashi Rungta", "Kalyan Saladi", "Alan Schelten", "Ruan Silva", "Eric Michael Smith", "Ranjan Subramanian", "Xiaoqing Ellen Tan", "Binh Tang", "Ross Taylor", "Adina Williams", "Jian Xiang Kuan", "Puxin Xu", "Zheng Yan", "Iliyan Zarov", "Yuchen Zhang", "Angela Fan", "Melanie Kambadur", "Sharan Narang", "Aurelien Rodriguez", "Robert Stojnic", "Sergey Edunov", "Thomas Scialom"], "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "comments": "", "official_code_urls": ["https://github.com/facebookresearch/llama"], "pwc_page_url": "https://paperswithcode.com/paper/llama-2-open-foundation-and-fine-tuned-chat", "bibtex": "@misc{touvron2023llama,\n      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, \n      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n      year={2023},\n      eprint={2307.09288},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "18-07-2023", "categories": ["cs.CL", "cs.AI"]}, "2310.08659v4": {"paper_id": "2310.08659v4", "abs_url": "https://arxiv.org/abs/2310.08659v4", "pdf_url": "https://arxiv.org/pdf/2310.08659v4.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.08659v4_LoftQ_LoRA-Fine-Tuning-Aware_Quantization_for_Large_Language_Models.pdf", "title": "LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Yixiao Li", "Yifan Yu", "Chen Liang", "Pengcheng He", "Nikos Karampatziakis", "Weizhu Chen", "Tuo Zhao"], "abstract": ".", "comments": "", "official_code_urls": ["https://github.com/yxli2123/loftq"], "pwc_page_url": "https://paperswithcode.com/paper/loftq-lora-fine-tuning-aware-quantization-for", "bibtex": "@misc{li2023loftq,\n      title={LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models}, \n      author={Yixiao Li and Yifan Yu and Chen Liang and Pengcheng He and Nikos Karampatziakis and Weizhu Chen and Tuo Zhao},\n      year={2023},\n      eprint={2310.08659},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "12-10-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2311.07463v1": {"paper_id": "2311.07463v1", "abs_url": "https://arxiv.org/abs/2311.07463v1", "pdf_url": "https://arxiv.org/pdf/2311.07463v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.07463v1_MEGAVERSE_Benchmarking_Large_Language_Models_Across_Languages_Modalities_Models_and_Tasks.pdf", "title": "MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks", "year": 2023, "paper_venue": null, "authors": ["Sanchit Ahuja", "Divyanshu Aggarwal", "Varun Gumma", "Ishaan Watts", "Ashutosh Sathe", "Millicent Ochieng", "Rishav Hada", "Prachi Jain", "Maxamed Axmed", "Kalika Bali", "Sunayana Sitaram"], "abstract": "Recently, there has been a rapid advancement in research on Large Language Models (LLMs), resulting in significant progress in several Natural Language Processing (NLP) tasks. Consequently, there has been a surge in LLM evaluation research to comprehend the models' capabilities and limitations. However, much of this research has been confined to the English language, leaving LLM building and evaluation for non-English languages relatively unexplored. There has been an introduction of several new LLMs, necessitating their evaluation on non-English languages. This study aims to expand our MEGA benchmarking suite by including six new datasets to form the MEGAVERSE benchmark. The benchmark comprises 22 datasets covering 81 languages, including low-resource African languages. We evaluate several state-of-the-art LLMs like GPT-3.5-Turbo, GPT4, PaLM2, and Llama2 on the MEGAVERSE datasets. Additionally, we include two multimodal datasets in the benchmark and assess the performance of the LLaVa-v1.5 model. Our experiments suggest that GPT4 and PaLM2 outperform the Llama models on various tasks, notably on low-resource languages, with GPT4 outperforming PaLM2 on more datasets than vice versa. However, issues such as data contamination must be addressed to obtain an accurate assessment of LLM performance on non-English languages.", "comments": "23 pages, 30 figures and 1 table", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/megaverse-benchmarking-large-language-models", "bibtex": "@misc{ahuja2023megaverse,\n      title={MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks}, \n      author={Sanchit Ahuja and Divyanshu Aggarwal and Varun Gumma and Ishaan Watts and Ashutosh Sathe and Millicent Ochieng and Rishav Hada and Prachi Jain and Maxamed Axmed and Kalika Bali and Sunayana Sitaram},\n      year={2023},\n      eprint={2311.07463},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "13-11-2023", "categories": ["cs.CL"]}, "2304.14979v1": {"paper_id": "2304.14979v1", "abs_url": "https://arxiv.org/abs/2304.14979v1", "pdf_url": "https://arxiv.org/pdf/2304.14979v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2304.14979v1_MLCopilot_Unleashing_the_Power_of_Large_Language_Models_in_Solving_Machine_Learning_Tasks.pdf", "title": "MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks", "year": 2023, "paper_venue": null, "authors": ["Lei Zhang", "Yuge Zhang", "Kan Ren", "Dongsheng Li", "Yuqing Yang"], "abstract": "The field of machine learning (ML) has gained widespread adoption, leading to a significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art LLMs to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the existing experiences of ML tasks and (ii) reason effectively to deliver promising results for new tasks. The solution generated can be used directly to achieve high levels of competitiveness.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/mlcopilot-unleashing-the-power-of-large", "bibtex": "@misc{zhang2023mlcopilot,\n      title={MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks}, \n      author={Lei Zhang and Yuge Zhang and Kan Ren and Dongsheng Li and Yuqing Yang},\n      year={2023},\n      eprint={2304.14979},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "28-04-2023", "categories": ["cs.LG", "cs.AI"]}, "2303.11381v1": {"paper_id": "2303.11381v1", "abs_url": "https://arxiv.org/abs/2303.11381v1", "pdf_url": "https://arxiv.org/pdf/2303.11381v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2303.11381v1_MM-REACT_Prompting_ChatGPT_for_Multimodal_Reasoning_and_Action.pdf", "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action", "year": 2023, "paper_venue": null, "authors": ["Zhengyuan Yang", "Linjie Li", "Jianfeng Wang", "Kevin Lin", "Ehsan Azarnasab", "Faisal Ahmed", "Zicheng Liu", "Ce Liu", "Michael Zeng", "Lijuan Wang"], "abstract": "", "comments": "", "official_code_urls": ["https://github.com/microsoft/MM-REACT"], "pwc_page_url": "https://paperswithcode.com/paper/mm-react-prompting-chatgpt-for-multimodal", "bibtex": "@misc{yang2023mmreact,\n      title={MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action}, \n      author={Zhengyuan Yang and Linjie Li and Jianfeng Wang and Kevin Lin and Ehsan Azarnasab and Faisal Ahmed and Zicheng Liu and Ce Liu and Michael Zeng and Lijuan Wang},\n      year={2023},\n      eprint={2303.11381},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "20-03-2023", "categories": ["cs.CV", "cs.CL", "cs.LG"]}, "2311.10537v1": {"paper_id": "2311.10537v1", "abs_url": "https://arxiv.org/abs/2311.10537v1", "pdf_url": "https://arxiv.org/pdf/2311.10537v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.10537v1_MedAgents_Large_Language_Models_as_Collaborators_for_Zero-shot_Medical_Reasoning.pdf", "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning", "year": 2023, "paper_venue": null, "authors": ["Xiangru Tang", "Anni Zou", "Zhuosheng Zhang", "Yilun Zhao", "Xingyao Zhang", "Arman Cohan", "Mark Gerstein"], "abstract": "}.", "comments": "", "official_code_urls": ["https://github.com/gersteinlab/medagents"], "pwc_page_url": "https://paperswithcode.com/paper/medagents-large-language-models-as", "bibtex": "@misc{tang2023medagents,\n      title={MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning}, \n      author={Xiangru Tang and Anni Zou and Zhuosheng Zhang and Yilun Zhao and Xingyao Zhang and Arman Cohan and Mark Gerstein},\n      year={2023},\n      eprint={2311.10537},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "16-11-2023", "categories": ["cs.CL", "cs.AI"]}, "2308.00352v5": {"paper_id": "2308.00352v5", "abs_url": "https://arxiv.org/abs/2308.00352v5", "pdf_url": "https://arxiv.org/pdf/2308.00352v5.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2308.00352v5_MetaGPT_Meta_Programming_for_A_Multi-Agent_Collaborative_Framework.pdf", "title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework", "year": 2023, "paper_venue": null, "authors": ["Sirui Hong", "Mingchen Zhuge", "Jonathan Chen", "Xiawu Zheng", "Yuheng Cheng", "Ceyao Zhang", "Jinlin Wang", "Zili Wang", "Steven Ka Shing Yau", "Zijuan Lin", "Liyang Zhou", "Chenyu Ran", "Lingfeng Xiao", "Chenglin Wu", "J\u00fcrgen Schmidhuber"], "abstract": "", "comments": "", "official_code_urls": ["https://github.com/geekan/metagpt"], "pwc_page_url": "https://paperswithcode.com/paper/metagpt-meta-programming-for-multi-agent", "bibtex": "@misc{hong2023metagpt,\n      title={MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework}, \n      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J\u00fcrgen Schmidhuber},\n      year={2023},\n      eprint={2308.00352},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "01-08-2023", "categories": ["cs.AI", "cs.MA"]}, "2306.06070v3": {"paper_id": "2306.06070v3", "abs_url": "https://arxiv.org/abs/2306.06070v3", "pdf_url": "https://arxiv.org/pdf/2306.06070v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2306.06070v3_Mind2Web_Towards_a_Generalist_Agent_for_the_Web.pdf", "title": "Mind2Web: Towards a Generalist Agent for the Web", "year": 2023, "paper_venue": null, "authors": ["Xiang Deng", "Yu Gu", "Boyuan Zheng", "Shijie Chen", "Samuel Stevens", "Boshi Wang", "Huan Sun", "Yu Su"], "abstract": ") to facilitate further research on building a generalist agent for the web.", "comments": "Website: this https URL . Updated with supplementary material. NeurIPS'23 Spotlight", "official_code_urls": ["https://github.com/osu-nlp-group/mind2web"], "pwc_page_url": "https://paperswithcode.com/paper/mind2web-towards-a-generalist-agent-for-the-1", "bibtex": "@misc{deng2023mind2web,\n      title={Mind2Web: Towards a Generalist Agent for the Web}, \n      author={Xiang Deng and Yu Gu and Boyuan Zheng and Shijie Chen and Samuel Stevens and Boshi Wang and Huan Sun and Yu Su},\n      year={2023},\n      eprint={2306.06070},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "09-06-2023", "categories": ["cs.CL"]}, "2309.09971v2": {"paper_id": "2309.09971v2", "abs_url": "https://arxiv.org/abs/2309.09971v2", "pdf_url": "https://arxiv.org/pdf/2309.09971v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.09971v2_MindAgent_Emergent_Gaming_Interaction.pdf", "title": "MindAgent: Emergent Gaming Interaction", "year": 2023, "paper_venue": null, "authors": ["Ran Gong", "Qiuyuan Huang", "Xiaojian Ma", "Hoi Vo", "Zane Durante", "Yusuke Noda", "Zilong Zheng", "Song-Chun Zhu", "Demetri Terzopoulos", "Li Fei-Fei", "Jianfeng Gao"], "abstract": "Large Language Models (LLMs) have the capacity of performing complex scheduling in a multi-agent system and can coordinate these agents into completing sophisticated tasks that require extensive collaboration. However, despite the introduction of numerous gaming frameworks, the community has insufficient benchmarks towards building general multi-agents collaboration infrastructure that encompass both LLM and human-NPCs collaborations. In this work, we propose a novel infrastructure - MindAgent - to evaluate planning and coordination emergent capabilities for gaming interaction. In particular, our infrastructure leverages existing gaming framework, to i) require understanding of the coordinator for a multi-agent system, ii) collaborate with human players via un-finetuned proper instructions, and iii) establish an in-context learning on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new gaming scenario and related benchmark that dispatch a multi-agent collaboration efficiency and supervise multiple agents playing the game simultaneously. We conduct comprehensive evaluations with new auto-metric CoS for calculating the collaboration efficiency. Finally, our infrastructure can be deployed into real-world gaming scenarios in a customized VR version of CUISINEWORLD and adapted in existing broader Minecraft gaming domain. We hope our findings on LLMs and the new infrastructure for general-purpose scheduling and coordination can help shed light on how such skills can be obtained by learning from large language corpora.", "comments": "The first three authors contributed equally. 28 pages", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/mindagent-emergent-gaming-interaction", "bibtex": "@misc{gong2023mindagent,\n      title={MindAgent: Emergent Gaming Interaction}, \n      author={Ran Gong and Qiuyuan Huang and Xiaojian Ma and Hoi Vo and Zane Durante and Yusuke Noda and Zilong Zheng and Song-Chun Zhu and Demetri Terzopoulos and Li Fei-Fei and Jianfeng Gao},\n      year={2023},\n      eprint={2309.09971},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "18-09-2023", "categories": ["cs.AI", "cs.HC", "cs.MA"]}, "2312.12682v1": {"paper_id": "2312.12682v1", "abs_url": "https://arxiv.org/abs/2312.12682v1", "pdf_url": "https://arxiv.org/pdf/2312.12682v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.12682v1_Mini-GPTs_Efficient_Large_Language_Models_through_Contextual_Pruning.pdf", "title": "Mini-GPTs: Efficient Large Language Models through Contextual Pruning", "year": 2023, "paper_venue": null, "authors": ["Tim Valicenti", "Justice Vidal", "Ritik Patnaik"], "abstract": "In AI research, the optimization of Large Language Models (LLMs) remains a significant challenge, crucial for advancing the field's practical applications and sustainability. Building upon the foundational work of Professor Song Han's lab at MIT, this paper introduces a novel approach in developing Mini-GPTs via contextual pruning. Our methodology strategically prunes the computational architecture of traditional LLMs, like Phi-1.5, focusing on retaining core functionalities while drastically reducing model sizes. We employ the technique across diverse and complex datasets, including US law, Medical Q&A, Skyrim dialogue, English-Taiwanese translation, and Economics articles. The results underscore the efficiency and effectiveness of contextual pruning, not merely as a theoretical concept but as a practical tool in developing domain-specific, resource-efficient LLMs. Contextual pruning is a promising method for building domain-specific LLMs, and this research is a building block towards future development with more hardware compute, refined fine-tuning, and quantization.", "comments": "7 pages, 4 figures, Neurips 2023 styling", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/mini-gpts-efficient-large-language-models", "bibtex": "@misc{valicenti2023minigpts,\n      title={Mini-GPTs: Efficient Large Language Models through Contextual Pruning}, \n      author={Tim Valicenti and Justice Vidal and Ritik Patnaik},\n      year={2023},\n      eprint={2312.12682},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "20-12-2023", "categories": ["cs.CL", "cs.AI", "I.2.7"]}, "2310.09478v3": {"paper_id": "2310.09478v3", "abs_url": "https://arxiv.org/abs/2310.09478v3", "pdf_url": "https://arxiv.org/pdf/2310.09478v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.09478v3_MiniGPT-v2_large_language_model_as_a_unified_interface_for_vision-language_multi-task_learning.pdf", "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning", "year": 2023, "paper_venue": null, "authors": ["Jun Chen", "Deyao Zhu", "Xiaoqian Shen", "Xiang Li", "Zechun Liu", "Pengchuan Zhang", "Raghuraman Krishnamoorthi", "Vikas Chandra", "Yunyang Xiong", "Mohamed Elhoseiny"], "abstract": "", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/minigpt-v2-large-language-model-as-a-unified", "bibtex": "@misc{chen2023minigptv2,\n      title={MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning}, \n      author={Jun Chen and Deyao Zhu and Xiaoqian Shen and Xiang Li and Zechun Liu and Pengchuan Zhang and Raghuraman Krishnamoorthi and Vikas Chandra and Yunyang Xiong and Mohamed Elhoseiny},\n      year={2023},\n      eprint={2310.09478},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "14-10-2023", "categories": ["cs.CV"]}, "2312.16886v2": {"paper_id": "2312.16886v2", "abs_url": "https://arxiv.org/abs/2312.16886v2", "pdf_url": "https://arxiv.org/pdf/2312.16886v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.16886v2_MobileVLM__A_Fast_Strong_and_Open_Vision_Language_Assistant_for_Mobile_Devices.pdf", "title": "MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices", "year": 2023, "paper_venue": null, "authors": ["Xiangxiang Chu", "Limeng Qiao", "Xinyang Lin", "Shuang Xu", "Yang Yang", "Yiming Hu", "Fei Wei", "Xinyu Zhang", "Bo Zhang", "Xiaolin Wei", "Chunhua Shen"], "abstract": ".", "comments": "Tech Report", "official_code_urls": ["https://github.com/meituan-automl/mobilevlm"], "pwc_page_url": "https://paperswithcode.com/paper/mobilevlm-a-fast-reproducible-and-strong", "bibtex": "@misc{chu2023mobilevlm,\n      title={MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices}, \n      author={Xiangxiang Chu and Limeng Qiao and Xinyang Lin and Shuang Xu and Yang Yang and Yiming Hu and Fei Wei and Xinyu Zhang and Bo Zhang and Xiaolin Wei and Chunhua Shen},\n      year={2023},\n      eprint={2312.16886},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "28-12-2023", "categories": ["cs.CV"]}, "2305.16896v1": {"paper_id": "2305.16896v1", "abs_url": "https://arxiv.org/abs/2305.16896v1", "pdf_url": "https://arxiv.org/pdf/2305.16896v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.16896v1_MultiTool-CoT_GPT-3_Can_Use_Multiple_External_Tools_with_Chain_of_Thought_Prompting.pdf", "title": "MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting", "year": 2023, "paper_venue": null, "authors": ["Tatsuro Inaba", "Hirokazu Kiyomaru", "Fei Cheng", "Sadao Kurohashi"], "abstract": "Large language models (LLMs) have achieved impressive performance on various reasoning tasks. To further improve the performance, we propose MultiTool-CoT, a novel framework that leverages chain-of-thought (CoT) prompting to incorporate multiple external tools, such as a calculator and a knowledge retriever, during the reasoning process. We apply MultiTool-CoT to the Task 2 dataset of NumGLUE, which requires both numerical reasoning and domain-specific knowledge. The experiments show that our method significantly outperforms strong baselines and achieves state-of-the-art performance.", "comments": "ACL2023. Our code is available at this https URL", "official_code_urls": ["https://github.com/inabatatsuro/multitool-cot"], "pwc_page_url": "https://paperswithcode.com/paper/multitool-cot-gpt-3-can-use-multiple-external", "bibtex": "@misc{inaba2023multitoolcot,\n      title={MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting}, \n      author={Tatsuro Inaba and Hirokazu Kiyomaru and Fei Cheng and Sadao Kurohashi},\n      year={2023},\n      eprint={2305.16896},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "26-05-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2303.02861v1": {"paper_id": "2303.02861v1", "abs_url": "https://arxiv.org/abs/2303.02861v1", "pdf_url": "https://arxiv.org/pdf/2303.02861v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2303.02861v1_Multitask_Prompt_Tuning_Enables_Parameter-Efficient_Transfer_Learning.pdf", "title": "Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning", "year": 2023, "paper_venue": null, "authors": ["Zhen Wang", "Rameswar Panda", "Leonid Karlinsky", "Rogerio Feris", "Huan Sun", "Yoon Kim"], "abstract": "Prompt tuning, in which a base pretrained model is adapted to each task via conditioning on learned prompt vectors, has emerged as a promising approach for efficiently adapting large language models to multiple downstream tasks. However, existing methods typically learn soft prompt vectors from scratch, and it has not been clear how to exploit the rich cross-task knowledge with prompt vectors in a multitask learning setting. We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts. We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task. Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases, despite only tuning 0.035% as many task-specific parameters.", "comments": "ICLR 2023. Project page: this https URL", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/multitask-prompt-tuning-enables-parameter", "bibtex": "@misc{wang2023multitask,\n      title={Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning}, \n      author={Zhen Wang and Rameswar Panda and Leonid Karlinsky and Rogerio Feris and Huan Sun and Yoon Kim},\n      year={2023},\n      eprint={2303.02861},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "06-03-2023", "categories": ["cs.CL"]}, "2303.14725v2": {"paper_id": "2303.14725v2", "abs_url": "https://arxiv.org/abs/2303.14725v2", "pdf_url": "https://arxiv.org/pdf/2303.14725v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2303.14725v2_Natural_Language_Reasoning_A_Survey.pdf", "title": "Natural Language Reasoning, A Survey", "year": 2023, "paper_venue": null, "authors": ["Fei Yu", "Hongbo Zhang", "Prayag Tiwari", "Benyou Wang"], "abstract": "This survey paper proposes a clearer view of natural language reasoning in the field of Natural Language Processing (NLP), both conceptually and practically. Conceptually, we provide a distinct definition for natural language reasoning in NLP, based on both philosophy and NLP scenarios, discuss what types of tasks require reasoning, and introduce a taxonomy of reasoning. Practically, we conduct a comprehensive literature review on natural language reasoning in NLP, mainly covering classical logical reasoning, natural language inference, multi-hop question answering, and commonsense reasoning. The paper also identifies and views backward reasoning, a powerful paradigm for multi-step reasoning, and introduces defeasible reasoning as one of the most important future directions in natural language reasoning research. We focus on single-modality unstructured natural language text, excluding neuro-symbolic techniques and mathematical reasoning.", "comments": "this https URL", "official_code_urls": ["https://github.com/freedomintelligence/reasoningnlp"], "pwc_page_url": "https://paperswithcode.com/paper/nature-language-reasoning-a-survey", "bibtex": "@misc{yu2023natural,\n      title={Natural Language Reasoning, A Survey}, \n      author={Fei Yu and Hongbo Zhang and Prayag Tiwari and Benyou Wang},\n      year={2023},\n      eprint={2303.14725},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "26-03-2023", "categories": ["cs.CL"]}, "2310.15959v2": {"paper_id": "2310.15959v2", "abs_url": "https://arxiv.org/abs/2310.15959v2", "pdf_url": "https://arxiv.org/pdf/2310.15959v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.15959v2_NoteChat_A_Dataset_of_Synthetic_Doctor-Patient_Conversations_Conditioned_on_Clinical_Notes.pdf", "title": "NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes", "year": 2023, "paper_venue": null, "authors": ["Junda Wang", "Zonghai Yao", "Zhichao Yang", "Huixue Zhou", "Rumeng Li", "Xun Wang", "Yucheng Xu", "Hong Yu"], "abstract": "We introduce NoteChat, a novel cooperative multi-agent framework leveraging Large Language Models (LLMs) to generate patient-physician dialogues. NoteChat embodies the principle that an ensemble of role-specific LLMs, through structured role-play and strategic prompting, can perform their assigned roles more effectively. The synergy among these role-playing LLMs results in a cohesive and efficient dialogue generation. Evaluation on MTS-dialogue, a benchmark dataset for patient-physician dialogues-note pairs, shows that models trained with the augmented synthetic patient-physician dialogues by NoteChat outperforms other state-of-the-art models for generating clinical notes. Our comprehensive automatic and human evaluation demonstrates that NoteChat substantially surpasses state-of-the-art models like ChatGPT and GPT-4 up to 22.78% by domain experts in generating superior synthetic patient-physician dialogues based on clinical notes. NoteChat has the potential to engage patients directly and help clinical documentation, a leading cause of physician burnout.", "comments": "", "official_code_urls": ["https://github.com/believewhat/dr.noteaid"], "pwc_page_url": "https://paperswithcode.com/paper/notechat-a-dataset-of-synthetic-doctor", "bibtex": "@misc{wang2023notechat,\n      title={NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes}, \n      author={Junda Wang and Zonghai Yao and Zhichao Yang and Huixue Zhou and Rumeng Li and Xun Wang and Yucheng Xu and Hong Yu},\n      year={2023},\n      eprint={2310.15959},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "24-10-2023", "categories": ["cs.CL"]}, "2309.09298v1": {"paper_id": "2309.09298v1", "abs_url": "https://arxiv.org/abs/2309.09298v1", "pdf_url": "https://arxiv.org/pdf/2309.09298v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.09298v1_OWL_A_Large_Language_Model_for_IT_Operations.pdf", "title": "OWL: A Large Language Model for IT Operations", "year": 2023, "paper_venue": null, "authors": ["Hongcheng Guo", "Jian Yang", "Jiaheng Liu", "Liqun Yang", "Linzheng Chai", "Jiaqi Bai", "Junran Peng", "Xiaorong Hu", "Chao Chen", "Dongfeng Zhang", "Xu Shi", "Tieqiao Zheng", "Liangfan Zheng", "Bo Zhang", "Ke Xu", "Zhoujun Li"], "abstract": "With the rapid development of IT operations, it has become increasingly crucial to efficiently manage and analyze large volumes of data for practical applications. The techniques of Natural Language Processing (NLP) have shown remarkable capabilities for various tasks, including named entity recognition, machine translation and dialogue systems. Recently, Large Language Models (LLMs) have achieved significant improvements across various NLP downstream tasks. However, there is a lack of specialized LLMs for IT operations. In this paper, we introduce the OWL, a large language model trained on our collected OWL-Instruct dataset with a wide range of IT-related information, where the mixture-of-adapter strategy is proposed to improve the parameter-efficient tuning across different domains or tasks. Furthermore, we evaluate the performance of our OWL on the OWL-Bench established by us and open IT-related benchmarks. OWL demonstrates superior performance results on IT tasks, which outperforms existing models by significant margins. Moreover, we hope that the findings of our work will provide more insights to revolutionize the techniques of IT operations with specialized LLMs.", "comments": "31 pages", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/owl-a-large-language-model-for-it-operations", "bibtex": "@misc{guo2023owl,\n      title={OWL: A Large Language Model for IT Operations}, \n      author={Hongcheng Guo and Jian Yang and Jiaheng Liu and Liqun Yang and Linzheng Chai and Jiaqi Bai and Junran Peng and Xiaorong Hu and Chao Chen and Dongfeng Zhang and Xu Shi and Tieqiao Zheng and Liangfan Zheng and Bo Zhang and Ke Xu and Zhoujun Li},\n      year={2023},\n      eprint={2309.09298},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "17-09-2023", "categories": ["cs.CL"]}, "2304.06488v1": {"paper_id": "2304.06488v1", "abs_url": "https://arxiv.org/abs/2304.06488v1", "pdf_url": "https://arxiv.org/pdf/2304.06488v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2304.06488v1_One_Small_Step_for_Generative_AI_One_Giant_Leap_for_AGI_A_Complete_Survey_on_ChatGPT_in_AIGC_Era.pdf", "title": "One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era", "year": 2023, "paper_venue": null, "authors": ["Chaoning Zhang", "Chenshuang Zhang", "Chenghao Li", "Yu Qiao", "Sheng Zheng", "Sumit Kumar Dam", "Mengchun Zhang", "Jung Uk Kim", "Seong Tae Kim", "Jinwoo Choi", "Gyeong-Moon Park", "Sung-Ho Bae", "Lik-Hang Lee", "Pan Hui", "In So Kweon", "Choong Seon Hong"], "abstract": "OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI.", "comments": "A Survey on ChatGPT and GPT-4, 29 pages. Feedback is appreciated (chaoningzhang1990@gmail.com)", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/one-small-step-for-generative-ai-one-giant", "bibtex": "@misc{zhang2023small,\n      title={One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era}, \n      author={Chaoning Zhang and Chenshuang Zhang and Chenghao Li and Yu Qiao and Sheng Zheng and Sumit Kumar Dam and Mengchun Zhang and Jung Uk Kim and Seong Tae Kim and Jinwoo Choi and Gyeong-Moon Park and Sung-Ho Bae and Lik-Hang Lee and Pan Hui and In So Kweon and Choong Seon Hong},\n      year={2023},\n      eprint={2304.06488},\n      archivePrefix={arXiv},\n      primaryClass={cs.CY}\n}", "published_date": "04-04-2023", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV", "cs.LG"]}, "2304.07327v2": {"paper_id": "2304.07327v2", "abs_url": "https://arxiv.org/abs/2304.07327v2", "pdf_url": "https://arxiv.org/pdf/2304.07327v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2304.07327v2_OpenAssistant_Conversations_--_Democratizing_Large_Language_Model_Alignment.pdf", "title": "OpenAssistant Conversations -- Democratizing Large Language Model Alignment", "year": 2023, "paper_venue": null, "authors": ["Andreas K\u00f6pf", "Yannic Kilcher", "Dimitri von R\u00fctte", "Sotiris Anagnostidis", "Zhi-Rui Tam", "Keith Stevens", "Abdullah Barhoum", "Nguyen Minh Duc", "Oliver Stanley", "Rich\u00e1rd Nagyfi", "Shahul ES", "Sameer Suri", "David Glushkov", "Arnav Dantuluri", "Andrew Maguire", "Christoph Schuhmann", "Huu Nguyen", "Alexander Mattick"], "abstract": "Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models. We release our code and data under a fully permissive licence.", "comments": "Published in NeurIPS 2023 Datasets and Benchmarks", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/openassistant-conversations-democratizing", "bibtex": "@misc{k\u00f6pf2023openassistant,\n      title={OpenAssistant Conversations -- Democratizing Large Language Model Alignment}, \n      author={Andreas K\u00f6pf and Yannic Kilcher and Dimitri von R\u00fctte and Sotiris Anagnostidis and Zhi-Rui Tam and Keith Stevens and Abdullah Barhoum and Nguyen Minh Duc and Oliver Stanley and Rich\u00e1rd Nagyfi and Shahul ES and Sameer Suri and David Glushkov and Arnav Dantuluri and Andrew Maguire and Christoph Schuhmann and Huu Nguyen and Alexander Mattick},\n      year={2023},\n      eprint={2304.07327},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "14-04-2023", "categories": ["cs.CL", "cs.AI"]}, "2309.11235v1": {"paper_id": "2309.11235v1", "abs_url": "https://arxiv.org/abs/2309.11235v1", "pdf_url": "https://arxiv.org/pdf/2309.11235v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.11235v1_OpenChat_Advancing_Open-source_Language_Models_with_Mixed-Quality_Data.pdf", "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data", "year": 2023, "paper_venue": null, "authors": ["Guan Wang", "Sijie Cheng", "Xianyuan Zhan", "Xiangang Li", "Sen Song", "Yang Liu"], "abstract": ".", "comments": "", "official_code_urls": ["https://github.com/imoneoi/openchat"], "pwc_page_url": "https://paperswithcode.com/paper/openchat-advancing-open-source-language", "bibtex": "@misc{wang2023openchat,\n      title={OpenChat: Advancing Open-source Language Models with Mixed-Quality Data}, \n      author={Guan Wang and Sijie Cheng and Xianyuan Zhan and Xiangang Li and Sen Song and Yang Liu},\n      year={2023},\n      eprint={2309.11235},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "20-09-2023", "categories": ["cs.CL"]}, "2401.00905v1": {"paper_id": "2401.00905v1", "abs_url": "https://arxiv.org/abs/2401.00905v1", "pdf_url": "https://arxiv.org/pdf/2401.00905v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.00905v1_Opening_A_Pandoras_Box_Things_You_Should_Know_in_the_Era_of_Custom_GPTs.pdf", "title": "Opening A Pandora's Box: Things You Should Know in the Era of Custom GPTs", "year": 2023, "paper_venue": null, "authors": ["Guanhong Tao", "Siyuan Cheng", "Zhuo Zhang", "Junmin Zhu", "Guangyu Shen", "Xiangyu Zhang"], "abstract": "The emergence of large language models (LLMs) has significantly accelerated the development of a wide range of applications across various fields. There is a growing trend in the construction of specialized platforms based on LLMs, such as the newly introduced custom GPTs by OpenAI. While custom GPTs provide various functionalities like web browsing and code execution, they also introduce significant security threats. In this paper, we conduct a comprehensive analysis of the security and privacy issues arising from the custom GPT platform. Our systematic examination categorizes potential attack scenarios into three threat models based on the role of the malicious actor, and identifies critical data exchange channels in custom GPTs. Utilizing the STRIDE threat modeling framework, we identify 26 potential attack vectors, with 19 being partially or fully validated in real-world settings. Our findings emphasize the urgent need for robust security and privacy measures in the custom GPT ecosystem, especially in light of the forthcoming launch of the official GPT store by OpenAI.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/opening-a-pandora-s-box-things-you-should", "bibtex": "@misc{tao2023opening,\n      title={Opening A Pandora's Box: Things You Should Know in the Era of Custom GPTs}, \n      author={Guanhong Tao and Siyuan Cheng and Zhuo Zhang and Junmin Zhu and Guangyu Shen and Xiangyu Zhang},\n      year={2023},\n      eprint={2401.00905},\n      archivePrefix={arXiv},\n      primaryClass={cs.CR}\n}", "published_date": "31-12-2023", "categories": ["cs.CR"]}, "2311.11045v2": {"paper_id": "2311.11045v2", "abs_url": "https://arxiv.org/abs/2311.11045v2", "pdf_url": "https://arxiv.org/pdf/2311.11045v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.11045v2_Orca_2_Teaching_Small_Language_Models_How_to_Reason.pdf", "title": "Orca 2: Teaching Small Language Models How to Reason", "year": 2023, "paper_venue": null, "authors": ["Arindam Mitra", "Luciano Del Corro", "Shweti Mahajan", "Andres Codas", "Clarisse Simoes", "Sahaj Agarwal", "Xuxi Chen", "Anastasia Razdaibiedina", "Erik Jones", "Kriti Aggarwal", "Hamid Palangi", "Guoqing Zheng", "Corby Rosset", "Hamed Khanpour", "Ahmed Awadallah"], "abstract": "to support research on the development, evaluation, and alignment of smaller LMs", "comments": "Added url to model weights fixed typo in Author name", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/orca-2-teaching-small-language-models-how-to", "bibtex": "@misc{mitra2023orca,\n      title={Orca 2: Teaching Small Language Models How to Reason}, \n      author={Arindam Mitra and Luciano Del Corro and Shweti Mahajan and Andres Codas and Clarisse Simoes and Sahaj Agarwal and Xuxi Chen and Anastasia Razdaibiedina and Erik Jones and Kriti Aggarwal and Hamid Palangi and Guoqing Zheng and Corby Rosset and Hamed Khanpour and Ahmed Awadallah},\n      year={2023},\n      eprint={2311.11045},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "18-11-2023", "categories": ["cs.AI"]}, "2309.08872v2": {"paper_id": "2309.08872v2", "abs_url": "https://arxiv.org/abs/2309.08872v2", "pdf_url": "https://arxiv.org/pdf/2309.08872v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.08872v2_PDFTriage_Question_Answering_over_Long_Structured_Documents.pdf", "title": "PDFTriage: Question Answering over Long, Structured Documents", "year": 2023, "paper_venue": null, "authors": ["Jon Saad-Falcon", "Joe Barrow", "Alexa Siu", "Ani Nenkova", "David Seunghyun Yoon", "Ryan A. Rossi", "Franck Dernoncourt"], "abstract": "Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmented models across several classes of questions where existing retrieval-augmented LLMs fail. To facilitate further research on this fundamental problem, we release our benchmark dataset consisting of 900+ human-generated questions over 80 structured documents from 10 different categories of question types for document QA. Our code and datasets will be released soon on Github.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/pdftriage-question-answering-over-long", "bibtex": "@misc{saadfalcon2023pdftriage,\n      title={PDFTriage: Question Answering over Long, Structured Documents}, \n      author={Jon Saad-Falcon and Joe Barrow and Alexa Siu and Ani Nenkova and David Seunghyun Yoon and Ryan A. Rossi and Franck Dernoncourt},\n      year={2023},\n      eprint={2309.08872},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "16-09-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2312.12148v1": {"paper_id": "2312.12148v1", "abs_url": "https://arxiv.org/abs/2312.12148v1", "pdf_url": "https://arxiv.org/pdf/2312.12148v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.12148v1_Parameter-Efficient_Fine-Tuning_Methods_for_Pretrained_Language_Models_A_Critical_Review_and_Assessment.pdf", "title": "Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment", "year": 2023, "paper_venue": null, "authors": ["Lingling Xu", "Haoran Xie", "Si-Zhao Joe Qin", "Xiaohui Tao", "Fu Lee Wang"], "abstract": "With the continuous growth in the number of parameters of transformer-based pretrained language models (PLMs), particularly the emergence of large language models (LLMs) with billions of parameters, many natural language processing (NLP) tasks have demonstrated remarkable success. However, the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning. The demands for fine-tuning PLMs, especially LLMs, have led to a surge in the development of PEFT methods, as depicted in Fig. 1. In this paper, we present a comprehensive and systematic review of PEFT methods for PLMs. We summarize these PEFT methods, discuss their applications, and outline future directions. Furthermore, we conduct experiments using several representative PEFT methods to better understand their effectiveness in parameter efficiency and memory efficiency. By offering insights into the latest advancements and practical applications, this survey serves as an invaluable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by PEFT in the context of PLMs.", "comments": "20 pages, 4 figures", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/parameter-efficient-fine-tuning-methods-for", "bibtex": "@misc{xu2023parameterefficient,\n      title={Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment}, \n      author={Lingling Xu and Haoran Xie and Si-Zhao Joe Qin and Xiaohui Tao and Fu Lee Wang},\n      year={2023},\n      eprint={2312.12148},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "19-12-2023", "categories": ["cs.CL"]}, "2305.02412v2": {"paper_id": "2305.02412v2", "abs_url": "https://arxiv.org/abs/2305.02412v2", "pdf_url": "https://arxiv.org/pdf/2305.02412v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.02412v2_Plan_Eliminate_and_Track_--_Language_Models_are_Good_Teachers_for_Embodied_Agents.pdf", "title": "Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents", "year": 2023, "paper_venue": null, "authors": ["Yue Wu", "So Yeon Min", "Yonatan Bisk", "Ruslan Salakhutdinov", "Amos Azaria", "Yuanzhi Li", "Tom Mitchell", "Shrimai Prabhumoye"], "abstract": "Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/plan-eliminate-and-track-language-models-are", "bibtex": "@misc{wu2023plan,\n      title={Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents}, \n      author={Yue Wu and So Yeon Min and Yonatan Bisk and Ruslan Salakhutdinov and Amos Azaria and Yuanzhi Li and Tom Mitchell and Shrimai Prabhumoye},\n      year={2023},\n      eprint={2305.02412},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "03-05-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2309.10238v1": {"paper_id": "2309.10238v1", "abs_url": "https://arxiv.org/abs/2309.10238v1", "pdf_url": "https://arxiv.org/pdf/2309.10238v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.10238v1_PolicyGPT_Automated_Analysis_of_Privacy_Policies_with_Large_Language_Models.pdf", "title": "PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Chenhao Tang", "Zhengliang Liu", "Chong Ma", "Zihao Wu", "Yiwei Li", "Wei Liu", "Dajiang Zhu", "Quanzheng Li", "Xiang Li", "Tianming Liu", "Lei Fan"], "abstract": "Privacy policies serve as the primary conduit through which online service providers inform users about their data collection and usage procedures. However, in a bid to be comprehensive and mitigate legal risks, these policy documents are often quite verbose. In practical use, users tend to click the Agree button directly rather than reading them carefully. This practice exposes users to risks of privacy leakage and legal issues. Recently, the advent of Large Language Models (LLM) such as ChatGPT and GPT-4 has opened new possibilities for text analysis, especially for lengthy documents like privacy policies. In this study, we investigate a privacy policy text analysis framework PolicyGPT based on the LLM. This framework was tested using two datasets. The first dataset comprises of privacy policies from 115 websites, which were meticulously annotated by legal experts, categorizing each segment into one of 10 classes. The second dataset consists of privacy policies from 304 popular mobile applications, with each sentence manually annotated and classified into one of another 10 categories. Under zero-shot learning conditions, PolicyGPT demonstrated robust performance. For the first dataset, it achieved an accuracy rate of 97%, while for the second dataset, it attained an 87% accuracy rate, surpassing that of the baseline machine learning and neural network models.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/policygpt-automated-analysis-of-privacy", "bibtex": "@misc{tang2023policygpt,\n      title={PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models}, \n      author={Chenhao Tang and Zhengliang Liu and Chong Ma and Zihao Wu and Yiwei Li and Wei Liu and Dajiang Zhu and Quanzheng Li and Xiang Li and Tianming Liu and Lei Fan},\n      year={2023},\n      eprint={2309.10238},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "19-09-2023", "categories": ["cs.CL"]}, "2312.12456v1": {"paper_id": "2312.12456v1", "abs_url": "https://arxiv.org/abs/2312.12456v1", "pdf_url": "https://arxiv.org/pdf/2312.12456v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.12456v1_PowerInfer_Fast_Large_Language_Model_Serving_with_a_Consumer-grade_GPU.pdf", "title": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU", "year": 2023, "paper_venue": null, "authors": ["Yixin Song", "Zeyu Mi", "Haotong Xie", "Haibo Chen"], "abstract": "This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high locality inherent in LLM inference, characterized by a power-law distribution in neuron activation. This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity. Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU, only 18% lower than that achieved by a top-tier server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.", "comments": "15 pages, 18 figures", "official_code_urls": ["https://github.com/sjtu-ipads/powerinfer"], "pwc_page_url": "https://paperswithcode.com/paper/powerinfer-fast-large-language-model-serving", "bibtex": "@misc{song2023powerinfer,\n      title={PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU}, \n      author={Yixin Song and Zeyu Mi and Haotong Xie and Haibo Chen},\n      year={2023},\n      eprint={2312.12456},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "16-12-2023", "categories": ["cs.LG", "cs.OS"]}, "2312.16171v2": {"paper_id": "2312.16171v2", "abs_url": "https://arxiv.org/abs/2312.16171v2", "pdf_url": "https://arxiv.org/pdf/2312.16171v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.16171v2_Principled_Instructions_Are_All_You_Need_for_Questioning_LLaMA-12_GPT-354.pdf", "title": "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4", "year": 2023, "paper_venue": null, "authors": ["Sondos Mahmoud Bsharat", "Aidar Myrzakhan", "Zhiqiang Shen"], "abstract": ".", "comments": "Github at: this https URL", "official_code_urls": ["https://github.com/vila-lab/atlas"], "pwc_page_url": "https://paperswithcode.com/paper/principled-instructions-are-all-you-need-for", "bibtex": "@misc{bsharat2024principled,\n      title={Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4}, \n      author={Sondos Mahmoud Bsharat and Aidar Myrzakhan and Zhiqiang Shen},\n      year={2024},\n      eprint={2312.16171},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "26-12-2023", "categories": ["cs.CL", "cs.AI"]}, "2309.10359v1": {"paper_id": "2309.10359v1", "abs_url": "https://arxiv.org/abs/2309.10359v1", "pdf_url": "https://arxiv.org/pdf/2309.10359v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.10359v1_Prompt_Condition_and_Generate_Classification_of_Unsupported_Claims_with_In-Context_Learning.pdf", "title": "Prompt, Condition, and Generate: Classification of Unsupported Claims with In-Context Learning", "year": 2023, "paper_venue": null, "authors": ["Peter Ebert Christensen", "Srishti Yadav", "Serge Belongie"], "abstract": "Unsupported and unfalsifiable claims we encounter in our daily lives can influence our view of the world. Characterizing, summarizing, and -- more generally -- making sense of such claims, however, can be challenging. In this work, we focus on fine-grained debate topics and formulate a new task of distilling, from such claims, a countable set of narratives. We present a crowdsourced dataset of 12 controversial topics, comprising more than 120k arguments, claims, and comments from heterogeneous sources, each annotated with a narrative label. We further investigate how large language models (LLMs) can be used to synthesise claims using In-Context Learning. We find that generated claims with supported evidence can be used to improve the performance of narrative classification models and, additionally, that the same model can infer the stance and aspect using a few training examples. Such a model can be useful in applications which rely on narratives , e.g. fact-checking.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/prompt-condition-and-generate-classification", "bibtex": "@misc{christensen2023prompt,\n      title={Prompt, Condition, and Generate: Classification of Unsupported Claims with In-Context Learning}, \n      author={Peter Ebert Christensen and Srishti Yadav and Serge Belongie},\n      year={2023},\n      eprint={2309.10359},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "19-09-2023", "categories": ["cs.CL"]}, "2312.07910v2": {"paper_id": "2312.07910v2", "abs_url": "https://arxiv.org/abs/2312.07910v2", "pdf_url": "https://arxiv.org/pdf/2312.07910v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.07910v2_PromptBench_A_Unified_Library_for_Evaluation_of_Large_Language_Models.pdf", "title": "PromptBench: A Unified Library for Evaluation of Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Kaijie Zhu", "Qinlin Zhao", "Hao Chen", "Jindong Wang", "Xing Xie"], "abstract": "and will be continuously supported.", "comments": "An extension to PromptBench ( arXiv:2306.04528 ) for unified evaluation of LLMs using the same name; code: this https URL", "official_code_urls": ["https://github.com/microsoft/promptbench"], "pwc_page_url": "https://paperswithcode.com/paper/promptbench-a-unified-library-for-evaluation", "bibtex": "@misc{zhu2024promptbench,\n      title={PromptBench: A Unified Library for Evaluation of Large Language Models}, \n      author={Kaijie Zhu and Qinlin Zhao and Hao Chen and Jindong Wang and Xing Xie},\n      year={2024},\n      eprint={2312.07910},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "13-12-2023", "categories": ["cs.AI", "cs.CL", "cs.LG"]}, "2401.00127v1": {"paper_id": "2401.00127v1", "abs_url": "https://arxiv.org/abs/2401.00127v1", "pdf_url": "https://arxiv.org/pdf/2401.00127v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2401.00127v1_Pushing_Boundaries_Exploring_Zero_Shot_Object_Classification_with_Large_Multimodal_Models.pdf", "title": "Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models", "year": 2023, "paper_venue": null, "authors": ["Ashhadul Islam", "Md. Rafiul Biswas", "Wajdi Zaghouani", "Samir Brahim Belhaouari", "Zubair Shah"], "abstract": "$ $The synergy of language and vision models has given rise to Large Language and Vision Assistant models (LLVAs), designed to engage users in rich conversational experiences intertwined with image-based queries. These comprehensive multimodal models seamlessly integrate vision encoders with Large Language Models (LLMs), expanding their applications in general-purpose language and visual comprehension. The advent of Large Multimodal Models (LMMs) heralds a new era in Artificial Intelligence (AI) assistance, extending the horizons of AI utilization. This paper takes a unique perspective on LMMs, exploring their efficacy in performing image classification tasks using tailored prompts designed for specific datasets. We also investigate the LLVAs zero-shot learning capabilities. Our study includes a benchmarking analysis across four diverse datasets: MNIST, Cats Vs. Dogs, Hymnoptera (Ants Vs. Bees), and an unconventional dataset comprising Pox Vs. Non-Pox skin images. The results of our experiments demonstrate the model's remarkable performance, achieving classification accuracies of 85\\%, 100\\%, 77\\%, and 79\\% for the respective datasets without any fine-tuning. To bolster our analysis, we assess the model's performance post fine-tuning for specific tasks. In one instance, fine-tuning is conducted over a dataset comprising images of faces of children with and without autism. Prior to fine-tuning, the model demonstrated a test accuracy of 55\\%, which significantly improved to 83\\% post fine-tuning. These results, coupled with our prior findings, underscore the transformative potential of LLVAs and their versatile applications in real-world scenarios.", "comments": "5 pages,6 figures, 4 tables, Accepted on The International Symposium on Foundation and Large Language Models (FLLM2023)", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/pushing-boundaries-exploring-zero-shot-object", "bibtex": "@misc{islam2023pushing,\n      title={Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models}, \n      author={Ashhadul Islam and Md. Rafiul Biswas and Wajdi Zaghouani and Samir Brahim Belhaouari and Zubair Shah},\n      year={2023},\n      eprint={2401.00127},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "30-12-2023", "categories": ["cs.CV", "cs.SI"]}, "2304.01373v2": {"paper_id": "2304.01373v2", "abs_url": "https://arxiv.org/abs/2304.01373v2", "pdf_url": "https://arxiv.org/pdf/2304.01373v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2304.01373v2_Pythia_A_Suite_for_Analyzing_Large_Language_Models_Across_Training_and_Scaling.pdf", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling", "year": 2023, "paper_venue": null, "authors": ["Stella Biderman", "Hailey Schoelkopf", "Quentin Anthony", "Herbie Bradley", "Kyle O'Brien", "Eric Hallahan", "Mohammad Aflah Khan", "Shivanshu Purohit", "USVSN Sai Prashanth", "Edward Raff", "Aviya Skowron", "Lintang Sutawika", "Oskar van der Wal"], "abstract": "}.", "comments": "Code at this https URL", "official_code_urls": ["https://github.com/eleutherai/gpt-neox", "https://github.com/eleutherai/pythia"], "pwc_page_url": "https://paperswithcode.com/paper/pythia-a-suite-for-analyzing-large-language", "bibtex": "@misc{biderman2023pythia,\n      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, \n      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},\n      year={2023},\n      eprint={2304.01373},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "03-04-2023", "categories": ["cs.CL"]}, "2305.14314v1": {"paper_id": "2305.14314v1", "abs_url": "https://arxiv.org/abs/2305.14314v1", "pdf_url": "https://arxiv.org/pdf/2305.14314v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.14314v1_QLoRA_Efficient_Finetuning_of_Quantized_LLMs.pdf", "title": "QLoRA: Efficient Finetuning of Quantized LLMs", "year": 2023, "paper_venue": null, "authors": ["Tim Dettmers", "Artidoro Pagnoni", "Ari Holtzman", "Luke Zettlemoyer"], "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.", "comments": "Extended NeurIPS submission", "official_code_urls": ["https://github.com/artidoro/qlora"], "pwc_page_url": "https://paperswithcode.com/paper/qlora-efficient-finetuning-of-quantized-llms", "bibtex": "@misc{dettmers2023qlora,\n      title={QLoRA: Efficient Finetuning of Quantized LLMs}, \n      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},\n      year={2023},\n      eprint={2305.14314},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "23-05-2023", "categories": ["cs.LG"]}, "2309.06553v3": {"paper_id": "2309.06553v3", "abs_url": "https://arxiv.org/abs/2309.06553v3", "pdf_url": "https://arxiv.org/pdf/2309.06553v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.06553v3_Query-Dependent_Prompt_Evaluation_and_Optimization_with_Offline_Inverse_RL.pdf", "title": "Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL", "year": 2023, "paper_venue": null, "authors": ["Hao Sun", "Alihan H\u00fcy\u00fck", "Mihaela van der Schaar"], "abstract": "In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization. We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges that impede the successful and economical design of prompt optimization techniques. One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable. Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive. To address this, we introduce Prompt-OIRL, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data. Such data exists as by-products when diverse prompts are benchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent prompt optimization objective is achieved by first learning an offline reward model. This model can evaluate any query-prompt pairs without accessing LLMs. Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt. Our experimental evaluations across various LLM scales and arithmetic reasoning datasets underscore both the efficacy and economic viability of the proposed approach.", "comments": "", "official_code_urls": ["https://github.com/holarissun/prompt-oirl"], "pwc_page_url": "https://paperswithcode.com/paper/offline-prompt-evaluation-and-optimization", "bibtex": "@misc{sun2023querydependent,\n      title={Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL}, \n      author={Hao Sun and Alihan H\u00fcy\u00fck and Mihaela van der Schaar},\n      year={2023},\n      eprint={2309.06553},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "13-09-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2311.09677v1": {"paper_id": "2311.09677v1", "abs_url": "https://arxiv.org/abs/2311.09677v1", "pdf_url": "https://arxiv.org/pdf/2311.09677v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.09677v1_R-Tuning_Teaching_Large_Language_Models_to_Refuse_Unknown_Questions.pdf", "title": "R-Tuning: Teaching Large Language Models to Refuse Unknown Questions", "year": 2023, "paper_venue": null, "authors": ["Hanning Zhang", "Shizhe Diao", "Yong Lin", "Yi R. Fung", "Qing Lian", "Xingyao Wang", "Yangyi Chen", "Heng Ji", "Tong Zhang"], "abstract": ".", "comments": "20 pages, 8 figures", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/r-tuning-teaching-large-language-models-to", "bibtex": "@misc{zhang2023rtuning,\n      title={R-Tuning: Teaching Large Language Models to Refuse Unknown Questions}, \n      author={Hanning Zhang and Shizhe Diao and Yong Lin and Yi R. Fung and Qing Lian and Xingyao Wang and Yangyi Chen and Heng Ji and Tong Zhang},\n      year={2023},\n      eprint={2311.09677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "16-11-2023", "categories": ["cs.CL"]}, "2302.04449v3": {"paper_id": "2302.04449v3", "abs_url": "https://arxiv.org/abs/2302.04449v3", "pdf_url": "https://arxiv.org/pdf/2302.04449v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2302.04449v3_Read_and_Reap_the_Rewards_Learning_to_Play_Atari_with_the_Help_of_Instruction_Manuals.pdf", "title": "Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals", "year": 2023, "paper_venue": null, "authors": ["Yue Wu", "Yewen Fan", "Paul Pu Liang", "Amos Azaria", "Yuanzhi Li", "Tom M. Mitchell"], "abstract": "High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary reward is then provided to a standard A2C RL agent, when interaction is detected. Experimentally, various RL algorithms obtain significant improvement in performance and training speed when assisted by our design.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/read-and-reap-the-rewards-learning-to-play", "bibtex": "@misc{wu2023read,\n      title={Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals}, \n      author={Yue Wu and Yewen Fan and Paul Pu Liang and Amos Azaria and Yuanzhi Li and Tom M. Mitchell},\n      year={2023},\n      eprint={2302.04449},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "09-02-2023", "categories": ["cs.LG", "cs.AI", "cs.CL"]}, "2305.11255v4": {"paper_id": "2305.11255v4", "abs_url": "https://arxiv.org/abs/2305.11255v4", "pdf_url": "https://arxiv.org/pdf/2305.11255v4.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.11255v4_Reasoning_Implicit_Sentiment_with_Chain-of-Thought_Prompting.pdf", "title": "Reasoning Implicit Sentiment with Chain-of-Thought Prompting", "year": 2023, "paper_venue": null, "authors": ["Hao Fei", "Bobo Li", "Qian Liu", "Lidong Bing", "Fei Li", "Tat-Seng Chua"], "abstract": ".", "comments": "ACL2023 Short Paper", "official_code_urls": ["https://github.com/scofield7419/thor-isa"], "pwc_page_url": "https://paperswithcode.com/paper/reasoning-implicit-sentiment-with-chain-of", "bibtex": "@misc{fei2023reasoning,\n      title={Reasoning Implicit Sentiment with Chain-of-Thought Prompting}, \n      author={Hao Fei and Bobo Li and Qian Liu and Lidong Bing and Fei Li and Tat-Seng Chua},\n      year={2023},\n      eprint={2305.11255},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "18-05-2023", "categories": ["cs.CL"]}, "2305.14992v2": {"paper_id": "2305.14992v2", "abs_url": "https://arxiv.org/abs/2305.14992v2", "pdf_url": "https://arxiv.org/pdf/2305.14992v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.14992v2_Reasoning_with_Language_Model_is_Planning_with_World_Model.pdf", "title": "Reasoning with Language Model is Planning with World Model", "year": 2023, "paper_venue": null, "authors": ["Shibo Hao", "Yi Gu", "Haodi Ma", "Joshua Jiahua Hong", "Zhen Wang", "Daisy Zhe Wang", "Zhiting Hu"], "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.", "comments": "EMNLP 2023. Code is available at this https URL", "official_code_urls": ["https://github.com/ber666/llm-reasoners"], "pwc_page_url": "https://paperswithcode.com/paper/reasoning-with-language-model-is-planning", "bibtex": "@misc{hao2023reasoning,\n      title={Reasoning with Language Model is Planning with World Model}, \n      author={Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and Daisy Zhe Wang and Zhiting Hu},\n      year={2023},\n      eprint={2305.14992},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "24-05-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2306.06891v1": {"paper_id": "2306.06891v1", "abs_url": "https://arxiv.org/abs/2306.06891v1", "pdf_url": "https://arxiv.org/pdf/2306.06891v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2306.06891v1_Recursion_of_Thought_A_Divide-and-Conquer_Approach_to_Multi-Context_Reasoning_with_Language_Models.pdf", "title": "Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models", "year": 2023, "paper_venue": null, "authors": ["Soochan Lee", "Gunhee Kim"], "abstract": "Generating intermediate steps, or Chain of Thought (CoT), is an effective way to significantly improve language models' (LM) multi-step reasoning capability. However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size. Instead of increasing the context limit, which has already been heavily investigated, we explore an orthogonal direction: making LMs divide a problem into multiple contexts. We propose a new inference framework, called Recursion of Thought (RoT), which introduces several special tokens that the models can output to trigger context-related operations. Extensive experiments with multiple architectures including GPT-3 show that RoT dramatically improves LMs' inference capability to solve problems, whose solution consists of hundreds of thousands of tokens.", "comments": "ACL 2023 (short, findings)", "official_code_urls": ["https://github.com/soochan-lee/rot"], "pwc_page_url": "https://paperswithcode.com/paper/recursion-of-thought-a-divide-and-conquer", "bibtex": "@misc{lee2023recursion,\n      title={Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models}, \n      author={Soochan Lee and Gunhee Kim},\n      year={2023},\n      eprint={2306.06891},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "12-06-2023", "categories": ["cs.CL", "cs.AI"]}, "2311.04205v1": {"paper_id": "2311.04205v1", "abs_url": "https://arxiv.org/abs/2311.04205v1", "pdf_url": "https://arxiv.org/pdf/2311.04205v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.04205v1_Rephrase_and_Respond_Let_Large_Language_Models_Ask_Better_Questions_for_Themselves.pdf", "title": "Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves", "year": 2023, "paper_venue": null, "authors": ["Yihe Deng", "Weitong Zhang", "Zixiang Chen", "Quanquan Gu"], "abstract": ".", "comments": "32 pages, 9 figures, 22 tables", "official_code_urls": ["https://github.com/uclaml/Rephrase-and-Respond"], "pwc_page_url": "https://paperswithcode.com/paper/rephrase-and-respond-let-large-language", "bibtex": "@misc{deng2023rephrase,\n      title={Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves}, \n      author={Yihe Deng and Weitong Zhang and Zixiang Chen and Quanquan Gu},\n      year={2023},\n      eprint={2311.04205},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "07-11-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2306.00622v1": {"paper_id": "2306.00622v1", "abs_url": "https://arxiv.org/abs/2306.00622v1", "pdf_url": "https://arxiv.org/pdf/2306.00622v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2306.00622v1_ReviewerGPT?_An_Exploratory_Study_on_Using_Large_Language_Models_for_Paper_Reviewing.pdf", "title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing", "year": 2023, "paper_venue": null, "authors": ["Ryan Liu", "Nihar B. Shah"], "abstract": "Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/reviewergpt-an-exploratory-study-on-using", "bibtex": "@misc{liu2023reviewergpt,\n      title={ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing}, \n      author={Ryan Liu and Nihar B. Shah},\n      year={2023},\n      eprint={2306.00622},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "01-06-2023", "categories": ["cs.CL", "cs.AI", "cs.DL"]}, "2311.03285v2": {"paper_id": "2311.03285v2", "abs_url": "https://arxiv.org/abs/2311.03285v2", "pdf_url": "https://arxiv.org/pdf/2311.03285v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.03285v2_S-LoRA_Serving_Thousands_of_Concurrent_LoRA_Adapters.pdf", "title": "S-LoRA: Serving Thousands of Concurrent LoRA Adapters", "year": 2023, "paper_venue": null, "authors": ["Ying Sheng", "Shiyi Cao", "Dacheng Li", "Coleman Hooper", "Nicholas Lee", "Shuo Yang", "Christopher Chou", "Banghua Zhu", "Lianmin Zheng", "Kurt Keutzer", "Joseph E. Gonzalez", "Ion Stoica"], "abstract": "", "comments": "", "official_code_urls": ["https://github.com/s-lora/s-lora"], "pwc_page_url": "https://paperswithcode.com/paper/s-lora-serving-thousands-of-concurrent-lora", "bibtex": "@misc{sheng2023slora,\n      title={S-LoRA: Serving Thousands of Concurrent LoRA Adapters}, \n      author={Ying Sheng and Shiyi Cao and Dacheng Li and Coleman Hooper and Nicholas Lee and Shuo Yang and Christopher Chou and Banghua Zhu and Lianmin Zheng and Kurt Keutzer and Joseph E. Gonzalez and Ion Stoica},\n      year={2023},\n      eprint={2311.03285},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "06-11-2023", "categories": ["cs.LG", "cs.AI", "cs.DC"]}, "2312.06974v1": {"paper_id": "2312.06974v1", "abs_url": "https://arxiv.org/abs/2312.06974v1", "pdf_url": "https://arxiv.org/pdf/2312.06974v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.06974v1_SM70_A_Large_Language_Model_for_Medical_Devices.pdf", "title": "SM70: A Large Language Model for Medical Devices", "year": 2023, "paper_venue": null, "authors": ["Anubhav Bhatti", "Surajsinh Parmar", "San Lee"], "abstract": "We are introducing SM70, a 70 billion-parameter Large Language Model that is specifically designed for SpassMed's medical devices under the brand name 'JEE1' (pronounced as G1 and means 'Life'). This large language model provides more accurate and safe responses to medical-domain questions. To fine-tune SM70, we used around 800K data entries from the publicly available dataset MedAlpaca. The Llama2 70B open-sourced model served as the foundation for SM70, and we employed the QLoRA technique for fine-tuning. The evaluation is conducted across three benchmark datasets - MEDQA - USMLE, PUBMEDQA, and USMLE - each representing a unique aspect of medical knowledge and reasoning. The performance of SM70 is contrasted with other notable LLMs, including Llama2 70B, Clinical Camel 70 (CC70), GPT 3.5, GPT 4, and Med-Palm, to provide a comparative understanding of its capabilities within the medical domain. Our results indicate that SM70 outperforms several established models in these datasets, showcasing its proficiency in handling a range of medical queries, from fact-based questions derived from PubMed abstracts to complex clinical decision-making scenarios. The robust performance of SM70, particularly in the USMLE and PUBMEDQA datasets, suggests its potential as an effective tool in clinical decision support and medical information retrieval. Despite its promising results, the paper also acknowledges the areas where SM70 lags behind the most advanced model, GPT 4, thereby highlighting the need for further development, especially in tasks demanding extensive medical knowledge and intricate reasoning.", "comments": "5 Pages, Technical Report", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/sm70-a-large-language-model-for-medical", "bibtex": "@misc{bhatti2023sm70,\n      title={SM70: A Large Language Model for Medical Devices}, \n      author={Anubhav Bhatti and Surajsinh Parmar and San Lee},\n      year={2023},\n      eprint={2312.06974},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "12-12-2023", "categories": ["cs.CL", "cs.AI"]}, "2305.15486v3": {"paper_id": "2305.15486v3", "abs_url": "https://arxiv.org/abs/2305.15486v3", "pdf_url": "https://arxiv.org/pdf/2305.15486v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.15486v3_SPRING_Studying_the_Paper_and_Reasoning_to_Play_Games.pdf", "title": "SPRING: Studying the Paper and Reasoning to Play Games", "year": 2023, "paper_venue": null, "authors": ["Yue Wu", "Shrimai Prabhumoye", "So Yeon Min", "Yonatan Bisk", "Ruslan Salakhutdinov", "Amos Azaria", "Tom Mitchell", "Yuanzhi Li"], "abstract": "Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context \"reasoning\" induced by different forms of prompts under the setting of the Crafter open-world environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of games as a test bed for LLMs.", "comments": "", "official_code_urls": ["https://github.com/holmeswww/spring"], "pwc_page_url": "https://paperswithcode.com/paper/spring-gpt-4-out-performs-rl-algorithms-by", "bibtex": "@misc{wu2023spring,\n      title={SPRING: Studying the Paper and Reasoning to Play Games}, \n      author={Yue Wu and Shrimai Prabhumoye and So Yeon Min and Yonatan Bisk and Ruslan Salakhutdinov and Amos Azaria and Tom Mitchell and Yuanzhi Li},\n      year={2023},\n      eprint={2305.15486},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "24-05-2023", "categories": ["cs.AI", "cs.LG"]}, "2310.11511v1": {"paper_id": "2310.11511v1", "abs_url": "https://arxiv.org/abs/2310.11511v1", "pdf_url": "https://arxiv.org/pdf/2310.11511v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.11511v1_Self-RAG_Learning_to_Retrieve_Generate_and_Critique_through_Self-Reflection.pdf", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "year": 2023, "paper_venue": null, "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.", "comments": "30 pages, 2 figures, 12 tables", "official_code_urls": ["https://github.com/AkariAsai/self-rag"], "pwc_page_url": "https://paperswithcode.com/paper/self-rag-learning-to-retrieve-generate-and", "bibtex": "@misc{asai2023selfrag,\n      title={Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection}, \n      author={Akari Asai and Zeqiu Wu and Yizhong Wang and Avirup Sil and Hannaneh Hajishirzi},\n      year={2023},\n      eprint={2310.11511},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "17-10-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2310.01557v3": {"paper_id": "2310.01557v3", "abs_url": "https://arxiv.org/abs/2310.01557v3", "pdf_url": "https://arxiv.org/pdf/2310.01557v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.01557v3_SmartPlay_A_Benchmark_for_LLMs_as_Intelligent_Agents.pdf", "title": "SmartPlay: A Benchmark for LLMs as Intelligent Agents", "year": 2023, "paper_venue": null, "authors": ["Yue Wu", "Xuan Tang", "Tom M. Mitchell", "Yuanzhi Li"], "abstract": "", "comments": "", "official_code_urls": ["https://github.com/microsoft/smartplay"], "pwc_page_url": "https://paperswithcode.com/paper/smartplay-a-benchmark-for-llms-as-intelligent", "bibtex": "@misc{wu2023smartplay,\n      title={SmartPlay: A Benchmark for LLMs as Intelligent Agents}, \n      author={Yue Wu and Xuan Tang and Tom M. Mitchell and Yuanzhi Li},\n      year={2023},\n      eprint={2310.01557},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "02-10-2023", "categories": ["cs.LG", "cs.AI"]}, "2303.12712v5": {"paper_id": "2303.12712v5", "abs_url": "https://arxiv.org/abs/2303.12712v5", "pdf_url": "https://arxiv.org/pdf/2303.12712v5.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2303.12712v5_Sparks_of_Artificial_General_Intelligence_Early_experiments_with_GPT-4.pdf", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4", "year": 2023, "paper_venue": null, "authors": ["S\u00e9bastien Bubeck", "Varun Chandrasekaran", "Ronen Eldan", "Johannes Gehrke", "Eric Horvitz", "Ece Kamar", "Peter Lee", "Yin Tat Lee", "Yuanzhi Li", "Scott Lundberg", "Harsha Nori", "Hamid Palangi", "Marco Tulio Ribeiro", "Yi Zhang"], "abstract": "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/sparks-of-artificial-general-intelligence", "bibtex": "@misc{bubeck2023sparks,\n      title={Sparks of Artificial General Intelligence: Early experiments with GPT-4}, \n      author={S\u00e9bastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},\n      year={2023},\n      eprint={2303.12712},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "22-03-2023", "categories": ["cs.CL", "cs.AI"]}, "2301.00774v3": {"paper_id": "2301.00774v3", "abs_url": "https://arxiv.org/abs/2301.00774v3", "pdf_url": "https://arxiv.org/pdf/2301.00774v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2301.00774v3_SparseGPT_Massive_Language_Models_Can_Be_Accurately_Pruned_in_One-Shot.pdf", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot", "year": 2023, "paper_venue": null, "authors": ["Elias Frantar", "Dan Alistarh"], "abstract": ".", "comments": "", "official_code_urls": ["https://github.com/ist-daslab/sparsegpt"], "pwc_page_url": "https://paperswithcode.com/paper/massive-language-models-can-be-accurately", "bibtex": "@misc{frantar2023sparsegpt,\n      title={SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot}, \n      author={Elias Frantar and Dan Alistarh},\n      year={2023},\n      eprint={2301.00774},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "02-01-2023", "categories": ["cs.LG"]}, "2312.15918v1": {"paper_id": "2312.15918v1", "abs_url": "https://arxiv.org/abs/2312.15918v1", "pdf_url": "https://arxiv.org/pdf/2312.15918v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.15918v1_Supervised_Knowledge_Makes_Large_Language_Models_Better_In-context_Learners.pdf", "title": "Supervised Knowledge Makes Large Language Models Better In-context Learners", "year": 2023, "paper_venue": null, "authors": ["Linyi Yang", "Shuibai Zhang", "Zhuohao Yu", "Guangsheng Bao", "Yidong Wang", "Jindong Wang", "Ruochen Xu", "Wei Ye", "Xing Xie", "Weizhu Chen", "Yue Zhang"], "abstract": "Large Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering. The recent progress in large-scale generative models has further expanded their use in real-world language applications. However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored. While previous in-context learning research has focused on enhancing models to adhere to users' specific instructions and quality expectations, and to avoid undesired outputs, little to no work has explored the use of task-Specific fine-tuned Language Models (SLMs) to improve LLMs' in-context learning during the inference stage. Our primary contribution is the establishment of a simple yet effective framework that enhances the reliability of LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs benefit from discriminative models, and 3) minimizes hallucinations in generative tasks. Using our proposed plug-in method, enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality. We offer a comprehensive suite of resources, including 16 curated datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks. Our empirical analysis sheds light on the advantages of incorporating discriminative models into LLMs and highlights the potential of our methodology in fostering more reliable LLMs.", "comments": "18 pages. Under review at ICLR 2024", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/supervised-knowledge-makes-large-language", "bibtex": "@misc{yang2023supervised,\n      title={Supervised Knowledge Makes Large Language Models Better In-context Learners}, \n      author={Linyi Yang and Shuibai Zhang and Zhuohao Yu and Guangsheng Bao and Yidong Wang and Jindong Wang and Ruochen Xu and Wei Ye and Xing Xie and Weizhu Chen and Yue Zhang},\n      year={2023},\n      eprint={2312.15918},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "26-12-2023", "categories": ["cs.CL", "cs.AI"]}, "2305.17812v1": {"paper_id": "2305.17812v1", "abs_url": "https://arxiv.org/abs/2305.17812v1", "pdf_url": "https://arxiv.org/pdf/2305.17812v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.17812v1_Tab-CoT_Zero-shot_Tabular_Chain_of_Thought.pdf", "title": "Tab-CoT: Zero-shot Tabular Chain of Thought", "year": 2023, "paper_venue": null, "authors": ["Ziqi Jin", "Wei Lu"], "abstract": "The chain-of-though (CoT) prompting methods were successful in various natural language processing (NLP) tasks thanks to their ability to unveil the underlying complex reasoning processes. Such reasoning processes typically exhibit implicitly structured steps. Recent efforts also started investigating methods to encourage more explicitly structured reasoning procedures to be captured. In this work, we propose Tab-CoT, a novel tabular-format CoT prompting method, which allows the complex reasoning process to be explicitly modelled in a highly structured manner. Despite its simplicity, we show that our approach is capable of performing reasoning across multiple dimensions (i.e., both rows and columns). We demonstrate our approach's strong zero-shot and few-shot capabilities through extensive experiments on a range of reasoning tasks.", "comments": "accepted by ACL 2023 Finding", "official_code_urls": ["https://github.com/xalp/tab-cot"], "pwc_page_url": "https://paperswithcode.com/paper/tab-cot-zero-shot-tabular-chain-of-thought", "bibtex": "@misc{jin2023tabcot,\n      title={Tab-CoT: Zero-shot Tabular Chain of Thought}, \n      author={Ziqi Jin and Wei Lu},\n      year={2023},\n      eprint={2305.17812},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "28-05-2023", "categories": ["cs.CL"]}, "2310.06117v1": {"paper_id": "2310.06117v1", "abs_url": "https://arxiv.org/abs/2310.06117v1", "pdf_url": "https://arxiv.org/pdf/2310.06117v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.06117v1_Take_a_Step_Back_Evoking_Reasoning_via_Abstraction_in_Large_Language_Models.pdf", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "abstract": "We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/take-a-step-back-evoking-reasoning-via", "bibtex": "@misc{zheng2023step,\n      title={Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models}, \n      author={Huaixiu Steven Zheng and Swaroop Mishra and Xinyun Chen and Heng-Tze Cheng and Ed H. Chi and Quoc V Le and Denny Zhou},\n      year={2023},\n      eprint={2310.06117},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "09-10-2023", "categories": ["cs.LG", "cs.AI", "cs.CL"]}, "2310.19019v2": {"paper_id": "2310.19019v2", "abs_url": "https://arxiv.org/abs/2310.19019v2", "pdf_url": "https://arxiv.org/pdf/2310.19019v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.19019v2_TeacherLM_Teaching_to_Fish_Rather_Than_Giving_the_Fish_Language_Modeling_Likewise.pdf", "title": "TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise", "year": 2023, "paper_venue": null, "authors": ["Nan He", "Hanyu Lai", "Chenyang Zhao", "Zirui Cheng", "Junting Pan", "Ruoyu Qin", "Ruofan Lu", "Rui Lu", "Yunchen Zhang", "Gangming Zhao", "Zhaohui Hou", "Zhiyuan Huang", "Shaoqing Lu", "Ding Liang", "Mingjie Zhan"], "abstract": "Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, which makes annotation more than just an answer, thus allowing other models to learn \"why\" instead of just \"what\". The TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we augmented 58 NLP datasets and taught various student models with different parameters from OPT and BLOOM series in a multi-task setting. The experimental results indicate that the data augmentation provided by TeacherLM has brought significant benefits. We will release the TeacherLM series of models and augmented datasets as open-source.", "comments": "5 figures, 15 pages", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/teacherlm-teaching-to-fish-rather-than-giving", "bibtex": "@misc{he2023teacherlm,\n      title={TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise}, \n      author={Nan He and Hanyu Lai and Chenyang Zhao and Zirui Cheng and Junting Pan and Ruoyu Qin and Ruofan Lu and Rui Lu and Yunchen Zhang and Gangming Zhao and Zhaohui Hou and Zhiyuan Huang and Shaoqing Lu and Ding Liang and Mingjie Zhan},\n      year={2023},\n      eprint={2310.19019},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "29-10-2023", "categories": ["cs.CL", "cs.AI"]}, "2311.07590v2": {"paper_id": "2311.07590v2", "abs_url": "https://arxiv.org/abs/2311.07590v2", "pdf_url": "https://arxiv.org/pdf/2311.07590v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.07590v2_Technical_Report_Large_Language_Models_can_Strategically_Deceive_their_Users_when_Put_Under_Pressure.pdf", "title": "Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure", "year": 2023, "paper_venue": null, "authors": ["J\u00e9r\u00e9my Scheurer", "Mikita Balesni", "Marius Hobbhahn"], "abstract": "We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception.", "comments": "", "official_code_urls": ["https://github.com/apolloresearch/insider-trading"], "pwc_page_url": "https://paperswithcode.com/paper/technical-report-large-language-models-can", "bibtex": "@misc{scheurer2023technical,\n      title={Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure}, \n      author={J\u00e9r\u00e9my Scheurer and Mikita Balesni and Marius Hobbhahn},\n      year={2023},\n      eprint={2311.07590},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "09-11-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2311.02262v1": {"paper_id": "2311.02262v1", "abs_url": "https://arxiv.org/abs/2311.02262v1", "pdf_url": "https://arxiv.org/pdf/2311.02262v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.02262v1_Tell_Your_Model_Where_to_Attend_Post-hoc_Attention_Steering_for_LLMs.pdf", "title": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs", "year": 2023, "paper_venue": null, "authors": ["Qingru Zhang", "Chandan Singh", "Liyuan Liu", "Xiaodong Liu", "Bin Yu", "Jianfeng Gao", "Tuo Zhao"], "abstract": ".", "comments": "16 pages", "official_code_urls": ["https://github.com/qingruzhang/pasta"], "pwc_page_url": "https://paperswithcode.com/paper/tell-your-model-where-to-attend-post-hoc", "bibtex": "@misc{zhang2023tell,\n      title={Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs}, \n      author={Qingru Zhang and Chandan Singh and Liyuan Liu and Xiaodong Liu and Bin Yu and Jianfeng Gao and Tuo Zhao},\n      year={2023},\n      eprint={2311.02262},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "03-11-2023", "categories": ["cs.CL", "cs.LG"]}, "2311.10538v3": {"paper_id": "2311.10538v3", "abs_url": "https://arxiv.org/abs/2311.10538v3", "pdf_url": "https://arxiv.org/pdf/2311.10538v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.10538v3_Testing_Language_Model_Agents_Safely_in_the_Wild.pdf", "title": "Testing Language Model Agents Safely in the Wild", "year": 2023, "paper_venue": null, "authors": ["Silen Naihin", "David Atkinson", "Marc Green", "Merwane Hamadi", "Craig Swift", "Douglas Schonholtz", "Adam Tauman Kalai", "David Bau"], "abstract": "A prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. Yet real-world autonomous tests face several unique safety challenges, both due to the possibility of causing harm during a test, as well as the risk of encountering new unsafe agent behavior through interactions with real-world and potentially malicious actors. We propose a framework for conducting safe autonomous agent tests on the open internet: agent actions are audited by a context-sensitive monitor that enforces a stringent safety boundary to stop an unsafe test, with suspect behavior ranked and logged to be examined by humans. We design a basic safety monitor (AgentMonitor) that is flexible enough to monitor existing LLM agents, and, using an adversarial simulated agent, we measure its ability to identify and stop unsafe situations. Then we apply the AgentMonitor on a battery of real-world tests of AutoGPT, and we identify several limitations and challenges that will face the creation of safe in-the-wild tests as autonomous agents grow more capable.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/testing-language-model-agents-safely-in-the", "bibtex": "@misc{naihin2023testing,\n      title={Testing Language Model Agents Safely in the Wild}, \n      author={Silen Naihin and David Atkinson and Marc Green and Merwane Hamadi and Craig Swift and Douglas Schonholtz and Adam Tauman Kalai and David Bau},\n      year={2023},\n      eprint={2311.10538},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "17-11-2023", "categories": ["cs.AI"]}, "2306.11644v2": {"paper_id": "2306.11644v2", "abs_url": "https://arxiv.org/abs/2306.11644v2", "pdf_url": "https://arxiv.org/pdf/2306.11644v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2306.11644v2_Textbooks_Are_All_You_Need.pdf", "title": "Textbooks Are All You Need", "year": 2023, "paper_venue": null, "authors": ["Suriya Gunasekar", "Yi Zhang", "Jyoti Aneja", "Caio C\u00e9sar Teodoro Mendes", "Allie Del Giorno", "Sivakanth Gopi", "Mojan Javaheripi", "Piero Kauffmann", "Gustavo de Rosa", "Olli Saarikivi", "Adil Salim", "Shital Shah", "Harkirat Singh Behl", "Xin Wang", "S\u00e9bastien Bubeck", "Ronen Eldan", "Adam Tauman Kalai", "Yin Tat Lee", "Yuanzhi Li"], "abstract": "We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.", "comments": "26 pages; changed color scheme of plot. fixed minor typos and added couple clarifications", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/textbooks-are-all-you-need", "bibtex": "@misc{gunasekar2023textbooks,\n      title={Textbooks Are All You Need}, \n      author={Suriya Gunasekar and Yi Zhang and Jyoti Aneja and Caio C\u00e9sar Teodoro Mendes and Allie Del Giorno and Sivakanth Gopi and Mojan Javaheripi and Piero Kauffmann and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Harkirat Singh Behl and Xin Wang and S\u00e9bastien Bubeck and Ronen Eldan and Adam Tauman Kalai and Yin Tat Lee and Yuanzhi Li},\n      year={2023},\n      eprint={2306.11644},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "20-06-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2309.05463v1": {"paper_id": "2309.05463v1", "abs_url": "https://arxiv.org/abs/2309.05463v1", "pdf_url": "https://arxiv.org/pdf/2309.05463v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.05463v1_Textbooks_Are_All_You_Need_II_phi-15_technical_report.pdf", "title": "Textbooks Are All You Need II: phi-1.5 technical report", "year": 2023, "paper_venue": null, "authors": ["Yuanzhi Li", "S\u00e9bastien Bubeck", "Ronen Eldan", "Allie Del Giorno", "Suriya Gunasekar", "Yin Tat Lee"], "abstract": "We continue the investigation into the power of smaller Transformer-based language models as initiated by \\textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality\" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need\" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \\textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \\textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step\" or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \\textbf{phi-1.5} to promote further research on these urgent topics.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/textbooks-are-all-you-need-ii-phi-1-5", "bibtex": "@misc{li2023textbooks,\n      title={Textbooks Are All You Need II: phi-1.5 technical report}, \n      author={Yuanzhi Li and S\u00e9bastien Bubeck and Ronen Eldan and Allie Del Giorno and Suriya Gunasekar and Yin Tat Lee},\n      year={2023},\n      eprint={2309.05463},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "11-09-2023", "categories": ["cs.CL", "cs.AI"]}, "2311.07961v1": {"paper_id": "2311.07961v1", "abs_url": "https://arxiv.org/abs/2311.07961v1", "pdf_url": "https://arxiv.org/pdf/2311.07961v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.07961v1_The_ART_of_LLM_Refinement_Ask_Refine_and_Trust.pdf", "title": "The ART of LLM Refinement: Ask, Refine, and Trust", "year": 2023, "paper_venue": null, "authors": ["Kumar Shridhar", "Koustuv Sinha", "Andrew Cohen", "Tianlu Wang", "Ping Yu", "Ram Pasunuru", "Mrinmaya Sachan", "Jason Weston", "Asli Celikyilmaz"], "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable generative abilities, but can they judge the quality of their own generations? A popular concept, referred to as self-refinement, postulates that LLMs can detect and correct the errors in their generations when asked to do so. However, recent empirical evidence points in the opposite direction, suggesting that LLMs often struggle to accurately identify errors when reasoning is involved. To address this, we propose a reasoning with refinement objective called ART: Ask, Refine, and Trust, which asks necessary questions to decide when an LLM should refine its output, and either affirm or withhold trust in its refinement by ranking the refinement and the initial prediction. On two multistep reasoning tasks of mathematical word problems (GSM8K) and question answering (StrategyQA), ART achieves a performance gain of +5 points over self-refinement baselines, while using a much smaller model as the decision maker. We also demonstrate the benefit of using smaller models to make refinement decisions as a cost-effective alternative to fine-tuning a larger model.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/the-art-of-llm-refinement-ask-refine-and", "bibtex": "@misc{shridhar2023art,\n      title={The ART of LLM Refinement: Ask, Refine, and Trust}, \n      author={Kumar Shridhar and Koustuv Sinha and Andrew Cohen and Tianlu Wang and Ping Yu and Ram Pasunuru and Mrinmaya Sachan and Jason Weston and Asli Celikyilmaz},\n      year={2023},\n      eprint={2311.07961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "14-11-2023", "categories": ["cs.CL"]}, "2305.14045v2": {"paper_id": "2305.14045v2", "abs_url": "https://arxiv.org/abs/2305.14045v2", "pdf_url": "https://arxiv.org/pdf/2305.14045v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.14045v2_The_CoT_Collection_Improving_Zero-shot_and_Few-shot_Learning_of_Language_Models_via_Chain-of-Thought_Fine-Tuning.pdf", "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning", "year": 2023, "paper_venue": null, "authors": ["Seungone Kim", "Se June Joo", "Doyoung Kim", "Joel Jang", "Seonghyeon Ye", "Jamin Shin", "Minjoon Seo"], "abstract": "Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B & 11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and +2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until the max length by a +13.98% margin. Our code, the CoT Collection data, and model checkpoints are publicly available.", "comments": "EMNLP 2023 (Main Conference)", "official_code_urls": ["https://github.com/kaistai/cot-collection", "https://github.com/kaist-lklab/cot-collection"], "pwc_page_url": "https://paperswithcode.com/paper/the-cot-collection-improving-zero-shot-and", "bibtex": "@misc{kim2023cot,\n      title={The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning}, \n      author={Seungone Kim and Se June Joo and Doyoung Kim and Joel Jang and Seonghyeon Ye and Jamin Shin and Minjoon Seo},\n      year={2023},\n      eprint={2305.14045},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "23-05-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2311.07361v2": {"paper_id": "2311.07361v2", "abs_url": "https://arxiv.org/abs/2311.07361v2", "pdf_url": "https://arxiv.org/pdf/2311.07361v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.07361v2_The_Impact_of_Large_Language_Models_on_Scientific_Discovery_a_Preliminary_Study_using_GPT-4.pdf", "title": "The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4", "year": 2023, "paper_venue": null, "authors": ["Microsoft Research AI4Science", "Microsoft Azure Quantum"], "abstract": "In recent years, groundbreaking advancements in natural language processing have culminated in the emergence of powerful large language models (LLMs), which have showcased remarkable capabilities across a vast array of domains, including the understanding, generation, and translation of natural language, and even tasks that extend beyond language processing. In this report, we delve into the performance of LLMs within the context of scientific discovery, focusing on GPT-4, the state-of-the-art language model. Our investigation spans a diverse range of scientific areas encompassing drug discovery, biology, computational chemistry (density functional theory (DFT) and molecular dynamics (MD)), materials design, and partial differential equations (PDE). Evaluating GPT-4 on scientific tasks is crucial for uncovering its potential across various research domains, validating its domain-specific expertise, accelerating scientific progress, optimizing resource allocation, guiding future model development, and fostering interdisciplinary research. Our exploration methodology primarily consists of expert-driven case assessments, which offer qualitative insights into the model's comprehension of intricate scientific concepts and relationships, and occasionally benchmark testing, which quantitatively evaluates the model's capacity to solve well-defined domain-specific problems. Our preliminary exploration indicates that GPT-4 exhibits promising potential for a variety of scientific applications, demonstrating its aptitude for handling complex problem-solving and knowledge integration tasks. Broadly speaking, we evaluate GPT-4's knowledge base, scientific understanding, scientific numerical calculation abilities, and various scientific prediction capabilities.", "comments": "230 pages report; 181 pages for main contents", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/the-impact-of-large-language-models-on", "bibtex": "@misc{ai4science2023impact,\n      title={The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4}, \n      author={Microsoft Research AI4Science and Microsoft Azure Quantum},\n      year={2023},\n      eprint={2311.07361},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "13-11-2023", "categories": ["cs.CL", "cs.AI"]}, "2310.13549v1": {"paper_id": "2310.13549v1", "abs_url": "https://arxiv.org/abs/2310.13549v1", "pdf_url": "https://arxiv.org/pdf/2310.13549v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.13549v1_The_Perils__Promises_of_Fact-checking_with_Large_Language_Models.pdf", "title": "The Perils & Promises of Fact-checking with Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Dorian Quelle", "Alexandre Bovet"], "abstract": "Autonomous fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large Language Models (LLMs) like GPT-4 are increasingly trusted to verify information and write academic papers, lawsuits, and news articles, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper comprehension of when agents succeed and when they fail.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/the-perils-promises-of-fact-checking-with", "bibtex": "@misc{quelle2023perils,\n      title={The Perils & Promises of Fact-checking with Large Language Models}, \n      author={Dorian Quelle and Alexandre Bovet},\n      year={2023},\n      eprint={2310.13549},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "20-10-2023", "categories": ["cs.CL", "cs.CY", "cs.HC"]}, "2306.01116v1": {"paper_id": "2306.01116v1", "abs_url": "https://arxiv.org/abs/2306.01116v1", "pdf_url": "https://arxiv.org/pdf/2306.01116v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2306.01116v1_The_RefinedWeb_Dataset_for_Falcon_LLM_Outperforming_Curated_Corpora_with_Web_Data_and_Web_Data_Only.pdf", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only", "year": 2023, "paper_venue": null, "authors": ["Guilherme Penedo", "Quentin Malartic", "Daniel Hesslow", "Ruxandra Cojocaru", "Alessandro Cappelli", "Hamza Alobeidli", "Baptiste Pannier", "Ebtesam Almazrouei", "Julien Launay"], "abstract": "Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/the-refinedweb-dataset-for-falcon-llm", "bibtex": "@misc{penedo2023refinedweb,\n      title={The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only}, \n      author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n      year={2023},\n      eprint={2306.01116},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "01-06-2023", "categories": ["cs.CL", "cs.AI"]}, "2309.07864v3": {"paper_id": "2309.07864v3", "abs_url": "https://arxiv.org/abs/2309.07864v3", "pdf_url": "https://arxiv.org/pdf/2309.07864v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2309.07864v3_The_Rise_and_Potential_of_Large_Language_Model_Based_Agents_A_Survey.pdf", "title": "The Rise and Potential of Large Language Model Based Agents: A Survey", "year": 2023, "paper_venue": null, "authors": ["Zhiheng Xi", "Wenxiang Chen", "Xin Guo", "Wei He", "Yiwen Ding", "Boyang Hong", "Ming Zhang", "Junzhe Wang", "Senjie Jin", "Enyu Zhou", "Rui Zheng", "Xiaoran Fan", "Xiao Wang", "Limao Xiong", "Yuhao Zhou", "Weiran Wang", "Changhao Jiang", "Yicheng Zou", "Xiangyang Liu", "Zhangyue Yin", "Shihan Dou", "Rongxiang Weng", "Wensen Cheng", "Qi Zhang", "Wenjuan Qin", "Yongyan Zheng", "Xipeng Qiu", "Xuanjing Huang", "Tao Gui"], "abstract": ".", "comments": "86 pages, 12 figures", "official_code_urls": ["https://github.com/woooodyy/llm-agent-paper-list"], "pwc_page_url": "https://paperswithcode.com/paper/the-rise-and-potential-of-large-language", "bibtex": "@misc{xi2023rise,\n      title={The Rise and Potential of Large Language Model Based Agents: A Survey}, \n      author={Zhiheng Xi and Wenxiang Chen and Xin Guo and Wei He and Yiwen Ding and Boyang Hong and Ming Zhang and Junzhe Wang and Senjie Jin and Enyu Zhou and Rui Zheng and Xiaoran Fan and Xiao Wang and Limao Xiong and Yuhao Zhou and Weiran Wang and Changhao Jiang and Yicheng Zou and Xiangyang Liu and Zhangyue Yin and Shihan Dou and Rongxiang Weng and Wensen Cheng and Qi Zhang and Wenjuan Qin and Yongyan Zheng and Xipeng Qiu and Xuanjing Huang and Tao Gui},\n      year={2023},\n      eprint={2309.07864},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "14-09-2023", "categories": ["cs.AI", "cs.CL"]}, "2312.08688v2": {"paper_id": "2312.08688v2", "abs_url": "https://arxiv.org/abs/2312.08688v2", "pdf_url": "https://arxiv.org/pdf/2312.08688v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.08688v2_TigerBot_An_Open_Multilingual_Multitask_LLM.pdf", "title": "TigerBot: An Open Multilingual Multitask LLM", "year": 2023, "paper_venue": null, "authors": ["Ye Chen", "Wei Cai", "Liangmin Wu", "Xiaowei Li", "Zhanxuan Xin", "Cong Fu"], "abstract": "We release and introduce the TigerBot family of large language models (LLMs), consisting of base and chat models, sized from 7, 13, 70 and 180 billion parameters. We develop our models embarking from Llama-2 and BLOOM, and push the boundary further in data, training algorithm, infrastructure, and application tools. Our models yield meaningful performance gain over SOTA open-source models, e.g., Llama-2, specifically 6% gain in English and 20% gain in Chinese. TigerBot model family also achieves leading performance in major academic and industrial benchmarks and leaderboards. We believe that TigerBot represents just a snapshot of lightning-fast progression in LLM open-source community. Therefore, we are thrilled to give back by publicly releasing our models and reporting our approach behind, with additional emphases on building SOTA LLMs in a democratized way and making LLMs of use in real-world applications.", "comments": "", "official_code_urls": ["https://github.com/tigerresearch/tigerbot"], "pwc_page_url": "https://paperswithcode.com/paper/tigerbot-an-open-multilingual-multitask-llm", "bibtex": "@misc{chen2023tigerbot,\n      title={TigerBot: An Open Multilingual Multitask LLM}, \n      author={Ye Chen and Wei Cai and Liangmin Wu and Xiaowei Li and Zhanxuan Xin and Cong Fu},\n      year={2023},\n      eprint={2312.08688},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "14-12-2023", "categories": ["cs.CL", "cs.AI"]}, "2312.13401v2": {"paper_id": "2312.13401v2", "abs_url": "https://arxiv.org/abs/2312.13401v2", "pdf_url": "https://arxiv.org/pdf/2312.13401v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2312.13401v2_Time_is_Encoded_in_the_Weights_of_Finetuned_Language_Models.pdf", "title": "Time is Encoded in the Weights of Finetuned Language Models", "year": 2023, "paper_venue": null, "authors": ["Kai Nylund", "Suchin Gururangan", "Noah A. Smith"], "abstract": "We present time vectors, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as our experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appear to be positioned closer together in a manifold. Using this structure, we interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. We demonstrate the consistency of our findings across different tasks, domains, model sizes, and time scales. Our results suggest that time is encoded in the weight space of finetuned models.", "comments": "Added references to Jaidka et al. (2018) and Loureiro et al. (2022)", "official_code_urls": ["https://github.com/KaiNylund/lm-weights-encode-time"], "pwc_page_url": "https://paperswithcode.com/paper/time-is-encoded-in-the-weights-of-finetuned", "bibtex": "@misc{nylund2023time,\n      title={Time is Encoded in the Weights of Finetuned Language Models}, \n      author={Kai Nylund and Suchin Gururangan and Noah A. Smith},\n      year={2023},\n      eprint={2312.13401},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "20-12-2023", "categories": ["cs.CL"]}, "2305.07759v2": {"paper_id": "2305.07759v2", "abs_url": "https://arxiv.org/abs/2305.07759v2", "pdf_url": "https://arxiv.org/pdf/2305.07759v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.07759v2_TinyStories_How_Small_Can_Language_Models_Be_and_Still_Speak_Coherent_English?.pdf", "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?", "year": 2023, "paper_venue": null, "authors": ["Ronen Eldan", "Yuanzhi Li"], "abstract": "We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/tinystories-how-small-can-language-models-be", "bibtex": "@misc{eldan2023tinystories,\n      title={TinyStories: How Small Can Language Models Be and Still Speak Coherent English?}, \n      author={Ronen Eldan and Yuanzhi Li},\n      year={2023},\n      eprint={2305.07759},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "12-05-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2311.01449v1": {"paper_id": "2311.01449v1", "abs_url": "https://arxiv.org/abs/2311.01449v1", "pdf_url": "https://arxiv.org/pdf/2311.01449v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.01449v1_TopicGPT_A_Prompt-based_Topic_Modeling_Framework.pdf", "title": "TopicGPT: A Prompt-based Topic Modeling Framework", "year": 2023, "paper_venue": null, "authors": ["Chau Minh Pham", "Alexander Hoyle", "Simeng Sun", "Mohit Iyyer"], "abstract": "Topic modeling is a well-established technique for exploring text corpora. Conventional topic models (e.g., LDA) represent topics as bags of words that often require \"reading the tea leaves\" to interpret; additionally, they offer users minimal semantic control over topics. To tackle these issues, we introduce TopicGPT, a prompt-based framework that uses large language models (LLMs) to uncover latent topics within a provided text collection. TopicGPT produces topics that align better with human categorizations compared to competing methods: for example, it achieves a harmonic mean purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for the strongest baseline. Its topics are also more interpretable, dispensing with ambiguous bags of words in favor of topics with natural language labels and associated free-form descriptions. Moreover, the framework is highly adaptable, allowing users to specify constraints and modify topics without the need for model retraining. TopicGPT can be further extended to hierarchical topical modeling, enabling users to explore topics at various levels of granularity. By streamlining access to high-quality and interpretable topics, TopicGPT represents a compelling, human-centered approach to topic modeling.", "comments": "", "official_code_urls": ["https://github.com/chtmp223/topicgpt"], "pwc_page_url": "https://paperswithcode.com/paper/topicgpt-a-prompt-based-topic-modeling", "bibtex": "@misc{pham2023topicgpt,\n      title={TopicGPT: A Prompt-based Topic Modeling Framework}, \n      author={Chau Minh Pham and Alexander Hoyle and Simeng Sun and Mohit Iyyer},\n      year={2023},\n      eprint={2311.01449},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "02-11-2023", "categories": ["cs.CL"]}, "2311.09188v1": {"paper_id": "2311.09188v1", "abs_url": "https://arxiv.org/abs/2311.09188v1", "pdf_url": "https://arxiv.org/pdf/2311.09188v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.09188v1_Towards_Verifiable_Text_Generation_with_Symbolic_References.pdf", "title": "Towards Verifiable Text Generation with Symbolic References", "year": 2023, "paper_venue": null, "authors": ["Lucas Torroba Hennigen", "Shannon Shen", "Aniruddha Nrusimha", "Bernhard Gapp", "David Sontag", "Yoon Kim"], "abstract": "Large language models (LLMs) have demonstrated an impressive ability to synthesize plausible and fluent text. However they remain vulnerable to hallucinations, and thus their outputs generally require manual human verification for high-stakes applications, which can be time-consuming and difficult. This paper proposes symbolically grounded generation (SymGen) as a simple approach for enabling easier validation of an LLM's output. SymGen prompts an LLM to interleave its regular output text with explicit symbolic references to fields present in some conditioning data (e.g., a table in JSON format). The references can be used to display the provenance of different spans of text in the generation, reducing the effort required for manual verification. Across data-to-text and question answering experiments, we find that LLMs are able to directly output text that makes use of symbolic references while maintaining fluency and accuracy.", "comments": "46 pages, 4 figures, 6 tables", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/towards-verifiable-text-generation-with", "bibtex": "@misc{hennigen2023verifiable,\n      title={Towards Verifiable Text Generation with Symbolic References}, \n      author={Lucas Torroba Hennigen and Shannon Shen and Aniruddha Nrusimha and Bernhard Gapp and David Sontag and Yoon Kim},\n      year={2023},\n      eprint={2311.09188},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "15-11-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2302.07730v3": {"paper_id": "2302.07730v3", "abs_url": "https://arxiv.org/abs/2302.07730v3", "pdf_url": "https://arxiv.org/pdf/2302.07730v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2302.07730v3_Transformer_models_an_introduction_and_catalog.pdf", "title": "Transformer models: an introduction and catalog", "year": 2023, "paper_venue": null, "authors": ["Xavier Amatriain", "Ananth Sankar", "Jie Bing", "Praveen Kumar Bodigutla", "Timothy J. Hazen", "Michaeel Kazi"], "abstract": "In the past few years we have seen the meteoric appearance of dozens of foundation models of the Transformer family, all of which have memorable and sometimes funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovations in Transformer models. Our catalog will include models that are trained using self-supervised learning (e.g., BERT or GPT3) as well as those that are further trained using a human-in-the-loop (e.g. the InstructGPT model used by ChatGPT).", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/transformer-models-an-introduction-and", "bibtex": "@misc{amatriain2023transformer,\n      title={Transformer models: an introduction and catalog}, \n      author={Xavier Amatriain and Ananth Sankar and Jie Bing and Praveen Kumar Bodigutla and Timothy J. Hazen and Michaeel Kazi},\n      year={2023},\n      eprint={2302.07730},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "12-02-2023", "categories": ["cs.CL"]}, "2305.10601v2": {"paper_id": "2305.10601v2", "abs_url": "https://arxiv.org/abs/2305.10601v2", "pdf_url": "https://arxiv.org/pdf/2305.10601v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2305.10601v2_Tree_of_Thoughts_Deliberate_Problem_Solving_with_Large_Language_Models.pdf", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "year": 2023, "paper_venue": null, "authors": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao", "Izhak Shafran", "Thomas L. Griffiths", "Yuan Cao", "Karthik Narasimhan"], "abstract": ".", "comments": "NeurIPS 2023 camera ready version. Code repo with all prompts: this https URL", "official_code_urls": ["https://github.com/princeton-nlp/tree-of-thought-llm", "https://github.com/ysymyth/tree-of-thought-llm"], "pwc_page_url": "https://paperswithcode.com/paper/tree-of-thoughts-deliberate-problem-solving-1", "bibtex": "@misc{yao2023tree,\n      title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models}, \n      author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},\n      year={2023},\n      eprint={2305.10601},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "17-05-2023", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2311.07989v3": {"paper_id": "2311.07989v3", "abs_url": "https://arxiv.org/abs/2311.07989v3", "pdf_url": "https://arxiv.org/pdf/2311.07989v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.07989v3_Unifying_the_Perspectives_of_NLP_and_Software_Engineering_A_Survey_on_Language_Models_for_Code.pdf", "title": "Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code", "year": 2023, "paper_venue": null, "authors": ["Ziyin Zhang", "Chaoyu Chen", "Bingchang Liu", "Cong Liao", "Zi Gong", "Hang Yu", "Jianguo Li", "Rui Wang"], "abstract": ".", "comments": "Repo is available at this https URL . 8 figures, 9 tables, and 694 references", "official_code_urls": ["https://github.com/codefuse-ai/awesome-code-llm"], "pwc_page_url": "https://paperswithcode.com/paper/a-survey-on-language-models-for-code", "bibtex": "@misc{zhang2023unifying,\n      title={Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code}, \n      author={Ziyin Zhang and Chaoyu Chen and Bingchang Liu and Cong Liao and Zi Gong and Hang Yu and Jianguo Li and Rui Wang},\n      year={2023},\n      eprint={2311.07989},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "14-11-2023", "categories": ["cs.CL", "cs.AI", "cs.SE"]}, "2307.13854v3": {"paper_id": "2307.13854v3", "abs_url": "https://arxiv.org/abs/2307.13854v3", "pdf_url": "https://arxiv.org/pdf/2307.13854v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2307.13854v3_WebArena_A_Realistic_Web_Environment_for_Building_Autonomous_Agents.pdf", "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents", "year": 2023, "paper_venue": null, "authors": ["Shuyan Zhou", "Frank F. Xu", "Hao Zhu", "Xuhui Zhou", "Robert Lo", "Abishek Sridhar", "Xianyi Cheng", "Tianyue Ou", "Yonatan Bisk", "Daniel Fried", "Uri Alon", "Graham Neubig"], "abstract": "With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.", "comments": "Our code, data, environment reproduction resources, and video demonstrations are publicly available at https://webarena.dev/", "official_code_urls": ["https://github.com/web-arena-x/webarena"], "pwc_page_url": "https://paperswithcode.com/paper/webarena-a-realistic-web-environment-for", "bibtex": "@misc{zhou2023webarena,\n      title={WebArena: A Realistic Web Environment for Building Autonomous Agents}, \n      author={Shuyan Zhou and Frank F. Xu and Hao Zhu and Xuhui Zhou and Robert Lo and Abishek Sridhar and Xianyi Cheng and Tianyue Ou and Yonatan Bisk and Daniel Fried and Uri Alon and Graham Neubig},\n      year={2023},\n      eprint={2307.13854},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}", "published_date": "25-07-2023", "categories": ["cs.AI", "cs.CL", "cs.LG"]}, "2306.02552v2": {"paper_id": "2306.02552v2", "abs_url": "https://arxiv.org/abs/2306.02552v2", "pdf_url": "https://arxiv.org/pdf/2306.02552v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2306.02552v2_When_Large_Language_Model_based_Agent_Meets_User_Behavior_Analysis_A_Novel_User_Simulation_Paradigm.pdf", "title": "When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm", "year": 2023, "paper_venue": null, "authors": ["Lei Wang", "Jingsen Zhang", "Hao Yang", "Zhiyuan Chen", "Jiakai Tang", "Zeyu Zhang", "Xu Chen", "Yankai Lin", "Ruihua Song", "Wayne Xin Zhao", "Jun Xu", "Zhicheng Dou", "Jun Wang", "Ji-Rong Wen"], "abstract": "}.", "comments": "26 pages, 9 figures", "official_code_urls": ["https://github.com/ruc-gsai/yulan-rec"], "pwc_page_url": "https://paperswithcode.com/paper/recagent-a-novel-simulation-paradigm-for", "bibtex": "@misc{wang2023large,\n      title={When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm}, \n      author={Lei Wang and Jingsen Zhang and Hao Yang and Zhiyuan Chen and Jiakai Tang and Zeyu Zhang and Xu Chen and Yankai Lin and Ruihua Song and Wayne Xin Zhao and Jun Xu and Zhicheng Dou and Jun Wang and Ji-Rong Wen},\n      year={2023},\n      eprint={2306.02552},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}", "published_date": "05-06-2023", "categories": ["cs.IR", "cs.AI"]}, "2308.05596v1": {"paper_id": "2308.05596v1", "abs_url": "https://arxiv.org/abs/2308.05596v1", "pdf_url": "https://arxiv.org/pdf/2308.05596v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2308.05596v1_You_Only_Prompt_Once_On_the_Capabilities_of_Prompt_Learning_on_Large_Language_Models_to_Tackle_Toxic_Content.pdf", "title": "You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content", "year": 2023, "paper_venue": null, "authors": ["Xinlei He", "Savvas Zannettou", "Yun Shen", "Yang Zhang"], "abstract": "The spread of toxic content online is an important problem that has adverse effects on user experience online and in our society at large. Motivated by the importance and impact of the problem, research focuses on developing solutions to detect toxic content, usually leveraging machine learning (ML) models trained on human-annotated datasets. While these efforts are important, these models usually do not generalize well and they can not cope with new trends (e.g., the emergence of new toxic terms). Currently, we are witnessing a shift in the approach to tackling societal issues online, particularly leveraging large language models (LLMs) like GPT-3 or T5 that are trained on vast corpora and have strong generalizability. In this work, we investigate how we can use LLMs and prompt learning to tackle the problem of toxic content, particularly focusing on three tasks; 1) Toxicity Classification, 2) Toxic Span Detection, and 3) Detoxification. We perform an extensive evaluation over five model architectures and eight datasets demonstrating that LLMs with prompt learning can achieve similar or even better performance compared to models trained on these specific tasks. We find that prompt learning achieves around 10\\% improvement in the toxicity classification task compared to the baselines, while for the toxic span detection task we find better performance to the best baseline (0.643 vs. 0.640 in terms of $F_1$-score). Finally, for the detoxification task, we find that prompt learning can successfully reduce the average toxicity score (from 0.775 to 0.213) while preserving semantic meaning.", "comments": "To Appear in the 45th IEEE Symposium on Security and Privacy, May 20-23, 2024", "official_code_urls": ["https://github.com/xinleihe/toxic-prompt"], "pwc_page_url": "https://paperswithcode.com/paper/you-only-prompt-once-on-the-capabilities-of", "bibtex": "@misc{he2023prompt,\n      title={You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content}, \n      author={Xinlei He and Savvas Zannettou and Yun Shen and Yang Zhang},\n      year={2023},\n      eprint={2308.05596},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "10-08-2023", "categories": ["cs.CL", "cs.SI"]}, "2310.16944v1": {"paper_id": "2310.16944v1", "abs_url": "https://arxiv.org/abs/2310.16944v1", "pdf_url": "https://arxiv.org/pdf/2310.16944v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2310.16944v1_Zephyr_Direct_Distillation_of_LM_Alignment.pdf", "title": "Zephyr: Direct Distillation of LM Alignment", "year": 2023, "paper_venue": null, "authors": ["Lewis Tunstall", "Edward Beeching", "Nathan Lambert", "Nazneen Rajani", "Kashif Rasul", "Younes Belkada", "Shengyi Huang", "Leandro von Werra", "Cl\u00e9mentine Fourrier", "Nathan Habib", "Nathan Sarrazin", "Omar Sanseviero", "Alexander M. Rush", "Thomas Wolf"], "abstract": ".", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/zephyr-direct-distillation-of-lm-alignment", "bibtex": "@misc{tunstall2023zephyr,\n      title={Zephyr: Direct Distillation of LM Alignment}, \n      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Cl\u00e9mentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},\n      year={2023},\n      eprint={2310.16944},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "25-10-2023", "categories": ["cs.LG", "cs.CL"]}, "2311.05584v1": {"paper_id": "2311.05584v1", "abs_url": "https://arxiv.org/abs/2311.05584v1", "pdf_url": "https://arxiv.org/pdf/2311.05584v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2311.05584v1_Zero-Shot_Goal-Directed_Dialogue_via_RL_on_Imagined_Conversations.pdf", "title": "Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations", "year": 2023, "paper_venue": null, "authors": ["Joey Hong", "Sergey Levine", "Anca Dragan"], "abstract": "Large language models (LLMs) have emerged as powerful and general solutions to many natural language tasks. However, many of the most important applications of language generation are interactive, where an agent has to talk to a person to reach a desired outcome. For example, a teacher might try to understand their student's current comprehension level to tailor their instruction accordingly, and a travel agent might ask questions of their customer to understand their preferences in order to recommend activities they might enjoy. LLMs trained with supervised fine-tuning or \"single-step\" RL, as with standard RLHF, might struggle which tasks that require such goal-directed behavior, since they are not trained to optimize for overall conversational outcomes after multiple turns of interaction. In this work, we explore a new method for adapting LLMs with RL for such goal-directed dialogue. Our key insight is that, though LLMs might not effectively solve goal-directed dialogue tasks out of the box, they can provide useful data for solving such tasks by simulating suboptimal but human-like behaviors. Given a textual description of a goal-directed dialogue task, we leverage LLMs to sample diverse synthetic rollouts of hypothetical in-domain human-human interactions. Our algorithm then utilizes this dataset with offline reinforcement learning to train an interactive conversational agent that can optimize goal-directed objectives over multiple turns. In effect, the LLM produces examples of possible interactions, and RL then processes these examples to learn to perform more optimal interactions. Empirically, we show that our proposed approach achieves state-of-the-art performance in various goal-directed dialogue tasks that include teaching and preference elicitation.", "comments": "25 pages, 6 figures", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/zero-shot-goal-directed-dialogue-via-rl-on", "bibtex": "@misc{hong2023zeroshot,\n      title={Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations}, \n      author={Joey Hong and Sergey Levine and Anca Dragan},\n      year={2023},\n      eprint={2311.05584},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "09-11-2023", "categories": ["cs.LG", "cs.AI", "cs.CL"]}}, {"1706.03762v7": {"paper_id": "1706.03762v7", "abs_url": "https://arxiv.org/abs/1706.03762v7", "pdf_url": "https://arxiv.org/pdf/1706.03762v7.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "1706.03762v7_Attention_Is_All_You_Need.pdf", "title": "Attention Is All You Need", "year": 2017, "paper_venue": null, "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "comments": "15 pages, 5 figures", "official_code_urls": ["https://github.com/tensorflow/tensor2tensor"], "pwc_page_url": "https://paperswithcode.com/paper/attention-is-all-you-need", "bibtex": "@misc{vaswani2023attention,\n      title={Attention Is All You Need}, \n      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n      year={2023},\n      eprint={1706.03762},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "12-06-2017", "categories": ["cs.CL", "cs.LG"]}}, {"1910.13461v1": {"paper_id": "1910.13461v1", "abs_url": "https://arxiv.org/abs/1910.13461v1", "pdf_url": "https://arxiv.org/pdf/1910.13461v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "1910.13461v1_BART_Denoising_Sequence-to-Sequence_Pre-training_for_Natural_Language_Generation_Translation_and_Comprehension.pdf", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019, "paper_venue": null, "authors": ["Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer"], "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/bart-denoising-sequence-to-sequence-pre", "bibtex": "@misc{lewis2019bart,\n      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, \n      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},\n      year={2019},\n      eprint={1910.13461},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "29-10-2019", "categories": ["cs.CL", "cs.LG", "stat.ML"]}, "1904.10509v1": {"paper_id": "1904.10509v1", "abs_url": "https://arxiv.org/abs/1904.10509v1", "pdf_url": "https://arxiv.org/pdf/1904.10509v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "1904.10509v1_Generating_Long_Sequences_with_Sparse_Transformers.pdf", "title": "Generating Long Sequences with Sparse Transformers", "year": 2019, "paper_venue": null, "authors": ["Rewon Child", "Scott Gray", "Alec Radford", "Ilya Sutskever"], "abstract": "Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.", "comments": "", "official_code_urls": ["https://github.com/openai/sparse_attention"], "pwc_page_url": "https://paperswithcode.com/paper/190410509", "bibtex": "@misc{child2019generating,\n      title={Generating Long Sequences with Sparse Transformers}, \n      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},\n      year={2019},\n      eprint={1904.10509},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "23-04-2019", "categories": ["cs.LG", "stat.ML"]}, "1911.12543v2": {"paper_id": "1911.12543v2", "abs_url": "https://arxiv.org/abs/1911.12543v2", "pdf_url": "https://arxiv.org/pdf/1911.12543v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "1911.12543v2_How_Can_We_Know_What_Language_Models_Know?.pdf", "title": "How Can We Know What Language Models Know?", "year": 2019, "paper_venue": null, "authors": ["Zhengbao Jiang", "Frank F. Xu", "Jun Araki", "Graham Neubig"], "abstract": ".", "comments": "TACL 2020", "official_code_urls": ["https://github.com/jzbjyb/LPAQA"], "pwc_page_url": "https://paperswithcode.com/paper/how-can-we-know-what-language-models-know", "bibtex": "@misc{jiang2020know,\n      title={How Can We Know What Language Models Know?}, \n      author={Zhengbao Jiang and Frank F. Xu and Jun Araki and Graham Neubig},\n      year={2020},\n      eprint={1911.12543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "28-11-2019", "categories": ["cs.CL", "cs.LG"]}, "1909.01066v2": {"paper_id": "1909.01066v2", "abs_url": "https://arxiv.org/abs/1909.01066v2", "pdf_url": "https://arxiv.org/pdf/1909.01066v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "1909.01066v2_Language_Models_as_Knowledge_Bases?.pdf", "title": "Language Models as Knowledge Bases?", "year": 2019, "paper_venue": null, "authors": ["Fabio Petroni", "Tim Rockt\u00e4schel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander H. Miller", "Sebastian Riedel"], "abstract": ".", "comments": "accepted at EMNLP 2019", "official_code_urls": ["https://github.com/facebookresearch/LAMA"], "pwc_page_url": "https://paperswithcode.com/paper/language-models-as-knowledge-bases", "bibtex": "@misc{petroni2019language,\n      title={Language Models as Knowledge Bases?}, \n      author={Fabio Petroni and Tim Rockt\u00e4schel and Patrick Lewis and Anton Bakhtin and Yuxiang Wu and Alexander H. Miller and Sebastian Riedel},\n      year={2019},\n      eprint={1909.01066},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "03-09-2019", "categories": ["cs.CL"]}, "1909.08053v4": {"paper_id": "1909.08053v4", "abs_url": "https://arxiv.org/abs/1909.08053v4", "pdf_url": "https://arxiv.org/pdf/1909.08053v4.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "1909.08053v4_Megatron-LM_Training_Multi-Billion_Parameter_Language_Models_Using_Model_Parallelism.pdf", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism", "year": 2019, "paper_venue": null, "authors": ["Mohammad Shoeybi", "Mostofa Patwary", "Raul Puri", "Patrick LeGresley", "Jared Casper", "Bryan Catanzaro"], "abstract": "Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).", "comments": "", "official_code_urls": ["https://github.com/NVIDIA/Megatron-LM"], "pwc_page_url": "https://paperswithcode.com/paper/megatron-lm-training-multi-billion-parameter", "bibtex": "@misc{shoeybi2020megatronlm,\n      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, \n      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},\n      year={2020},\n      eprint={1909.08053},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "17-09-2019", "categories": ["cs.CL"]}, "1912.10165v1": {"paper_id": "1912.10165v1", "abs_url": "https://arxiv.org/abs/1912.10165v1", "pdf_url": "https://arxiv.org/pdf/1912.10165v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "1912.10165v1_Zero-shot_Text_Classification_With_Generative_Language_Models.pdf", "title": "Zero-shot Text Classification With Generative Language Models", "year": 2019, "paper_venue": null, "authors": ["Raul Puri", "Bryan Catanzaro"], "abstract": "This work investigates the use of natural language to enable zero-shot model adaptation to new tasks. We use text and metadata from social commenting platforms as a source for a simple pretraining task. We then provide the language model with natural language descriptions of classification tasks as input and train it to generate the correct answer in natural language via a language modeling objective. This allows the model to generalize to new classification tasks without the need for multiple multitask classification heads. We show the zero-shot performance of these generative language models, trained with weak supervision, on six benchmark text classification datasets from the torchtext library. Despite no access to training data, we achieve up to a 45% absolute improvement in classification accuracy over random or majority class baselines. These results show that natural language can serve as simple and powerful descriptors for task adaptation. We believe this points the way to new metalearning strategies for text problems.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/zero-shot-text-classification-with-generative", "bibtex": "@misc{puri2019zeroshot,\n      title={Zero-shot Text Classification With Generative Language Models}, \n      author={Raul Puri and Bryan Catanzaro},\n      year={2019},\n      eprint={1912.10165},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "10-12-2019", "categories": ["cs.CL"]}}, {"2104.04670v5": {"paper_id": "2104.04670v5", "abs_url": "https://arxiv.org/abs/2104.04670v5", "pdf_url": "https://arxiv.org/pdf/2104.04670v5.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2104.04670v5_Adapting_Language_Models_for_Zero-shot_Learning_by_Meta-tuning_on_Dataset_and_Prompt_Collections.pdf", "title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections", "year": 2021, "paper_venue": null, "authors": ["Ruiqi Zhong", "Kristy Lee", "Zheng Zhang", "Dan Klein"], "abstract": "Large pre-trained language models (LMs) such as GPT-3 have acquired a surprising ability to perform zero-shot learning. For example, to classify sentiment without any training examples, we can \"prompt\" the LM with the review and the label description \"Does the user like this movie?\", and ask whether the next word is \"yes\" or \"no\". However, the next word prediction training objective is still misaligned with the target zero-shot learning objective. To address this weakness, we propose meta-tuning, which directly optimizes the zero-shot learning objective by fine-tuning pre-trained language models on a collection of datasets. We focus on classification tasks, and construct the meta-dataset by aggregating 43 existing datasets and annotating 441 label descriptions in a question-answering (QA) format. When evaluated on unseen tasks, meta-tuned models outperform a same-sized QA model and the previous SOTA zero-shot learning system based on natural language inference. Additionally, increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%, and we forecast that even larger models would perform better. Therefore, measuring zero-shot learning performance on language models out-of-the-box might underestimate their true potential, and community-wide efforts on aggregating datasets and unifying their formats can help build models that answer prompts better.", "comments": "EMNLP 2021, Findings", "official_code_urls": ["https://github.com/ruiqi-zhong/Meta-tuning"], "pwc_page_url": "https://paperswithcode.com/paper/meta-tuning-language-models-to-answer-prompts", "bibtex": "@misc{zhong2021adapting,\n      title={Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections}, \n      author={Ruiqi Zhong and Kristy Lee and Zheng Zhang and Dan Klein},\n      year={2021},\n      eprint={2104.04670},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "10-04-2021", "categories": ["cs.CL", "cs.AI"]}, "2109.04144v1": {"paper_id": "2109.04144v1", "abs_url": "https://arxiv.org/abs/2109.04144v1", "pdf_url": "https://arxiv.org/pdf/2109.04144v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.04144v1_Avoiding_Inference_Heuristics_in_Few-shot_Prompt-based_Finetuning.pdf", "title": "Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning", "year": 2021, "paper_venue": null, "authors": ["Prasetya Ajie Utama", "Nafise Sadat Moosavi", "Victor Sanh", "Iryna Gurevych"], "abstract": "Recent prompt-based approaches allow pretrained language models to achieve strong performances on few-shot finetuning by reformulating downstream tasks as a language modeling problem. In this work, we demonstrate that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting inference heuristics based on lexical overlap, e.g., models incorrectly assuming a sentence pair is of the same meaning because they consist of the same set of words. Interestingly, we find that this particular inference heuristic is significantly less present in the zero-shot evaluation of the prompt-based model, indicating how finetuning can be destructive to useful knowledge learned during the pretraining. We then show that adding a regularization that preserves pretraining weights is effective in mitigating this destructive tendency of few-shot finetuning. Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics.", "comments": "Accepted at EMNLP 2021", "official_code_urls": ["https://github.com/ukplab/emnlp2021-prompt-ft-heuristics"], "pwc_page_url": "https://paperswithcode.com/paper/avoiding-inference-heuristics-in-few-shot", "bibtex": "@misc{utama2021avoiding,\n      title={Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning}, \n      author={Prasetya Ajie Utama and Nafise Sadat Moosavi and Victor Sanh and Iryna Gurevych},\n      year={2021},\n      eprint={2109.04144},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "09-09-2021", "categories": ["cs.CL", "cs.AI"]}, "2109.04645v4": {"paper_id": "2109.04645v4", "abs_url": "https://arxiv.org/abs/2109.04645v4", "pdf_url": "https://arxiv.org/pdf/2109.04645v4.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.04645v4_CINS_Comprehensive_Instruction_for_Few-shot_Learning_in_Task-oriented_Dialog_Systems.pdf", "title": "CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems", "year": 2021, "paper_venue": null, "authors": ["Fei Mi", "Yitong Li", "Yasheng Wang", "Xin Jiang", "Qun Liu"], "abstract": "As labeling cost for different modules in task-oriented dialog (ToD) systems is high, a major challenge in practice is to learn different tasks with the least amount of labeled data. Recently, prompting methods over pre-trained language models (PLMs) have shown promising results for few-shot learning in ToD. To better utilize the power of PLMs, this paper proposes Comprehensive Instruction (CINS) that exploits PLMs with extra task-specific instructions. We design a schema (definition, constraint, prompt) of instructions and their customized realizations for three important downstream tasks in ToD, i.e. intent classification, dialog state tracking, and natural language generation. A sequence-to-sequence model (T5) is adopted to solve these three tasks in a unified framework. Extensive experiments are conducted on these ToD tasks in realistic few-shot learning scenarios with small validation data. Empirical results demonstrate that the proposed CINS approach consistently improves techniques that finetune PLMs with raw input or short prompts.", "comments": "Accepted at AAAI2022", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/cins-comprehensive-instruction-for-few-shot", "bibtex": "@misc{mi2022cins,\n      title={CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems}, \n      author={Fei Mi and Yitong Li and Yasheng Wang and Xin Jiang and Qun Liu},\n      year={2022},\n      eprint={2109.04645},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "10-09-2021", "categories": ["cs.CL", "cs.LG"]}, "2102.09690v2": {"paper_id": "2102.09690v2", "abs_url": "https://arxiv.org/abs/2102.09690v2", "pdf_url": "https://arxiv.org/pdf/2102.09690v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2102.09690v2_Calibrate_Before_Use_Improving_Few-Shot_Performance_of_Language_Models.pdf", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "year": 2021, "paper_venue": null, "authors": ["Tony Z. Zhao", "Eric Wallace", "Shi Feng", "Dan Klein", "Sameer Singh"], "abstract": "GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.", "comments": "ICML 2021", "official_code_urls": ["https://github.com/tonyzhaozh/few-shot-learning"], "pwc_page_url": "https://paperswithcode.com/paper/calibrate-before-use-improving-few-shot", "bibtex": "@misc{zhao2021calibrate,\n      title={Calibrate Before Use: Improving Few-Shot Performance of Language Models}, \n      author={Tony Z. Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh},\n      year={2021},\n      eprint={2102.09690},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "19-02-2021", "categories": ["cs.CL", "cs.LG"]}, "2109.07154v1": {"paper_id": "2109.07154v1", "abs_url": "https://arxiv.org/abs/2109.07154v1", "pdf_url": "https://arxiv.org/pdf/2109.07154v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.07154v1_Can_Language_Models_be_Biomedical_Knowledge_Bases?.pdf", "title": "Can Language Models be Biomedical Knowledge Bases?", "year": 2021, "paper_venue": null, "authors": ["Mujeen Sung", "Jinhyuk Lee", "Sean Yi", "Minji Jeon", "Sungdong Kim", "Jaewoo Kang"], "abstract": "Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51% Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing.", "comments": "EMNLP 2021. Code available at this https URL", "official_code_urls": ["https://github.com/dmis-lab/biolama"], "pwc_page_url": "https://paperswithcode.com/paper/can-language-models-be-biomedical-knowledge", "bibtex": "@misc{sung2021language,\n      title={Can Language Models be Biomedical Knowledge Bases?}, \n      author={Mujeen Sung and Jinhyuk Lee and Sean Yi and Minji Jeon and Sungdong Kim and Jaewoo Kang},\n      year={2021},\n      eprint={2109.07154},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "15-09-2021", "categories": ["cs.CL"]}, "2112.01488v3": {"paper_id": "2112.01488v3", "abs_url": "https://arxiv.org/abs/2112.01488v3", "pdf_url": "https://arxiv.org/pdf/2112.01488v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2112.01488v3_ColBERTv2_Effective_and_Efficient_Retrieval_via_Lightweight_Late_Interaction.pdf", "title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction", "year": 2021, "paper_venue": null, "authors": ["Keshav Santhanam", "Omar Khattab", "Jon Saad-Falcon", "Christopher Potts", "Matei Zaharia"], "abstract": "Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6--10$\\times$.", "comments": "NAACL 2022. Omar and Keshav contributed equally to this work", "official_code_urls": ["https://github.com/stanford-futuredata/ColBERT"], "pwc_page_url": "https://paperswithcode.com/paper/colbertv2-effective-and-efficient-retrieval", "bibtex": "@misc{santhanam2022colbertv2,\n      title={ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction}, \n      author={Keshav Santhanam and Omar Khattab and Jon Saad-Falcon and Christopher Potts and Matei Zaharia},\n      year={2022},\n      eprint={2112.01488},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}", "published_date": "02-12-2021", "categories": ["cs.IR", "cs.CL"]}, "2106.13353v2": {"paper_id": "2106.13353v2", "abs_url": "https://arxiv.org/abs/2106.13353v2", "pdf_url": "https://arxiv.org/pdf/2106.13353v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2106.13353v2_Cutting_Down_on_Prompts_and_Parameters_Simple_Few-Shot_Learning_with_Language_Models.pdf", "title": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models", "year": 2021, "paper_venue": null, "authors": ["Robert L. Logan IV", "Ivana Bala\u017eevi\u0107", "Eric Wallace", "Fabio Petroni", "Sameer Singh", "Sebastian Riedel"], "abstract": "Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. In fact, one can use null prompts, prompts that contain neither task-specific templates nor training examples, and achieve competitive accuracy to manually-tuned prompts across a wide range of tasks. While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1% of the parameters. All in all, we recommend finetuning LMs for few-shot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs.", "comments": "", "official_code_urls": ["https://github.com/ucinlp/null-prompts"], "pwc_page_url": "https://paperswithcode.com/paper/cutting-down-on-prompts-and-parameters-simple", "bibtex": "@misc{logan2021cutting,\n      title={Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models}, \n      author={Robert L. Logan IV au2 and Ivana Bala\u017eevi\u0107 and Eric Wallace and Fabio Petroni and Sameer Singh and Sebastian Riedel},\n      year={2021},\n      eprint={2106.13353},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "24-06-2021", "categories": ["cs.CL", "cs.LG"]}, "2107.06499v2": {"paper_id": "2107.06499v2", "abs_url": "https://arxiv.org/abs/2107.06499v2", "pdf_url": "https://arxiv.org/pdf/2107.06499v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2107.06499v2_Deduplicating_Training_Data_Makes_Language_Models_Better.pdf", "title": "Deduplicating Training Data Makes Language Models Better", "year": 2021, "paper_venue": null, "authors": ["Katherine Lee", "Daphne Ippolito", "Andrew Nystrom", "Chiyuan Zhang", "Douglas Eck", "Chris Callison-Burch", "Nicholas Carlini"], "abstract": ".", "comments": "Accepted to ACL 2022", "official_code_urls": ["https://github.com/google-research/deduplicate-text-datasets"], "pwc_page_url": "https://paperswithcode.com/paper/deduplicating-training-data-makes-language", "bibtex": "@misc{lee2022deduplicating,\n      title={Deduplicating Training Data Makes Language Models Better}, \n      author={Katherine Lee and Daphne Ippolito and Andrew Nystrom and Chiyuan Zhang and Douglas Eck and Chris Callison-Burch and Nicholas Carlini},\n      year={2022},\n      eprint={2107.06499},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "14-07-2021", "categories": ["cs.CL", "cs.LG"]}, "2108.13161v7": {"paper_id": "2108.13161v7", "abs_url": "https://arxiv.org/abs/2108.13161v7", "pdf_url": "https://arxiv.org/pdf/2108.13161v7.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2108.13161v7_Differentiable_Prompt_Makes_Pre-trained_Language_Models_Better_Few-shot_Learners.pdf", "title": "Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners", "year": 2021, "paper_venue": null, "authors": ["Ningyu Zhang", "Luoqiu Li", "Xiang Chen", "Shumin Deng", "Zhen Bi", "Chuanqi Tan", "Fei Huang", "Huajun Chen"], "abstract": ".", "comments": "Accepted by ICLR 2022", "official_code_urls": ["https://github.com/zjunlp/DART"], "pwc_page_url": "https://paperswithcode.com/paper/differentiable-prompt-makes-pre-trained", "bibtex": "@misc{zhang2022differentiable,\n      title={Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners}, \n      author={Ningyu Zhang and Luoqiu Li and Xiang Chen and Shumin Deng and Zhen Bi and Chuanqi Tan and Fei Huang and Huajun Chen},\n      year={2022},\n      eprint={2108.13161},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "30-08-2021", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"]}, "2109.03630v1": {"paper_id": "2109.03630v1", "abs_url": "https://arxiv.org/abs/2109.03630v1", "pdf_url": "https://arxiv.org/pdf/2109.03630v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.03630v1_Discrete_and_Soft_Prompting_for_Multilingual_Models.pdf", "title": "Discrete and Soft Prompting for Multilingual Models", "year": 2021, "paper_venue": null, "authors": ["Mengjie Zhao", "Hinrich Sch\u00fctze"], "abstract": "It has been shown for English that discrete and soft prompting perform strongly in few-shot learning with pretrained language models (PLMs). In this paper, we show that discrete and soft prompting perform better than finetuning in multilingual cases: Crosslingual transfer and in-language training of multilingual natural language inference. For example, with 48 English training examples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely surpassing the majority baseline (33.33%). In contrast, discrete and soft prompting outperform finetuning, achieving 36.43% and 38.79%. We also demonstrate good performance of prompting with training data in multiple languages other than English.", "comments": "EMNLP 2021", "official_code_urls": ["https://github.com/mprompting/xlmrprompt"], "pwc_page_url": "https://paperswithcode.com/paper/discrete-and-soft-prompting-for-multilingual", "bibtex": "@misc{zhao2021discrete,\n      title={Discrete and Soft Prompting for Multilingual Models}, \n      author={Mengjie Zhao and Hinrich Sch\u00fctze},\n      year={2021},\n      eprint={2109.03630},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "08-09-2021", "categories": ["cs.CL"]}, "2109.01247v2": {"paper_id": "2109.01247v2", "abs_url": "https://arxiv.org/abs/2109.01247v2", "pdf_url": "https://arxiv.org/pdf/2109.01247v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.01247v2_Do_Prompt-Based_Models_Really_Understand_the_Meaning_of_their_Prompts?.pdf", "title": "Do Prompt-Based Models Really Understand the Meaning of their Prompts?", "year": 2021, "paper_venue": null, "authors": ["Albert Webson", "Ellie Pavlick"], "abstract": "Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompt templates manually written for natural language inference (NLI). We find that models learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively \"good\" prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2022). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models' impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans' use of task instructions.", "comments": "NAACL 2022. Unabridged version. Code available at this https URL", "official_code_urls": ["https://github.com/awebson/prompt_semantics"], "pwc_page_url": "https://paperswithcode.com/paper/do-prompt-based-models-really-understand-the", "bibtex": "@misc{webson2022promptbased,\n      title={Do Prompt-Based Models Really Understand the Meaning of their Prompts?}, \n      author={Albert Webson and Ellie Pavlick},\n      year={2022},\n      eprint={2109.01247},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "02-09-2021", "categories": ["cs.CL"]}, "2109.06513v2": {"paper_id": "2109.06513v2", "abs_url": "https://arxiv.org/abs/2109.06513v2", "pdf_url": "https://arxiv.org/pdf/2109.06513v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.06513v2_Exploring_Prompt-based_Few-shot_Learning_for_Grounded_Dialog_Generation.pdf", "title": "Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation", "year": 2021, "paper_venue": null, "authors": ["Chujie Zheng", "Minlie Huang"], "abstract": "Dialog models can be greatly strengthened through grounding on various external information, but grounded dialog corpora are usually not naturally accessible. In this work, we focus on the few-shot learning for grounded dialog generation (GDG). We first propose a simple prompting method for GDG tasks, where different constructs of model input, such as the grounding source and the conversation context, are distinguished through continuous or discrete prompts. On three typical GDG tasks, we empirically demonstrate and analyze in-depth the effectiveness of our method. We then conduct extensive experiments to thoroughly investigate how our prompting method works with different pre-trained models. We show that prompted language models perform superiorly to conversational models, and further analyze various factors that influence the effects of prompting. Overall, our work introduces a prompt-based perspective to the few-shot learning for GDG tasks, and provides valuable findings and insights for future research.", "comments": "Work in progress", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/exploring-prompt-based-few-shot-learning-for", "bibtex": "@misc{zheng2022exploring,\n      title={Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation}, \n      author={Chujie Zheng and Minlie Huang},\n      year={2022},\n      eprint={2109.06513},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "14-09-2021", "categories": ["cs.CL"]}, "2104.08786v2": {"paper_id": "2104.08786v2", "abs_url": "https://arxiv.org/abs/2104.08786v2", "pdf_url": "https://arxiv.org/pdf/2104.08786v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2104.08786v2_Fantastically_Ordered_Prompts_and_Where_to_Find_Them_Overcoming_Few-Shot_Prompt_Order_Sensitivity.pdf", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity", "year": 2021, "paper_venue": null, "authors": ["Yao Lu", "Max Bartolo", "Alastair Moore", "Sebastian Riedel", "Pontus Stenetorp"], "abstract": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are \"fantastic\" and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.", "comments": "ACL 2022", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/fantastically-ordered-prompts-and-where-to", "bibtex": "@misc{lu2022fantastically,\n      title={Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity}, \n      author={Yao Lu and Max Bartolo and Alastair Moore and Sebastian Riedel and Pontus Stenetorp},\n      year={2022},\n      eprint={2104.08786},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "18-04-2021", "categories": ["cs.CL", "cs.AI"]}, "2108.06098v3": {"paper_id": "2108.06098v3", "abs_url": "https://arxiv.org/abs/2108.06098v3", "pdf_url": "https://arxiv.org/pdf/2108.06098v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2108.06098v3_FedPara_Low-Rank_Hadamard_Product_for_Communication-Efficient_Federated_Learning.pdf", "title": "FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning", "year": 2021, "paper_venue": null, "authors": ["Nam Hyeon-Woo", "Moon Ye-Bin", "Tae-Hyun Oh"], "abstract": "In this work, we propose a communication-efficient parameterization, FedPara, for federated learning (FL) to overcome the burdens on frequent model uploads and downloads. Our method re-parameterizes weight parameters of layers using low-rank weights followed by the Hadamard product. Compared to the conventional low-rank parameterization, our FedPara method is not restricted to low-rank constraints, and thereby it has a far larger capacity. This property enables to achieve comparable performance while requiring 3 to 10 times lower communication costs than the model with the original layers, which is not achievable by the traditional low-rank methods. The efficiency of our method can be further improved by combining with other efficient FL optimizers. In addition, we extend our method to a personalized FL application, pFedPara, which separates parameters into global and local ones. We show that pFedPara outperforms competing personalized FL methods with more than three times fewer parameters.", "comments": "Accepted at ICLR 2022", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/fedpara-low-rank-hadamard-product", "bibtex": "@misc{hyeonwoo2023fedpara,\n      title={FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning}, \n      author={Nam Hyeon-Woo and Moon Ye-Bin and Tae-Hyun Oh},\n      year={2023},\n      eprint={2108.06098},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "13-08-2021", "categories": ["cs.LG", "cs.CV"]}, "2109.01652v5": {"paper_id": "2109.01652v5", "abs_url": "https://arxiv.org/abs/2109.01652v5", "pdf_url": "https://arxiv.org/pdf/2109.01652v5.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.01652v5_Finetuned_Language_Models_Are_Zero-Shot_Learners.pdf", "title": "Finetuned Language Models Are Zero-Shot Learners", "year": 2021, "paper_venue": null, "authors": ["Jason Wei", "Maarten Bosma", "Vincent Y. Zhao", "Kelvin Guu", "Adams Wei Yu", "Brian Lester", "Nan Du", "Andrew M. Dai", "Quoc V. Le"], "abstract": "We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.", "comments": "Version 5. Find list of changes in Appendix F (page 35)", "official_code_urls": ["https://github.com/google-research/flan"], "pwc_page_url": "https://paperswithcode.com/paper/finetuned-language-models-are-zero-shot", "bibtex": "@misc{wei2022finetuned,\n      title={Finetuned Language Models Are Zero-Shot Learners}, \n      author={Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},\n      year={2022},\n      eprint={2109.01652},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "03-09-2021", "categories": ["cs.CL"]}, "2103.10385v2": {"paper_id": "2103.10385v2", "abs_url": "https://arxiv.org/abs/2103.10385v2", "pdf_url": "https://arxiv.org/pdf/2103.10385v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2103.10385v2_GPT_Understands_Too.pdf", "title": "GPT Understands, Too", "year": 2021, "paper_venue": null, "authors": ["Xiao Liu", "Yanan Zheng", "Zhengxiao Du", "Ming Ding", "Yujie Qian", "Zhilin Yang", "Jie Tang"], "abstract": "Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance -- e.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.", "comments": "", "official_code_urls": ["https://github.com/THUDM/P-tuning"], "pwc_page_url": "https://paperswithcode.com/paper/gpt-understands-too", "bibtex": "@misc{liu2023gpt,\n      title={GPT Understands, Too}, \n      author={Xiao Liu and Yanan Zheng and Zhengxiao Du and Ming Ding and Yujie Qian and Zhilin Yang and Jie Tang},\n      year={2023},\n      eprint={2103.10385},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "18-03-2021", "categories": ["cs.CL", "cs.LG"]}, "2104.08826v2": {"paper_id": "2104.08826v2", "abs_url": "https://arxiv.org/abs/2104.08826v2", "pdf_url": "https://arxiv.org/pdf/2104.08826v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2104.08826v2_GPT3Mix_Leveraging_Large-scale_Language_Models_for_Text_Augmentation.pdf", "title": "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation", "year": 2021, "paper_venue": null, "authors": ["Kang Min Yoo", "Dongju Park", "Jaewook Kang", "Sang-Woo Lee", "Woomyeong Park"], "abstract": "Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts. Recent studies report that prompt-based direct classification eliminates the need for fine-tuning but lacks data and inference scalability. This paper proposes a novel data augmentation technique that leverages large-scale language models to generate realistic text samples from a mixture of real samples. We also propose utilizing soft-labels predicted by the language models, effectively distilling knowledge from the large-scale language models and creating textual perturbations simultaneously. We perform data augmentation experiments on diverse classification tasks and show that our method hugely outperforms existing text augmentation methods. Ablation studies and a qualitative analysis provide more insights into our approach.", "comments": "Accepted to EMNLP2021 Findings; 11 pages, 7 tables, 2 figures", "official_code_urls": ["https://github.com/naver-ai/hypermix"], "pwc_page_url": "https://paperswithcode.com/paper/gpt3mix-leveraging-large-scale-language", "bibtex": "@misc{yoo2021gpt3mix,\n      title={GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation}, \n      author={Kang Min Yoo and Dongju Park and Jaewook Kang and Sang-Woo Lee and Woomyeong Park},\n      year={2021},\n      eprint={2104.08826},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "18-04-2021", "categories": ["cs.CL", "cs.AI"]}, "2109.02593v1": {"paper_id": "2109.02593v1", "abs_url": "https://arxiv.org/abs/2109.02593v1", "pdf_url": "https://arxiv.org/pdf/2109.02593v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.02593v1_General-Purpose_Question-Answering_with_Macaw.pdf", "title": "General-Purpose Question-Answering with Macaw", "year": 2021, "paper_venue": null, "authors": ["Oyvind Tafjord", "Peter Clark"], "abstract": "", "comments": "", "official_code_urls": ["https://github.com/allenai/macaw"], "pwc_page_url": "https://paperswithcode.com/paper/general-purpose-question-answering-with-macaw", "bibtex": "@misc{tafjord2021generalpurpose,\n      title={General-Purpose Question-Answering with Macaw}, \n      author={Oyvind Tafjord and Peter Clark},\n      year={2021},\n      eprint={2109.02593},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "06-09-2021", "categories": ["cs.CL", "cs.AI"]}, "2110.08387v3": {"paper_id": "2110.08387v3", "abs_url": "https://arxiv.org/abs/2110.08387v3", "pdf_url": "https://arxiv.org/pdf/2110.08387v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2110.08387v3_Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf", "title": "Generated Knowledge Prompting for Commonsense Reasoning", "year": 2021, "paper_venue": null, "authors": ["Jiacheng Liu", "Alisa Liu", "Ximing Lu", "Sean Welleck", "Peter West", "Ronan Le Bras", "Yejin Choi", "Hannaneh Hajishirzi"], "abstract": "", "comments": "ACL 2022 main conference", "official_code_urls": ["https://github.com/liujch1998/gkp"], "pwc_page_url": "https://paperswithcode.com/paper/generated-knowledge-prompting-for-commonsense", "bibtex": "@misc{liu2022generated,\n      title={Generated Knowledge Prompting for Commonsense Reasoning}, \n      author={Jiacheng Liu and Alisa Liu and Ximing Lu and Sean Welleck and Peter West and Ronan Le Bras and Yejin Choi and Hannaneh Hajishirzi},\n      year={2022},\n      eprint={2110.08387},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "15-10-2021", "categories": ["cs.CL"]}, "2104.07540v3": {"paper_id": "2104.07540v3", "abs_url": "https://arxiv.org/abs/2104.07540v3", "pdf_url": "https://arxiv.org/pdf/2104.07540v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2104.07540v3_Generating_Datasets_with_Pretrained_Language_Models.pdf", "title": "Generating Datasets with Pretrained Language Models", "year": 2021, "paper_venue": null, "authors": ["Timo Schick", "Hinrich Sch\u00fctze"], "abstract": "To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.", "comments": "Accepted at EMNLP2021", "official_code_urls": ["https://github.com/timoschick/dino"], "pwc_page_url": "https://paperswithcode.com/paper/generating-datasets-with-pretrained-language", "bibtex": "@misc{schick2021generating,\n      title={Generating Datasets with Pretrained Language Models}, \n      author={Timo Schick and Hinrich Sch\u00fctze},\n      year={2021},\n      eprint={2104.07540},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "15-04-2021", "categories": ["cs.CL", "cs.LG"]}, "2103.08493v2": {"paper_id": "2103.08493v2", "abs_url": "https://arxiv.org/abs/2103.08493v2", "pdf_url": "https://arxiv.org/pdf/2103.08493v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2103.08493v2_How_Many_Data_Points_is_a_Prompt_Worth?.pdf", "title": "How Many Data Points is a Prompt Worth?", "year": 2021, "paper_venue": null, "authors": ["Teven Le Scao", "Alexander M. Rush"], "abstract": "When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.", "comments": "NAACL HLT 2021", "official_code_urls": ["https://github.com/TevenLeScao/pet"], "pwc_page_url": "https://paperswithcode.com/paper/how-many-data-points-is-a-prompt-worth", "bibtex": "@misc{scao2021data,\n      title={How Many Data Points is a Prompt Worth?}, \n      author={Teven Le Scao and Alexander M. Rush},\n      year={2021},\n      eprint={2103.08493},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "15-03-2021", "categories": ["cs.LG"]}, "2108.02035v2": {"paper_id": "2108.02035v2", "abs_url": "https://arxiv.org/abs/2108.02035v2", "pdf_url": "https://arxiv.org/pdf/2108.02035v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2108.02035v2_Knowledgeable_Prompt-tuning_Incorporating_Knowledge_into_Prompt_Verbalizer_for_Text_Classification.pdf", "title": "Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification", "year": 2021, "paper_venue": null, "authors": ["Shengding Hu", "Ning Ding", "Huadong Wang", "Zhiyuan Liu", "Jingang Wang", "Juanzi Li", "Wei Wu", "Maosong Sun"], "abstract": "Tuning pre-trained language models (PLMs) with task-specific prompts has been a promising approach for text classification. Particularly, previous studies suggest that prompt-tuning has remarkable superiority in the low-data scenario over the generic fine-tuning methods with extra classifiers. The core idea of prompt-tuning is to insert text pieces, i.e., template, to the input and transform a classification problem into a masked language modeling problem, where a crucial step is to construct a projection, i.e., verbalizer, between a label space and a label word space. A verbalizer is usually handcrafted or searched by gradient descent, which may lack coverage and bring considerable bias and high variances to the results. In this work, we focus on incorporating external knowledge into the verbalizer, forming a knowledgeable prompt-tuning (KPT), to improve and stabilize prompt-tuning. Specifically, we expand the label word space of the verbalizer using external knowledge bases (KBs) and refine the expanded label word space with the PLM itself before predicting with the expanded label word space. Extensive experiments on zero and few-shot text classification tasks demonstrate the effectiveness of knowledgeable prompt-tuning.", "comments": "ACL 2022 main", "official_code_urls": ["https://github.com/thunlp/knowledgeableprompttuning"], "pwc_page_url": "https://paperswithcode.com/paper/knowledgeable-prompt-tuning-incorporating", "bibtex": "@misc{hu2022knowledgeable,\n      title={Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification}, \n      author={Shengding Hu and Ning Ding and Huadong Wang and Zhiyuan Liu and Jingang Wang and Juanzi Li and Wei Wu and Maosong Sun},\n      year={2022},\n      eprint={2108.02035},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "04-08-2021", "categories": ["cs.CL"]}, "2109.07684v1": {"paper_id": "2109.07684v1", "abs_url": "https://arxiv.org/abs/2109.07684v1", "pdf_url": "https://arxiv.org/pdf/2109.07684v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.07684v1_Language_Models_are_Few-shot_Multilingual_Learners.pdf", "title": "Language Models are Few-shot Multilingual Learners", "year": 2021, "paper_venue": null, "authors": ["Genta Indra Winata", "Andrea Madotto", "Zhaojiang Lin", "Rosanne Liu", "Jason Yosinski", "Pascale Fung"], "abstract": "General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models.", "comments": "14 pages", "official_code_urls": ["https://github.com/gentaiscool/few-shot-lm"], "pwc_page_url": "https://paperswithcode.com/paper/language-models-are-few-shot-multilingual", "bibtex": "@misc{winata2021language,\n      title={Language Models are Few-shot Multilingual Learners}, \n      author={Genta Indra Winata and Andrea Madotto and Zhaojiang Lin and Rosanne Liu and Jason Yosinski and Pascale Fung},\n      year={2021},\n      eprint={2109.07684},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "16-09-2021", "categories": ["cs.CL", "cs.AI"]}, "2104.06599v1": {"paper_id": "2104.06599v1", "abs_url": "https://arxiv.org/abs/2104.06599v1", "pdf_url": "https://arxiv.org/pdf/2104.06599v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2104.06599v1_Learning_How_to_Ask_Querying_LMs_with_Mixtures_of_Soft_Prompts.pdf", "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts", "year": 2021, "paper_venue": null, "authors": ["Guanghui Qin", "Jason Eisner"], "abstract": "Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-the-blank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al., 2020). For example, language models retain factual knowledge from their training corpora that can be extracted by asking them to \"fill in the blank\" in a sentential prompt. However, where does this prompt come from? We explore the idea of learning prompts by gradient descent -- either fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of \"soft words,\" i.e., continuous vectors that are not necessarily word type embeddings from the language model. Furthermore, for each task, we optimize a mixture of prompts, learning which prompts are most effective and how to ensemble them. Across multiple English LMs and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in language models was previously underestimated. Moreover, this knowledge is cheap to elicit: random initialization is nearly as good as informed initialization.", "comments": "NAACL-HLT 2021 camera-ready", "official_code_urls": ["https://github.com/hiaoxui/soft-prompts"], "pwc_page_url": "https://paperswithcode.com/paper/learning-how-to-ask-querying-lms-with", "bibtex": "@misc{qin2021learning,\n      title={Learning How to Ask: Querying LMs with Mixtures of Soft Prompts}, \n      author={Guanghui Qin and Jason Eisner},\n      year={2021},\n      eprint={2104.06599},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "14-04-2021", "categories": ["cs.CL", "cs.LG"]}, "2103.00020v1": {"paper_id": "2103.00020v1", "abs_url": "https://arxiv.org/abs/2103.00020v1", "pdf_url": "https://arxiv.org/pdf/2103.00020v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2103.00020v1_Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021, "paper_venue": null, "authors": ["Alec Radford", "Jong Wook Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark", "Gretchen Krueger", "Ilya Sutskever"], "abstract": ".", "comments": "", "official_code_urls": ["https://github.com/openai/CLIP"], "pwc_page_url": "https://paperswithcode.com/paper/learning-transferable-visual-models-from", "bibtex": "@misc{radford2021learning,\n      title={Learning Transferable Visual Models From Natural Language Supervision}, \n      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\n      year={2021},\n      eprint={2103.00020},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}", "published_date": "26-02-2021", "categories": ["cs.CV", "cs.LG"]}, "2106.09685v2": {"paper_id": "2106.09685v2", "abs_url": "https://arxiv.org/abs/2106.09685v2", "pdf_url": "https://arxiv.org/pdf/2106.09685v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2106.09685v2_LoRA_Low-Rank_Adaptation_of_Large_Language_Models.pdf", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "year": 2021, "paper_venue": null, "authors": ["Edward J. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Lu Wang", "Weizhu Chen"], "abstract": ".", "comments": "Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency", "official_code_urls": ["https://github.com/microsoft/LoRA"], "pwc_page_url": "https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language", "bibtex": "@misc{hu2021lora,\n      title={LoRA: Low-Rank Adaptation of Large Language Models}, \n      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},\n      year={2021},\n      eprint={2106.09685},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "17-06-2021", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2110.08207v3": {"paper_id": "2110.08207v3", "abs_url": "https://arxiv.org/abs/2110.08207v3", "pdf_url": "https://arxiv.org/pdf/2110.08207v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2110.08207v3_Multitask_Prompted_Training_Enables_Zero-Shot_Task_Generalization.pdf", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "year": 2021, "paper_venue": null, "authors": ["Victor Sanh", "Albert Webson", "Colin Raffel", "Stephen H. Bach", "Lintang Sutawika", "Zaid Alyafeai", "Antoine Chaffin", "Arnaud Stiegler", "Teven Le Scao", "Arun Raja", "Manan Dey", "M Saiful Bari", "Canwen Xu", "Urmish Thakker", "Shanya Sharma Sharma", "Eliza Szczechla", "Taewoon Kim", "Gunjan Chhablani", "Nihal Nayak", "Debajyoti Datta", "Jonathan Chang", "Mike Tian-Jian Jiang", "Han Wang", "Matteo Manica", "Sheng Shen", "Zheng Xin Yong", "Harshit Pandey", "Rachel Bawden", "Thomas Wang", "Trishala Neeraj", "Jos Rozen", "Abheesht Sharma", "Andrea Santilli", "Thibault Fevry", "Jason Alan Fries", "Ryan Teehan", "Tali Bers", "Stella Biderman", "Leo Gao", "Thomas Wolf", "Alexander M. Rush"], "abstract": ".", "comments": "ICLR 2022 Spotlight (with extended discussion)", "official_code_urls": ["https://github.com/bigscience-workshop/promptsource", "https://github.com/bigscience-workshop/t-zero"], "pwc_page_url": "https://paperswithcode.com/paper/multitask-prompted-training-enables-zero-shot-1", "bibtex": "@misc{sanh2022multitask,\n      title={Multitask Prompted Training Enables Zero-Shot Task Generalization}, \n      author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Tali Bers and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M. Rush},\n      year={2022},\n      eprint={2110.08207},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "15-10-2021", "categories": ["cs.LG", "cs.CL"]}, "2108.04106v3": {"paper_id": "2108.04106v3", "abs_url": "https://arxiv.org/abs/2108.04106v3", "pdf_url": "https://arxiv.org/pdf/2108.04106v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2108.04106v3_Noisy_Channel_Language_Model_Prompting_for_Few-Shot_Text_Classification.pdf", "title": "Noisy Channel Language Model Prompting for Few-Shot Text Classification", "year": 2021, "paper_venue": null, "authors": ["Sewon Min", "Mike Lewis", "Hannaneh Hajishirzi", "Luke Zettlemoyer"], "abstract": "We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive methods (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.", "comments": "15 pages, 6 figures. Published as a conference paper at ACL 2022 (long). Code available at this https URL", "official_code_urls": ["https://github.com/shmsw25/Channel-LM-Prompting"], "pwc_page_url": "https://paperswithcode.com/paper/noisy-channel-language-model-prompting-for", "bibtex": "@misc{min2022noisy,\n      title={Noisy Channel Language Model Prompting for Few-Shot Text Classification}, \n      author={Sewon Min and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},\n      year={2022},\n      eprint={2108.04106},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "09-08-2021", "categories": ["cs.CL", "cs.AI"]}, "2109.03685v1": {"paper_id": "2109.03685v1", "abs_url": "https://arxiv.org/abs/2109.03685v1", "pdf_url": "https://arxiv.org/pdf/2109.03685v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.03685v1_Open_Aspect_Target_Sentiment_Classification_with_Natural_Language_Prompts.pdf", "title": "Open Aspect Target Sentiment Classification with Natural Language Prompts", "year": 2021, "paper_venue": null, "authors": ["Ronald Seoh", "Ian Birle", "Mrinal Tak", "Haw-Shiuan Chang", "Brian Pinette", "Alfred Hough"], "abstract": "For many business applications, we often seek to analyze sentiments associated with any arbitrary aspects of commercial products, despite having a very limited amount of labels or even without any labels at all. However, existing aspect target sentiment classification (ATSC) models are not trainable if annotated datasets are not available. Even with labeled data, they fall short of reaching satisfactory performance. To address this, we propose simple approaches that better solve ATSC with natural language prompts, enabling the task under zero-shot cases and enhancing supervised settings, especially for few-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop domain, our method of reformulating ATSC as an NLI task outperforms supervised SOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points. Moreover, we demonstrate that our prompts could handle implicitly stated aspects as well: our models reach about 77% accuracy on detecting sentiments for aspect categories (e.g., food), which do not necessarily appear within the text, even though we trained the models only with explicitly mentioned aspect terms (e.g., fajitas) from just 16 reviews - while the accuracy of the no-prompt baseline is only around 65%.", "comments": "", "official_code_urls": ["https://github.com/ronaldseoh/atsc_prompts"], "pwc_page_url": "https://paperswithcode.com/paper/open-aspect-target-sentiment-classification", "bibtex": "@misc{seoh2021open,\n      title={Open Aspect Target Sentiment Classification with Natural Language Prompts}, \n      author={Ronald Seoh and Ian Birle and Mrinal Tak and Haw-Shiuan Chang and Brian Pinette and Alfred Hough},\n      year={2021},\n      eprint={2109.03685},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "08-09-2021", "categories": ["cs.CL", "cs.LG"]}, "2110.07602v3": {"paper_id": "2110.07602v3", "abs_url": "https://arxiv.org/abs/2110.07602v3", "pdf_url": "https://arxiv.org/pdf/2110.07602v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2110.07602v3_P-Tuning_v2_Prompt_Tuning_Can_Be_Comparable_to_Fine-tuning_Universally_Across_Scales_and_Tasks.pdf", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks", "year": 2021, "paper_venue": null, "authors": ["Xiao Liu", "Kaixuan Ji", "Yicheng Fu", "Weng Lam Tam", "Zhengxiao Du", "Zhilin Yang", "Jie Tang"], "abstract": ".", "comments": "Proceedings of the 60th Annual Meeting of the Association of Computational Linguistics, 2022", "official_code_urls": ["https://github.com/thudm/p-tuning-v2"], "pwc_page_url": "https://paperswithcode.com/paper/p-tuning-v2-prompt-tuning-can-be-comparable", "bibtex": "@misc{liu2022ptuning,\n      title={P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks}, \n      author={Xiao Liu and Kaixuan Ji and Yicheng Fu and Weng Lam Tam and Zhengxiao Du and Zhilin Yang and Jie Tang},\n      year={2022},\n      eprint={2110.07602},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "14-10-2021", "categories": ["cs.CL"]}, "2102.12206v4": {"paper_id": "2102.12206v4", "abs_url": "https://arxiv.org/abs/2102.12206v4", "pdf_url": "https://arxiv.org/pdf/2102.12206v4.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2102.12206v4_PADA_Example-based_Prompt_Learning_for_on-the-fly_Adaptation_to_Unseen_Domains.pdf", "title": "PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains", "year": 2021, "paper_venue": null, "authors": ["Eyal Ben-David", "Nadav Oved", "Roi Reichart"], "abstract": "Natural Language Processing algorithms have made incredible progress, but they still struggle when applied to out-of-distribution examples. We address a challenging and underexplored version of this domain adaptation problem, where an algorithm is trained on several source domains, and then applied to examples from unseen domains that are unknown at training time. Particularly, no examples, labeled or unlabeled, or any other knowledge about the target domain are available to the algorithm at training time. We present PADA: An example-based autoregressive Prompt learning algorithm for on-the-fly Any-Domain Adaptation, based on the T5 language model. Given a test example, PADA first generates a unique prompt for it and then, conditioned on this prompt, labels the example with respect to the NLP prediction task. PADA is trained to generate a prompt which is a token sequence of unrestricted length, consisting of Domain Related Features (DRFs) that characterize each of the source domains. Intuitively, the generated prompt is a unique signature that maps the test example to a semantic space spanned by the source domains. In experiments with 3 tasks (text classification and sequence tagging), for a total of 14 multi-source adaptation scenarios, PADA substantially outperforms strong baselines.", "comments": "Accepted for publication at TACL in January 2022. First two authors contributed equally to this work. Our code and data are available at: this https URL", "official_code_urls": ["https://github.com/eyalbd2/PADA"], "pwc_page_url": "https://paperswithcode.com/paper/pada-a-prompt-based-autoregressive-approach", "bibtex": "@misc{bendavid2022pada,\n      title={PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains}, \n      author={Eyal Ben-David and Nadav Oved and Roi Reichart},\n      year={2022},\n      eprint={2102.12206},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "24-02-2021", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2109.04332v3": {"paper_id": "2109.04332v3", "abs_url": "https://arxiv.org/abs/2109.04332v3", "pdf_url": "https://arxiv.org/pdf/2109.04332v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.04332v3_PPT_Pre-trained_Prompt_Tuning_for_Few-shot_Learning.pdf", "title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning", "year": 2021, "paper_venue": null, "authors": ["Yuxian Gu", "Xu Han", "Zhiyuan Liu", "Minlie Huang"], "abstract": "Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model fine-tuning when downstream data are sufficient, whereas it performs much worse under few-shot learning settings, which may hinder the application of prompt tuning in practice. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework \"PPT\". To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice.", "comments": "Accepted by ACL2022 (main conference)", "official_code_urls": ["https://github.com/thu-coai/ppt"], "pwc_page_url": "https://paperswithcode.com/paper/ppt-pre-trained-prompt-tuning-for-few-shot", "bibtex": "@misc{gu2022ppt,\n      title={PPT: Pre-trained Prompt Tuning for Few-shot Learning}, \n      author={Yuxian Gu and Xu Han and Zhiyuan Liu and Minlie Huang},\n      year={2022},\n      eprint={2109.04332},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "09-09-2021", "categories": ["cs.CL"]}, "2105.11259v3": {"paper_id": "2105.11259v3", "abs_url": "https://arxiv.org/abs/2105.11259v3", "pdf_url": "https://arxiv.org/pdf/2105.11259v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2105.11259v3_PTR_Prompt_Tuning_with_Rules_for_Text_Classification.pdf", "title": "PTR: Prompt Tuning with Rules for Text Classification", "year": 2021, "paper_venue": null, "authors": ["Xu Han", "Weilin Zhao", "Ning Ding", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Fine-tuned pre-trained language models (PLMs) have achieved awesome performance on almost all NLP tasks. By using additional prompts to fine-tune PLMs, we can further stimulate the rich knowledge distributed in PLMs to better serve downstream tasks. Prompt tuning has achieved promising results on some few-class classification tasks such as sentiment classification and natural language inference. However, manually designing lots of language prompts is cumbersome and fallible. For those auto-generated prompts, it is also expensive and time-consuming to verify their effectiveness in non-few-shot scenarios. Hence, it is still challenging for prompt tuning to address many-class classification tasks. To this end, we propose prompt tuning with rules (PTR) for many-class text classification and apply logic rules to construct prompts with several sub-prompts. In this way, PTR is able to encode prior knowledge of each class into prompt tuning. We conduct experiments on relation classification, a typical and complicated many-class classification task, and the results show that PTR can significantly and consistently outperform existing state-of-the-art baselines. This indicates that PTR is a promising approach to take advantage of both human prior knowledge and PLMs for those complicated classification tasks.", "comments": "", "official_code_urls": ["https://github.com/thunlp/PTR"], "pwc_page_url": "https://paperswithcode.com/paper/ptr-prompt-tuning-with-rules-for-text", "bibtex": "@misc{han2021ptr,\n      title={PTR: Prompt Tuning with Rules for Text Classification}, \n      author={Xu Han and Weilin Zhao and Ning Ding and Zhiyuan Liu and Maosong Sun},\n      year={2021},\n      eprint={2105.11259},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "24-05-2021", "categories": ["cs.CL"]}, "2109.05190v3": {"paper_id": "2109.05190v3", "abs_url": "https://arxiv.org/abs/2109.05190v3", "pdf_url": "https://arxiv.org/pdf/2109.05190v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.05190v3_PoKE_A_Prompt-based_Knowledge_Eliciting_Approach_for_Event_Argument_Extraction.pdf", "title": "PoKE: A Prompt-based Knowledge Eliciting Approach for Event Argument Extraction", "year": 2021, "paper_venue": null, "authors": ["Jiaju Lin", "Qin Chen"], "abstract": "Eliciting knowledge from pre-trained language models via prompt-based learning has shown great potential in many natural language processing tasks. Whereas, the applications for more complex tasks such as event extraction are less studied since the design of prompt is not straightforward for the structured event containing various triggers and arguments. % Meanwhile, current conditional generation methods employ large encoder-decoder models, which are costly to train and serve. In this paper, we present a novel prompt-based approach, which elicits both the independent and joint knowledge about different events for event argument extraction. The experimental results on the benchmark ACE2005 dataset show the great advantages of our proposed approach. In particular, our approach is superior to the recent advanced methods in both fully-supervised and low-resource scenarios.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/eliciting-knowledge-from-language-models-for", "bibtex": "@misc{lin2022poke,\n      title={PoKE: A Prompt-based Knowledge Eliciting Approach for Event Argument Extraction}, \n      author={Jiaju Lin and Qin Chen},\n      year={2022},\n      eprint={2109.05190},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "11-09-2021", "categories": ["cs.CL", "cs.AI"]}, "2107.13586v1": {"paper_id": "2107.13586v1", "abs_url": "https://arxiv.org/abs/2107.13586v1", "pdf_url": "https://arxiv.org/pdf/2107.13586v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2107.13586v1_Pre-train_Prompt_and_Predict_A_Systematic_Survey_of_Prompting_Methods_in_Natural_Language_Processing.pdf", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing", "year": 2021, "paper_venue": null, "authors": ["Pengfei Liu", "Weizhe Yuan", "Jinlan Fu", "Zhengbao Jiang", "Hiroaki Hayashi", "Graham Neubig"], "abstract": "including constantly-updated survey, and paperlist.", "comments": "Website: this http URL", "official_code_urls": ["https://github.com/pfliu-nlp/NLPedia-Pretrain"], "pwc_page_url": "https://paperswithcode.com/paper/pre-train-prompt-and-predict-a-systematic", "bibtex": "@misc{liu2021pretrain,\n      title={Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing}, \n      author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},\n      year={2021},\n      eprint={2107.13586},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "28-07-2021", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2101.00190v1": {"paper_id": "2101.00190v1", "abs_url": "https://arxiv.org/abs/2101.00190v1", "pdf_url": "https://arxiv.org/pdf/2101.00190v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2101.00190v1_Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.pdf", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021, "paper_venue": null, "authors": ["Xiang Lisa Li", "Percy Liang"], "abstract": "Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.", "comments": "", "official_code_urls": ["https://github.com/XiangLi1999/PrefixTuning"], "pwc_page_url": "https://paperswithcode.com/paper/prefix-tuning-optimizing-continuous-prompts", "bibtex": "@misc{li2021prefixtuning,\n      title={Prefix-Tuning: Optimizing Continuous Prompts for Generation}, \n      author={Xiang Lisa Li and Percy Liang},\n      year={2021},\n      eprint={2101.00190},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "01-01-2021", "categories": ["cs.CL"]}, "2102.07350v1": {"paper_id": "2102.07350v1", "abs_url": "https://arxiv.org/abs/2102.07350v1", "pdf_url": "https://arxiv.org/pdf/2102.07350v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2102.07350v1_Prompt_Programming_for_Large_Language_Models_Beyond_the_Few-Shot_Paradigm.pdf", "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm", "year": 2021, "paper_venue": null, "authors": ["Laria Reynolds", "Kyle McDonell"], "abstract": "Prevailing methods for mapping large generative language models to supervised tasks may fail to sufficiently probe models' novel capabilities. Using GPT-3 as a case study, we show that 0-shot prompts can significantly outperform few-shot prompts. We suggest that the function of few-shot examples in these cases is better described as locating an already learned task rather than meta-learning. This analysis motivates rethinking the role of prompts in controlling and evaluating powerful language models. In this work, we discuss methods of prompt programming, emphasizing the usefulness of considering prompts through the lens of natural language. We explore techniques for exploiting the capacity of narratives and cultural anchors to encode nuanced intentions and techniques for encouraging deconstruction of a problem into components before producing a verdict. Informed by this more encompassing theory of prompt programming, we also introduce the idea of a metaprompt that seeds the model to generate its own natural language prompts for a range of tasks. Finally, we discuss how these more general methods of interacting with language models can be incorporated into existing and future benchmarks and practical applications.", "comments": "", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/prompt-programming-for-large-language-models", "bibtex": "@misc{reynolds2021prompt,\n      title={Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm}, \n      author={Laria Reynolds and Kyle McDonell},\n      year={2021},\n      eprint={2102.07350},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "15-02-2021", "categories": ["cs.CL", "cs.AI"]}, "2109.07830v3": {"paper_id": "2109.07830v3", "abs_url": "https://arxiv.org/abs/2109.07830v3", "pdf_url": "https://arxiv.org/pdf/2109.07830v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.07830v3_Reframing_Instructional_Prompts_to_GPTks_Language.pdf", "title": "Reframing Instructional Prompts to GPTk's Language", "year": 2021, "paper_venue": null, "authors": ["Swaroop Mishra", "Daniel Khashabi", "Chitta Baral", "Yejin Choi", "Hannaneh Hajishirzi"], "abstract": "What kinds of instructional prompts are easier to follow for Language Models (LMs)? We study this question by conducting extensive empirical analysis that shed light on important features of successful instructional prompts. Specifically, we study several classes of reframing techniques for manual reformulation of prompts into more effective ones. Some examples include decomposing a complex task instruction into multiple simpler tasks or itemizing instructions into sequential steps. Our experiments compare the zero-shot and few-shot performance of LMs prompted with reframed instructions on 12 NLP tasks across 6 categories. Compared with original instructions, our reframed instructions lead to significant improvements across LMs with different sizes. For example, the same reframed prompts boost few-shot performance of GPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all tasks. Furthermore, reframed instructions reduce the number of examples required to prompt LMs in the few-shot setting. We hope these empirically-driven techniques will pave the way towards more effective future prompting algorithms.", "comments": "ACL 2022 Findings", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/reframing-instructional-prompts-to-gptk-s", "bibtex": "@misc{mishra2022reframing,\n      title={Reframing Instructional Prompts to GPTk's Language}, \n      author={Swaroop Mishra and Daniel Khashabi and Chitta Baral and Yejin Choi and Hannaneh Hajishirzi},\n      year={2022},\n      eprint={2109.07830},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "16-09-2021", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2109.08306v1": {"paper_id": "2109.08306v1", "abs_url": "https://arxiv.org/abs/2109.08306v1", "pdf_url": "https://arxiv.org/pdf/2109.08306v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2109.08306v1_SentiPrompt_Sentiment_Knowledge_Enhanced_Prompt-Tuning_for_Aspect-Based_Sentiment_Analysis.pdf", "title": "SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis", "year": 2021, "paper_venue": null, "authors": ["Chengxi Li", "Feiyu Gao", "Jiajun Bu", "Lu Xu", "Xiang Chen", "Yu Gu", "Zirui Shao", "Qi Zheng", "Ningyu Zhang", "Yongpan Wang", "Zhi Yu"], "abstract": "Aspect-based sentiment analysis (ABSA) is an emerging fine-grained sentiment analysis task that aims to extract aspects, classify corresponding sentiment polarities and find opinions as the causes of sentiment. The latest research tends to solve the ABSA task in a unified way with end-to-end frameworks. Yet, these frameworks get fine-tuned from downstream tasks without any task-adaptive modification. Specifically, they do not use task-related knowledge well or explicitly model relations between aspect and opinion terms, hindering them from better performance. In this paper, we propose SentiPrompt to use sentiment knowledge enhanced prompts to tune the language model in the unified framework. We inject sentiment knowledge regarding aspects, opinions, and polarities into prompt and explicitly model term relations via constructing consistency and polarity judgment templates from the ground truth triplets. Experimental results demonstrate that our approach can outperform strong baselines on Triplet Extraction, Pair Extraction, and Aspect Term Extraction with Sentiment Classification by a notable margin.", "comments": "7pages, under blind review", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/sentiprompt-sentiment-knowledge-enhanced", "bibtex": "@misc{li2021sentiprompt,\n      title={SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis}, \n      author={Chengxi Li and Feiyu Gao and Jiajun Bu and Lu Xu and Xiang Chen and Yu Gu and Zirui Shao and Qi Zheng and Ningyu Zhang and Yongpan Wang and Zhi Yu},\n      year={2021},\n      eprint={2109.08306},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "17-09-2021", "categories": ["cs.CL", "cs.AI"]}, "2104.08315v9": {"paper_id": "2104.08315v9", "abs_url": "https://arxiv.org/abs/2104.08315v9", "pdf_url": "https://arxiv.org/pdf/2104.08315v9.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2104.08315v9_Surface_Form_Competition_Why_the_Highest_Probability_Answer_Isnt_Always_Right.pdf", "title": "Surface Form Competition: Why the Highest Probability Answer Isn't Always Right", "year": 2021, "paper_venue": null, "authors": ["Ari Holtzman", "Peter West", "Vered Shwartz", "Yejin Choi", "Luke Zettlemoyer"], "abstract": "We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to a term that is proportional to its a priori likelihood within the context of the specific zero-shot task. It achieves consistent gains in zero-shot performance over both calibrated (Zhao et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models over a variety of multiple choice datasets.", "comments": "", "official_code_urls": ["https://github.com/peterwestuw/surface-form-competition"], "pwc_page_url": "https://paperswithcode.com/paper/surface-form-competition-why-the-highest", "bibtex": "@misc{holtzman2022surface,\n      title={Surface Form Competition: Why the Highest Probability Answer Isn't Always Right}, \n      author={Ari Holtzman and Peter West and Vered Shwartz and Yejin Choi and Luke Zettlemoyer},\n      year={2022},\n      eprint={2104.08315},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "16-04-2021", "categories": ["cs.CL"]}, "2104.08691v2": {"paper_id": "2104.08691v2", "abs_url": "https://arxiv.org/abs/2104.08691v2", "pdf_url": "https://arxiv.org/pdf/2104.08691v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2104.08691v2_The_Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "year": 2021, "paper_venue": null, "authors": ["Brian Lester", "Rami Al-Rfou", "Noah Constant"], "abstract": "In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.", "comments": "Accepted to EMNLP 2021", "official_code_urls": ["https://github.com/google-research/prompt-tuning"], "pwc_page_url": "https://paperswithcode.com/paper/the-power-of-scale-for-parameter-efficient", "bibtex": "@misc{lester2021power,\n      title={The Power of Scale for Parameter-Efficient Prompt Tuning}, \n      author={Brian Lester and Rami Al-Rfou and Noah Constant},\n      year={2021},\n      eprint={2104.08691},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "18-04-2021", "categories": ["cs.CL"]}, "2105.11447v1": {"paper_id": "2105.11447v1", "abs_url": "https://arxiv.org/abs/2105.11447v1", "pdf_url": "https://arxiv.org/pdf/2105.11447v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2105.11447v1_True_Few-Shot_Learning_with_Language_Models.pdf", "title": "True Few-Shot Learning with Language Models", "year": 2021, "paper_venue": null, "authors": ["Ethan Perez", "Douwe Kiela", "Kyunghyun Cho"], "abstract": "Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates (\"prompts\"). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.", "comments": "Code at this https URL", "official_code_urls": ["https://github.com/ethanjperez/true_few_shot"], "pwc_page_url": "https://paperswithcode.com/paper/true-few-shot-learning-with-language-models", "bibtex": "@misc{perez2021true,\n      title={True Few-Shot Learning with Language Models}, \n      author={Ethan Perez and Douwe Kiela and Kyunghyun Cho},\n      year={2021},\n      eprint={2105.11447},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "24-05-2021", "categories": ["cs.CL", "cs.LG", "stat.ML"]}, "2108.13487v1": {"paper_id": "2108.13487v1", "abs_url": "https://arxiv.org/abs/2108.13487v1", "pdf_url": "https://arxiv.org/pdf/2108.13487v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2108.13487v1_Want_To_Reduce_Labeling_Cost?_GPT-3_Can_Help.pdf", "title": "Want To Reduce Labeling Cost? GPT-3 Can Help", "year": 2021, "paper_venue": null, "authors": ["Shuohang Wang", "Yang Liu", "Yichong Xu", "Chenguang Zhu", "Michael Zeng"], "abstract": "Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-specific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 175 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We find that, to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance with limited labeling budget. These results present a cost-effective data labeling methodology that is generalizable to many practical applications.", "comments": "Findings of EMNLP 2021, 11 pages", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/want-to-reduce-labeling-cost-gpt-3-can-help", "bibtex": "@misc{wang2021want,\n      title={Want To Reduce Labeling Cost? GPT-3 Can Help}, \n      author={Shuohang Wang and Yang Liu and Yichong Xu and Chenguang Zhu and Michael Zeng},\n      year={2021},\n      eprint={2108.13487},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "30-08-2021", "categories": ["cs.CL", "cs.AI"]}, "2112.09332v3": {"paper_id": "2112.09332v3", "abs_url": "https://arxiv.org/abs/2112.09332v3", "pdf_url": "https://arxiv.org/pdf/2112.09332v3.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2112.09332v3_WebGPT_Browser-assisted_question-answering_with_human_feedback.pdf", "title": "WebGPT: Browser-assisted question-answering with human feedback", "year": 2021, "paper_venue": null, "authors": ["Reiichiro Nakano", "Jacob Hilton", "Suchir Balaji", "Jeff Wu", "Long Ouyang", "Christina Kim", "Christopher Hesse", "Shantanu Jain", "Vineet Kosaraju", "William Saunders", "Xu Jiang", "Karl Cobbe", "Tyna Eloundou", "Gretchen Krueger", "Kevin Button", "Matthew Knight", "Benjamin Chess", "John Schulman"], "abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.", "comments": "32 pages", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/webgpt-browser-assisted-question-answering", "bibtex": "@misc{nakano2022webgpt,\n      title={WebGPT: Browser-assisted question-answering with human feedback}, \n      author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},\n      year={2022},\n      eprint={2112.09332},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "17-12-2021", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2106.09226v2": {"paper_id": "2106.09226v2", "abs_url": "https://arxiv.org/abs/2106.09226v2", "pdf_url": "https://arxiv.org/pdf/2106.09226v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2106.09226v2_Why_Do_Pretrained_Language_Models_Help_in_Downstream_Tasks?_An_Analysis_of_Head_and_Prompt_Tuning.pdf", "title": "Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning", "year": 2021, "paper_venue": null, "authors": ["Colin Wei", "Sang Michael Xie", "Tengyu Ma"], "abstract": "Pretrained language models have achieved state-of-the-art performance when adapted to a downstream NLP task. However, theoretical analysis of these models is scarce and challenging since the pretraining and downstream tasks can be very different. We propose an analysis framework that links the pretraining and downstream tasks with an underlying latent variable generative model of text -- the downstream classifier must recover a function of the posterior distribution over the latent variables. We analyze head tuning (learning a classifier on top of the frozen pretrained model) and prompt tuning in this setting. The generative model in our analysis is either a Hidden Markov Model (HMM) or an HMM augmented with a latent memory component, motivated by long-term dependencies in natural language. We show that 1) under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task, 2) prompt tuning obtains downstream guarantees with weaker non-degeneracy conditions, and 3) our recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM because task-relevant information is easier to recover from the long-term memory. Experiments on synthetically generated data from HMMs back our theoretical findings.", "comments": "", "official_code_urls": ["https://github.com/sangmichaelxie/pretraining_analysis"], "pwc_page_url": "https://paperswithcode.com/paper/why-do-pretrained-language-models-help-in", "bibtex": "@misc{wei2022pretrained,\n      title={Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning}, \n      author={Colin Wei and Sang Michael Xie and Tengyu Ma},\n      year={2022},\n      eprint={2106.09226},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}", "published_date": "17-06-2021", "categories": ["cs.LG", "stat.ML"]}}, {"2010.15980v2": {"paper_id": "2010.15980v2", "abs_url": "https://arxiv.org/abs/2010.15980v2", "pdf_url": "https://arxiv.org/pdf/2010.15980v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2010.15980v2_AutoPrompt_Eliciting_Knowledge_from_Language_Models_with_Automatically_Generated_Prompts.pdf", "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "year": 2020, "paper_venue": null, "authors": ["Taylor Shin", "Yasaman Razeghi", "Robert L. Logan IV", "Eric Wallace", "Sameer Singh"], "abstract": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.", "comments": "v2: Fixed error in Figure 2", "official_code_urls": ["https://github.com/ucinlp/autoprompt"], "pwc_page_url": "https://paperswithcode.com/paper/autoprompt-eliciting-knowledge-from-language", "bibtex": "@misc{shin2020autoprompt,\n      title={AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts}, \n      author={Taylor Shin and Yasaman Razeghi and Robert L. Logan IV au2 and Eric Wallace and Sameer Singh},\n      year={2020},\n      eprint={2010.15980},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "29-10-2020", "categories": ["cs.CL", "cs.LG"]}, "2010.13641v1": {"paper_id": "2010.13641v1", "abs_url": "https://arxiv.org/abs/2010.13641v1", "pdf_url": "https://arxiv.org/pdf/2010.13641v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2010.13641v1_Automatically_Identifying_Words_That_Can_Serve_as_Labels_for_Few-Shot_Text_Classification.pdf", "title": "Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification", "year": 2020, "paper_venue": null, "authors": ["Timo Schick", "Helmut Schmid", "Hinrich Sch\u00fctze"], "abstract": "A recent approach for few-shot text classification is to convert textual inputs to cloze questions that contain some form of task description, process them with a pretrained language model and map the predicted words to labels. Manually defining this mapping between words and labels requires both domain expertise and an understanding of the language model's abilities. To mitigate this issue, we devise an approach that automatically finds such a mapping given small amounts of training data. For a number of tasks, the mapping found by our approach performs almost as well as hand-crafted label-to-word mappings.", "comments": "To appear at COLING 2020", "official_code_urls": ["https://github.com/timoschick/pet"], "pwc_page_url": "https://paperswithcode.com/paper/automatically-identifying-words-that-can", "bibtex": "@misc{schick2020automatically,\n      title={Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification}, \n      author={Timo Schick and Helmut Schmid and Hinrich Sch\u00fctze},\n      year={2020},\n      eprint={2010.13641},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "26-10-2020", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2004.12832v2": {"paper_id": "2004.12832v2", "abs_url": "https://arxiv.org/abs/2004.12832v2", "pdf_url": "https://arxiv.org/pdf/2004.12832v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2004.12832v2_ColBERT_Efficient_and_Effective_Passage_Search_via_Contextualized_Late_Interaction_over_BERT.pdf", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "year": 2020, "paper_venue": null, "authors": ["Omar Khattab", "Matei Zaharia"], "abstract": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.", "comments": "Accepted at SIGIR 2020", "official_code_urls": ["https://github.com/stanford-futuredata/ColBERT"], "pwc_page_url": "https://paperswithcode.com/paper/colbert-efficient-and-effective-passage", "bibtex": "@misc{khattab2020colbert,\n      title={ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT}, \n      author={Omar Khattab and Matei Zaharia},\n      year={2020},\n      eprint={2004.12832},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}", "published_date": "27-04-2020", "categories": ["cs.IR", "cs.CL"]}, "2010.12844v2": {"paper_id": "2010.12844v2", "abs_url": "https://arxiv.org/abs/2010.12844v2", "pdf_url": "https://arxiv.org/pdf/2010.12844v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2010.12844v2_FLIN_A_Flexible_Natural_Language_Interface_for_Web_Navigation.pdf", "title": "FLIN: A Flexible Natural Language Interface for Web Navigation", "year": 2020, "paper_venue": null, "authors": ["Sahisnu Mazumder", "Oriana Riva"], "abstract": "AI assistants can now carry out tasks for users by directly interacting with website UIs. Current semantic parsing and slot-filling techniques cannot flexibly adapt to many different websites without being constantly re-trained. We propose FLIN, a natural language interface for web navigation that maps user commands to concept-level actions (rather than low-level UI actions), thus being able to flexibly adapt to different websites and handle their transient nature. We frame this as a ranking problem: given a user command and a webpage, FLIN learns to score the most relevant navigation instruction (involving action and parameter values). To train and evaluate FLIN, we collect a dataset using nine popular websites from three domains. Our results show that FLIN was able to adapt to new websites in a given domain.", "comments": "Accepted to NAACL-HLT 2021", "official_code_urls": [], "pwc_page_url": "https://paperswithcode.com/paper/flin-a-flexible-natural-language-interface", "bibtex": "@misc{mazumder2021flin,\n      title={FLIN: A Flexible Natural Language Interface for Web Navigation}, \n      author={Sahisnu Mazumder and Oriana Riva},\n      year={2021},\n      eprint={2010.12844},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "24-10-2020", "categories": ["cs.CL", "cs.AI"]}, "2009.07118v2": {"paper_id": "2009.07118v2", "abs_url": "https://arxiv.org/abs/2009.07118v2", "pdf_url": "https://arxiv.org/pdf/2009.07118v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2009.07118v2_Its_Not_Just_Size_That_Matters_Small_Language_Models_Are_Also_Few-Shot_Learners.pdf", "title": "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners", "year": 2020, "paper_venue": null, "authors": ["Timo Schick", "Hinrich Sch\u00fctze"], "abstract": "When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much \"greener\" in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.", "comments": "Accepted at NAACL2021", "official_code_urls": ["https://github.com/timoschick/pet", "https://github.com/timoschick/fewglue"], "pwc_page_url": "https://paperswithcode.com/paper/it-s-not-just-size-that-matters-small", "bibtex": "@misc{schick2021its,\n      title={It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners}, \n      author={Timo Schick and Hinrich Sch\u00fctze},\n      year={2021},\n      eprint={2009.07118},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "15-09-2020", "categories": ["cs.CL", "cs.AI", "cs.LG"]}, "2004.05150v2": {"paper_id": "2004.05150v2", "abs_url": "https://arxiv.org/abs/2004.05150v2", "pdf_url": "https://arxiv.org/pdf/2004.05150v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2004.05150v2_Longformer_The_Long-Document_Transformer.pdf", "title": "Longformer: The Long-Document Transformer", "year": 2020, "paper_venue": null, "authors": ["Iz Beltagy", "Matthew E. Peters", "Arman Cohan"], "abstract": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.", "comments": "Version 2 introduces the Longformer-Encoder-Decoder (LED) model", "official_code_urls": ["https://github.com/allenai/longformer"], "pwc_page_url": "https://paperswithcode.com/paper/longformer-the-long-document-transformer", "bibtex": "@misc{beltagy2020longformer,\n      title={Longformer: The Long-Document Transformer}, \n      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},\n      year={2020},\n      eprint={2004.05150},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "10-04-2020", "categories": ["cs.CL"]}, "2012.15723v2": {"paper_id": "2012.15723v2", "abs_url": "https://arxiv.org/abs/2012.15723v2", "pdf_url": "https://arxiv.org/pdf/2012.15723v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2012.15723v2_Making_Pre-trained_Language_Models_Better_Few-shot_Learners.pdf", "title": "Making Pre-trained Language Models Better Few-shot Learners", "year": 2020, "paper_venue": null, "authors": ["Tianyu Gao", "Adam Fisch", "Danqi Chen"], "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.", "comments": "Accepted to ACL 2021. The code is publicly available at this https URL", "official_code_urls": ["https://github.com/princeton-nlp/LM-BFF"], "pwc_page_url": "https://paperswithcode.com/paper/making-pre-trained-language-models-better-few", "bibtex": "@misc{gao2021making,\n      title={Making Pre-trained Language Models Better Few-shot Learners}, \n      author={Tianyu Gao and Adam Fisch and Danqi Chen},\n      year={2021},\n      eprint={2012.15723},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "31-12-2020", "categories": ["cs.CL", "cs.LG"]}, "2101.00027v1": {"paper_id": "2101.00027v1", "abs_url": "https://arxiv.org/abs/2101.00027v1", "pdf_url": "https://arxiv.org/pdf/2101.00027v1.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "2101.00027v1_The_Pile_An_800GB_Dataset_of_Diverse_Text_for_Language_Modeling.pdf", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "year": 2020, "paper_venue": null, "authors": ["Leo Gao", "Stella Biderman", "Sid Black", "Laurence Golding", "Travis Hoppe", "Charles Foster", "Jason Phang", "Horace He", "Anish Thite", "Noa Nabeshima", "Shawn Presser", "Connor Leahy"], "abstract": "Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \\textit{the Pile}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.", "comments": "", "official_code_urls": ["https://github.com/EleutherAI/The-Pile"], "pwc_page_url": "https://paperswithcode.com/paper/the-pile-an-800gb-dataset-of-diverse-text-for", "bibtex": "@misc{gao2020pile,\n      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, \n      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},\n      year={2020},\n      eprint={2101.00027},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "31-12-2020", "categories": ["cs.CL"]}}, {"1810.04805v2": {"paper_id": "1810.04805v2", "abs_url": "https://arxiv.org/abs/1810.04805v2", "pdf_url": "https://arxiv.org/pdf/1810.04805v2.pdf", "supp_url": null, "src_website": "ArXiv", "download_name": "1810.04805v2_BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding.pdf", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018, "paper_venue": null, "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "abstract": "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "comments": "", "official_code_urls": ["https://github.com/google-research/bert"], "pwc_page_url": "https://paperswithcode.com/paper/bert-pre-training-of-deep-bidirectional", "bibtex": "@misc{devlin2019bert,\n      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, \n      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},\n      year={2019},\n      eprint={1810.04805},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}", "published_date": "11-10-2018", "categories": ["cs.CL"]}}]