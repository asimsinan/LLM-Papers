1. [Langchain:](https://www.langchain.com) LangChain is a framework for developing applications powered by language models. It enables applications that:
   * **Are context-aware:** connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)
   * **Reason:** rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)

2. [llamaindex:](https://www.llamaindex.ai) LlamaIndex is a data framework for LLM-based applications which benefit from context augmentation. Such LLM systems have been termed as RAG systems, standing for “Retrieval-Augmented Generation”. LlamaIndex provides the essential abstractions to more easily ingest, structure, and access private or domain-specific data in order to inject these safely and reliably into LLMs for more accurate text generation. It’s available in Python (these docs) and Typescript.

3. [Haystack:](https://haystack.deepset.ai) Haystack is the open source Python framework by deepset for building custom apps with large language models (LLMs). It lets you quickly try out the latest models in natural language processing (NLP) while being flexible and easy to use. Our inspiring community of users and builders has helped shape Haystack into what it is today: a complete framework for building production-ready NLP apps.
4. [FlowiseAI:](https://flowiseai.com) FlowiseAI is a drag-and-drop UI for building LLM flows and developing LangChain apps. It's an excellent choice for developers who want to construct large language models. At the same time, it's aimed at organizations that want to develop LLM apps but lack the means to employ a developer. You can use Flowise AI to build apps such as chatbots, virtual assistants, and data analysis tools.
5. [AgentGPT:](https://agentgpt.reworkd.ai) AgentGPT is designed for organizations that wish to deploy autonomous AI agents in their browsers. While Auto-GPT operates independently and generates its own prompts, Agent GPT depends on user inputs and works by interacting with humans to achieve tasks. Though still in the beta stage, AgentGPT currently provides long-term memory and web browsing capabilities.
6. [BabyAGI:](https://github.com/yoheinakajima/babyagi) BabyAGI is a Python script that acts as an AI-powered task manager. It uses OpenAI, LangChain, and vector databases, such as Chroma and Pinecone, to create, prioritize, and execute tasks. It does this by selecting a task from a list and sending the task to an agent, which uses OpenAI to complete the task based on context. The vector database then enriches and stores the result. BabyAGI then goes on to create new tasks and reprioritizes the list according to the result and objective of the previous task.
7. [Langdock:](https://www.langdock.com) LangDock was built for developers searching for an all-in-one product suite for creating, testing, deploying, and monitoring their LLM plugins. It lets you add your API documentation manually or import an existing OpenAPI specification.
8. [GradientJ:](https://www.gradientj.com) GradientJ is a tool for developers looking to build and manage large language model applications. It lets you orchestrate and manage complex applications by chaining prompts and knowledge bases into complex APIs and enhances the accuracy of your models by integrating them with your proprietary data.
9. [Semantic Kernel:](https://github.com/microsoft/semantic-kernel) Semantic Kernel is an SDK that integrates Large Language Models (LLMs) like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C#, Python, and Java. Semantic Kernel achieves this by allowing you to define plugins that can be chained together in just a few lines of code.
10. [MetaGPT:](https://github.com/geekan/MetaGPT) Created by a team of researchers from Chinese and US universities, MetaGPT is a new LLM-based meta programming framework aiming to enable collaboration in multi-agent systems by leveraging human procedural knowledge to enhance robustness, reduce errors, and engineer software solutions for complex tasks
11. [AutoChain:](https://github.com/Forethought-Technologies/AutoChain) AutoChain takes inspiration from LangChain and AutoGPT and aims to solve both problems by providing a lightweight and extensible framework for developers to build their own agents using LLMs with custom tools and automatically evaluating different user scenarios with simulated conversations. Experienced user of LangChain would find AutoChain is easy to navigate since they share similar but simpler concepts.
12. [Fastchat:](https://github.com/lm-sys/FastChat) FastChat is an open platform for training, serving, and evaluating large language model based chatbots.
13. [Skypilot:](https://github.com/skypilot-org/skypilot) SkyPilot is a framework for running LLMs, AI, and batch jobs on any cloud, offering maximum cost savings, highest GPU availability, and managed execution.
14. [Swiss Army Llama:](https://github.com/Dicklesworthstone/swiss_army_llama) The Swiss Army Llama is designed to facilitate and optimize the process of working with local LLMs by using FastAPI to expose convenient REST endpoints for various tasks, including obtaining text embeddings and completions using different LLMs via llama_cpp, as well as automating the process of obtaining all the embeddings for most common document types, including PDFs (even ones that require OCR), Word file, etc; it even allows you to submit an audio file and automatically transcribes it with the Whisper model, cleans up the resulting text, and then computes the embeddings for it. To avoid wasting computation, these embeddings are cached in SQlite and retrieved if they have already been computed before. To speed up the process of loading multiple LLMs, optional RAM Disks can be used, and the process for creating and managing them is handled automatically for you. With a quick and easy setup process, you will immediately get access to a veritable "Swiss Army Knife" of LLM related tools, all accessible via a convenient Swagger UI and ready to be integrated into your own applications with minimal fuss or configuration required.
15. [Langstream:](https://github.com/rogeriochaves/langstream) angStream is a lighter alternative to LangChain for building LLMs application, instead of having a massive amount of features and classes, LangStream focuses on having a single small core, that is easy to learn, easy to adapt, well documented, fully typed and truly composable, with Streams instead of chains as the building block.
16. [Magentic:](https://github.com/jackmpcollins/magentic) asily integrate Large Language Models into your Python code. Simply use the @prompt decorator to create functions that return structured output from the LLM. Mix LLM queries and function calling with regular Python code to create complex logic.
17. [Promptfoo:](https://github.com/promptfoo/promptfoo Promptfoo is a tool for testing and evaluating LLM output quality.With promptfoo, you can:
    * Systematically test prompts, models, and RAGs with predefined test cases
    * Evaluate quality and catch regressions by comparing LLM outputs side-by-side
    * Speed up evaluations with caching and concurrency
    * Score outputs automatically by defining test cases
    * Use as a CLI, library, or in CI/CD
    * Use OpenAI, Anthropic, Azure, Google, HuggingFace, open-source models like Llama, or integrate custom API providers for any LLM API
18. [Agenta:](https://github.com/agenta-ai/agenta) he open-source LLM developer platform for prompt-engineering, evaluation, human feedback, and deployment of complex LLM apps.
19. [Langdroid:](https://github.com/langroid/langroid) angroid is an intuitive, lightweight, extensible and principled Python framework to easily build LLM-powered applications, from ex-CMU and UW-Madison researchers. You set up Agents, equip them with optional components (LLM, vector-store and tools/functions), assign them tasks, and have them collaboratively solve a problem by exchanging messages. This Multi-Agent paradigm is inspired by the Actor Framework.
20. [Embedchain:](https://github.com/embedchain/embedchain) mbedchain is an Open Source Framework for personalizing LLM responses. It makes it easy to create and deploy personalized AI apps. At its core, Embedchain follows the design principle of being "Conventional but Configurable" to serve both software engineers and machine learning engineers. Embedchain streamlines the creation of personalized LLM applications, offering a seamless process for managing various types of unstructured data. It efficiently segments data into manageable chunks, generates relevant embeddings, and stores them in a vector database for optimized retrieval. With a suite of diverse APIs, it enables users to extract contextual information, find precise answers, or engage in interactive chat conversations, all tailored to their own data.
21. [CometLLM:](https://github.com/comet-ml/comet-llm) CometLLM is a tool to log and visualize your LLM prompts and chains. Use CometLLM to identify effective prompt strategies, streamline your troubleshooting, and ensure reproducible workflows!
22. [OpenLLM:](https://github.com/bentoml/OpenLLM) OpenLLM is an open-source platform designed to facilitate the deployment and operation of large language models (LLMs) in real-world applications. With OpenLLM, you can run inference on any open-source LLM, deploy them on the cloud or on-premises, and build powerful AI applications.
23. [TensorRT-LLM:](https://github.com/NVIDIA/TensorRT-LLM) TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. It also includes a backend for integration with the NVIDIA Triton Inference Server; a production-quality system to serve LLMs. Models built with TensorRT-LLM can be executed on a wide range of configurations going from a single GPU to multiple nodes with multiple GPUs (using Tensor Parallelism and/or Pipeline Parallelism).
24. [Chainlit:](https://docs.chainlit.io/get-started/overview) Chainlit is an open-source Python package to build production ready Conversational AI.
25. [DeepSpeed:](https://github.com/microsoft/DeepSpeed) DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.
26. [BMTrain:](https://github.com/OpenBMB/BMTrain) MTrain is an efficient large model training toolkit that can be used to train large models with tens of billions of parameters. It can train models in a distributed manner while keeping the code as simple as stand-alone training.
27. [LM Evaluation Harness:](https://github.com/EleutherAI/lm-evaluation-harness) The project provides a unified framework to test generative language models on a large number of different evaluation tasks.
28. [Lighteval:](https://github.com/huggingface/lighteval) LightEval is a lightweight LLM evaluation suite that Hugging Face has been using internally with the recently released LLM data processing library datatrove and LLM training library nanotron.
29. [InstructEval:](https://github.com/declare-lab/instruct-eval) InstructEval is an ICL evaluation suite to conduct a thorough assessment of these techniques. The suite includes 13 open-sourced LLMs of varying scales from four model families, and covers nine tasks across three categories.
30. [Bonito:](https://github.com/BatsResearch/bonito) Bonito is an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. This repo is a lightweight library for Bonito to easily create synthetic datasets built on top of the Hugging Face transformers and vllm libraries.
31. [LitGPT:](https://github.com/Lightning-AI/litgpt) Hackable implementation of state-of-the-art open-source large language models released under the Apache 2.0 license.
32. [DSPy:](https://github.com/stanfordnlp/dspy) DSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used one or more times within a pipeline. To use LMs to build a complex system without DSPy, you generally have to: (1) break the problem down into steps, (2) prompt your LM well until each step works well in isolation, (3) tweak the steps to work well together, (4) generate synthetic examples to tune each step, and (5) use these examples to finetune smaller LMs to cut costs. Currently, this is hard and messy: every time you change your pipeline, your LM, or your data, all prompts (or finetuning steps) may need to change.
33. [Guidance:](https://github.com/guidance-ai/guidance) Guidance is a programming paradigm that offers superior control and efficiency compared to conventional prompting and chaining. It allows users to constrain generation (e.g. with regex and CFGs) as well as to interleave control (conditional, loops) and generation seamlessly. 
34. [Mergekit:](https://github.com/arcee-ai/mergekit) mergekit is a toolkit for merging pre-trained language models. mergekit uses an out-of-core approach to perform unreasonably elaborate merges in resource-constrained situations. Merges can be run entirely on CPU or accelerated with as little as 8 GB of VRAM. Many merging algorithms are supported, with more coming as they catch my attention.
35. [Turkish NLP Resources:](https://github.com/agmmnn/turkish-nlp-resources) urkish NLP (Türkçe Doğal Dil İşleme) related Tools, Libraries, Models, Datasets and other resources.
36. [nanoGPT:](https://github.com/karpathy/nanoGPT) The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of minGPT that prioritizes teeth over education.
37. [OpenGPT:](https://github.com/CogStack/OpenGPT) A framework for creating grounded instruction based datasets and training conversational domain expert Large Language Models (LLMs).
38. [OpenAGI:](https://github.com/agiresearch/OpenAGI) The project presents OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language queries, serving as input to the LLM. The LLM subsequently selects, synthesizes, and executes models provided by OpenAGI to address the task. Furthermore, the project presents the Reinforcement Learning from Task Feedback (RLTF) mechanism, which uses the task-solving result as feedback to improve the LLM's task-solving ability. Thus, the LLM is responsible for synthesizing various external models for solving complex tasks, while RLTF provides feedback to improve its task-solving ability, enabling a feedback loop for self-improving AI. We believe that the paradigm of LLMs operating various expert models for complex task-solving is a promising approach towards AGI.
39. [PromptAppGPT:](https://github.com/mleoking/PromptAppGPT) PromptAppGPT is a low-code prompt-based rapid app development framework. PromptAppGPT contains features such as low-code prompt-based development, GPT text generation, DALLE image generation, online prompt editer+compiler+runer, automatic user interface generation, support for plug-in extensions, etc. PromptAppGPT aims to enable natural language app development based on GPT.
40. [OpenPrompt:](https://github.com/thunlp/OpenPrompt) Prompt-learning is the latest paradigm to adapt pre-trained language models (PLMs) to downstream NLP tasks, which modifies the input text with a textual template and directly uses PLMs to conduct pre-trained tasks. This library provides a standard, flexible and extensible framework to deploy the prompt-learning pipeline. OpenPrompt supports loading PLMs directly from huggingface transformers. In the future, we will also support PLMs implemented by other libraries.
41. [DeepSparse:](https://github.com/neuralmagic/deepsparse) DeepSparse is a CPU inference runtime that takes advantage of sparsity to accelerate neural network inference. Coupled with SparseML, our optimization library for pruning and quantizing your models, DeepSparse delivers exceptional inference performance on CPU hardware.
42. [Vllm:](https://github.com/vllm-project/vllm) vLLM is a fast and easy-to-use library for LLM inference and serving.
43. [txtai:](https://github.com/neuml/txtai) txtai is an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.