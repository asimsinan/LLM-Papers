
## 2023 (226 papers)

1. [SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot](https://arxiv.org/abs/2301.00774v3), Elias Frantar,Dan Alistarh, 02-01-2023
      ### Categories
      Machine Learning
     ### Abstract

1. [Faithful Chain-of-Thought Reasoning](https://arxiv.org/abs/2301.13379v3), Qing Lyu,Shreya Havaldar,Adam Stein,Li Zhang,Delip Rao,Eric Wong,Marianna Apidianaki,Chris Callison-Burch, 31-01-2023
      ### Categories
      Computation and Language
     ### Abstract
     While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.
     ### Bullet Points

     * Faithful CoT is a reasoning framework involving two stages: Translation and Problem Solving, which guarantees a faithful explanation of the final answer

     * It outperforms standard CoT on 9 out of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference

     * It sets the new state-of-the-art few-shot performance on 7 datasets with 95.0+ accuracy on 6 of them, showing a strong synergy between faithfulness and accuracy.




1. [Large Language Models Can Be Easily Distracted by Irrelevant Context](https://arxiv.org/abs/2302.00093v3), Freda Shi,Xinyun Chen,Kanishka Misra,Nathan Scales,David Dohan,Ed Chi,Nathanael Sch√§rli,Denny Zhou, 31-01-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.
     ### Bullet Points

     * Large language models have impressive performance on natural language processing tasks, but they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task

     * To investigate the distractibility of large language models, we use Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information

     * We find that the model performance is significantly decreased when irrelevant information is included, and we identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding an instruction that tells the language model to ignore irrelevant information to the prompt.


1. [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/abs/2301.13688), Shayne Longpre,Le Hou,Tu Vu,Albert Webson,Hyung Won Chung,Yi Tay,Denny Zhou,Quoc V. Le,Barret Zoph,Jason Wei,Adam Roberts, 31-01-2023
      ### Categories
      Artificial Intelligence, Computation and Language, Machine Learning
     ### Abstract


1. [Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning](https://arxiv.org/abs/2301.13808v3), Yunhu Ye,Binyuan Hui,Min Yang,Binhua Li,Fei Huang,Yongbin Li, 31-01-2023
      ### Categories
      Computation and Language
     ### Abstract
     Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition, most existing methods struggle to reason over complex questions since the required information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning; and (ii) decompose complex questions into simpler sub-questions for text reasoning. Specifically, we first use the LLMs to break down the evidence (tables) involved in the current question, retaining the relevant evidence and excluding the remaining irrelevant evidence from the huge table. In addition, we propose a "parsing-execution-filling" strategy to alleviate the hallucination dilemma of the chain of thought by decoupling logic and numerical computation in each step. Extensive experiments show that our method can effectively leverage decomposed evidence and questions and outperforms the strong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably, our model outperforms human performance for the first time on the TabFact dataset.
     ### Bullet Points

     * Table-based reasoning has shown progress in combining deep models with discrete reasoning

     * However, previous solutions suffer from performance degradation on huge evidence, and most existing methods struggle to reason over complex questions

     * To alleviate these challenges, we use large language models (LLMs) to decompose large evidence into sub-evidence and simplify complex questions into simpler sub-questions for text reasoning

     * We propose a "parsing-execution-filling" strategy to alleviate the hallucination dilemma of the chain of thought by decoupling logic and numerical computation in each step

     * Extensive experiments show that our method can effectively leverage decomposed evidence and questions and outperforms strong baselines on TabFact, WikiTableQuestion, and FetaQA datasets

     * Notably, our model outperformed human performance for the first time on the tabFact dataset.




1. [Collaborating with language models for embodied reasoning](https://arxiv.org/abs/2302.00763v1), Ishita Dasgupta,Christine Kaeser-Chen,Kenneth Marino,Arun Ahuja,Sheila Babayan,Felix Hill,Rob Fergus, 01-02-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language
     ### Abstract
     Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.
     ### Bullet Points

     * Reinforcement Learning (RL) agents aim to reason in a complex and ambiguous environment, but RL agents require a large amount of training data and often struggle to generalize to new unseen environments and new tasks

     * Large Scale Language Models (LSLMs) have strong reasoning ability and can adapt to new tasks through in-context learning, but they do not inherently have the ability to interrogate or intervene on the environment

     * To combine these complementary abilities, we propose a system consisting of three parts: a Planner, an Actor, and a Reporter

     * The Planner is a pre-trained language model that can issue commands to a simple embodied agent, while the Reporter communicates with the Planner to inform its next command

     * We present a set of tasks that require reasoning, test this system's generalization ability, investigate failure cases, and demonstrate how components of this system can



1. [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752), Albert Gu,Tri Dao, 01-12-2023
      ### Categories
      Machine Learning, Artificial Intelligence
     ### Abstract
     Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.
     ### Bullet Points

    * Foundation models in deep learning are based on the Transformer architecture and its core attention module

    * Subquadratic-time architectures such as linear attention, gated convolution, and recurrent models have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language

    * To address this weakness, we have developed a hardware-aware parallel algorithm for integrating selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba)

    * Mamba achieves fast inference and linear scaling in sequence length, and its performance improves on real data up to million-length sequences

    * On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size.


1. [Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents](https://arxiv.org/abs/2302.01560v2), Zihao Wang,Shaofei Cai,Guanzhou Chen,Anji Liu,Xiaojian Ma,Yitao Liang, 03-02-2023
      ### Categories
      Artificial Intelligence
     ### Abstract

1. [Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals](https://arxiv.org/abs/2302.04449v3), Yue Wu,Yewen Fan,Paul Pu Liang,Amos Azaria,Yuanzhi Li,Tom M. Mitchell, 09-02-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language
     ### Abstract
     High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary reward is then provided to a standard A2C RL agent, when interaction is detected. Experimentally, various RL algorithms obtain significant improvement in performance and training speed when assisted by our design.
     ### Bullet Points

     * The Read and Reward framework is designed to speed up RL algorithms on Atari games by reading manuals released by the Atari game developers

     * The framework consists of a QA Extraction module and a Reasoning module that evaluates object-agent interactions based on information from the manual, and an auxiliary reward is provided to a standard A2C RL agent when interaction is detected

     * This leads to significant improvement in performance and training speed.




1. [Transformer models: an introduction and catalog](https://arxiv.org/abs/2302.07730v3), Xavier Amatriain,Ananth Sankar,Jie Bing,Praveen Kumar Bodigutla,Timothy J. Hazen,Michaeel Kazi, 12-02-2023
      ### Categories
      Computation and Language
     ### Abstract
     In the past few years we have seen the meteoric appearance of dozens of foundation models of the Transformer family, all of which have memorable and sometimes funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovations in Transformer models. Our catalog will include models that are trained using self-supervised learning (e.g., BERT or GPT3) as well as those that are further trained using a human-in-the-loop (e.g. the InstructGPT model used by ChatGPT).
     ### Bullet Points

     * The paper aims to provide a comprehensive catalog and classification of the most popular Transformer models, including models trained using self-supervised learning and human-in-the-loop training

     * It also includes an introduction to the most important aspects and innovations in Transformer models.




1. [Guiding Pretraining in Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2302.06692v2), Yuqing Du,Olivia Watkins,Zihan Wang,C√©dric Colas,Trevor Darrell,Pieter Abbeel,Abhishek Gupta,Jacob Andreas, 13-02-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language
     ### Abstract
     .
     None


1. [GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks](https://arxiv.org/abs/2302.08043v3), Zemin Liu,Xingtong Yu,Yuan Fang,Xinming Zhang, 16-02-2023
      ### Categories
      Machine Learning, Computation and Language
     ### Abstract
     Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks(GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily rely on a large amount of task-specific supervision. To reduce labeling requirement, the "pre-train, fine-tune" and "pre-train, prompt" paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-train model in a task-specific manner. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt.
     ### Bullet Points

     * Graphs can model complex relationships between objects, enabling web applications such as online page/article classification and social recommendation

     * While graph neural networks (GNNs) are powerful for graph representation learning, their performance heavily relies on task-specific supervision

     * To reduce labeling requirement, the "pre-train, fine-tune" and "prep, prompt" paradigms have become increasingly common

     * In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs that unifies pre-train and downstream tasks into a common task template, and employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-trained model in a task specific manner

     * Experiments on five public datasets are conducted to evaluate and analyze this framework.




1. [A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT](https://arxiv.org/abs/2302.11382v1), Jules White,Quchen Fu,Sam Hays,Michael Sandborn,Carlos Olea,Henry Gilbert,Ashraf Elnashar,Jesse Spencer-Smith,Douglas C. Schmidt, 21-02-2023
      ### Categories
      Software Engineering, Artificial Intelligence
     ### Abstract
     Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.
     ### Bullet Points

     * The paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs

     * Prompt patterns are a knowledge transfer method that provide reusable solutions to common problems faced in a particular context

     * The paper provides contributions to research on prompt engineering that apply LLM to automate software development tasks by providing a framework for documenting patterns for structuring prompts to solve a range of problems, presenting a catalogue of patterns that are applied successfully to improve the outputs of LLM conversations, and explaining how prompts can be built from multiple patterns and illustrated by combination with other prompt patterns.




1. [Guiding Large Language Models via Directional Stimulus Prompting](https://arxiv.org/abs/2302.11520v4), Zekun Li,Baolin Peng,Pengcheng He,Michel Galley,Jianfeng Gao,Xifeng Yan, 22-02-2023
      ### Categories
      Computation and Language
     ### Abstract


1. [Active Prompting with Chain-of-Thought for Large Language Models](https://arxiv.org/abs/2302.12246v3), Shizhe Diao,Pengcheng Wang,Yong Lin,Tong Zhang, 23-02-2023
      ### Categories
      Computation and Language
     ### Abstract


1. [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1), Hugo Touvron,Thibaut Lavril,Gautier Izacard,Xavier Martinet,Marie-Anne Lachaux,Timoth√©e Lacroix,Baptiste Rozi√®re,Naman Goyal,Eric Hambro,Faisal Azhar,Aurelien Rodriguez,Armand Joulin,Edouard Grave,Guillaume Lample, 27-02-2023
      ### Categories
      Computation and Language
     ### Abstract
     We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.
     ### Bullet Points

     * LLaMA is a collection of foundation language models ranging from 7B to 65B parameters

     * We train them on trillions of tokens and show that it is possible to train state-of-the-art models using publicly available datasets exclusively

     * Our models outperform GPT-3 (175B) on most benchmarks and are competitive with Chinchilla-70B and PaLM-540B

     * We release all our models to the research community.




1. [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045v2), Shaohan Huang,Li Dong,Wenhui Wang,Yaru Hao,Saksham Singhal,Shuming Ma,Tengchao Lv,Lei Cui,Owais Khan Mohammed,Barun Patra,Qiang Liu,Kriti Aggarwal,Zewen Chi,Johan Bjorck,Vishrav Chaudhary,Subhojit Som,Xia Song,Furu Wei, 27-02-2023
      ### Categories
      Computation and Language, Computer Vision
     ### Abstract
     A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.
     ### Bullet Points

     * The work introduces Kosmos-1, a MLLM that can perceive general modalities, learn in context, and follow instructions

     * We train it on web-scale multimodal corpora, and evaluate various settings without any gradient updates or finetuning

     * Experimental results show that ILLMs achieve impressive performance on language understanding, generation, OCR-free NLP, perception-language tasks, multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks

     * Cross-modal transfer and a dataset of Raven IQ test are also introduced.




1. [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861v1), Zhen Wang,Rameswar Panda,Leonid Karlinsky,Rogerio Feris,Huan Sun,Yoon Kim, 06-03-2023
      ### Categories
      Computation and Language
     ### Abstract
     Prompt tuning, in which a base pretrained model is adapted to each task via conditioning on learned prompt vectors, has emerged as a promising approach for efficiently adapting large language models to multiple downstream tasks. However, existing methods typically learn soft prompt vectors from scratch, and it has not been clear how to exploit the rich cross-task knowledge with prompt vectors in a multitask learning setting. We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts. We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task. Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases, despite only tuning 0.035% as many task-specific parameters.
     ### Bullet Points

     * Prompt tuning is a promising approach for efficiently adapting large language models to multiple downstream tasks

     * However, existing methods typically learn soft prompt vectors from scratch, and it is not clear how to exploit the rich cross-task knowledge with prompt Vectors in a multitask learning setting

     * We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts

     * We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task

     * Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases.




1. [Foundation Models for Decision Making: Problems, Methods, and Opportunities](https://arxiv.org/abs/2303.04129v1), Sherry Yang,Ofir Nachum,Yilun Du,Jason Wei,Pieter Abbeel,Dale Schuurmans, 07-03-2023
      ### Categories
      Artificial Intelligence, Machine Learning
     ### Abstract
     Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.
     ### Bullet Points

     * Foundation models have demonstrated extraordinary capabilities in vision and language tasks and interact with other entities and agents in real-world environments

     * New paradigms are emerging for training foundation models to perform long-term reasoning, leveraging ever-larger datasets curated for multimodal, multitask, and generalist interaction

     * Research at the intersection of foundation models and decision making holds promise for creating powerful new systems that can interact effectively across diverse applications such as dialogue, autonomous driving, healthcare, education, and robotics

     * Recent approaches ground foundation models in practical decision making applications through prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.




1. [Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification](https://arxiv.org/abs/2303.07142), Benjamin Clavi√©,Alexandru Ciceu,Frederick Naylor,Guillaume Souli√©,Thomas Brightwell, 13-03-2023
      ### Categories
      Computation and Language
     ### Abstract
     This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language job posting is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance. Our results show that, with a well-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all other models, achieving a 6% increase in Precision@95% Recall compared to the best supervised approach. Furthermore, we observe that the wording of the prompt is a critical factor in eliciting the appropriate "reasoning" in the model, and that seemingly minor aspects of the prompt significantly affect the model's performance.
     ### Bullet Points

    * The case study investigates the task of job classification in a real-world setting to determine whether an English-language job posting is appropriate for a graduate or entry-level position

    * We explore different approaches to text classification, such as supervised approaches like SVMs and state-of-the-art deep learning methods like DeBERTa, and compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings

    * We employ prompt engineering to guide the LLMs towards the desired output

    * We evaluate the performance of two commercially available GPT-3.5-based language models, text-davinci-003 and gpt-3.5 -turbo and conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance.

   

1. [Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification](https://arxiv.org/abs/2303.07142v3), Benjamin Clavi√©,Alexandru Ciceu,Frederick Naylor,Guillaume Souli√©,Thomas Brightwell, 13-03-2023
      ### Categories
      Computation and Language
     ### Abstract
     This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language job posting is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance. Our results show that, with a well-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all other models, achieving a 6% increase in Precision@95% Recall compared to the best supervised approach. Furthermore, we observe that the wording of the prompt is a critical factor in eliciting the appropriate "reasoning" in the model, and that seemingly minor aspects of the prompt significantly affect the model's performance.
     ### Bullet Points

     * The case study investigates the task of job classification in a real-world setting to determine whether an English-language job posting is appropriate for a graduate or entry-level position

     * We explore different approaches to text classification, such as supervised approaches like SVMs and state-of-the-art deep learning methods like DeBERTa, and compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings

     * We employ prompt engineering to guide the LLMs towards the desired output

     * We evaluate the performance of two commercially available GPT-3.5-based language models, text-davinci-003 and gpt-3.5 -turbo and conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance.




1. [ART: Automatic multi-step reasoning and tool-use for large language models](https://arxiv.org/abs/2303.09014v1), Bhargavi Paranjape,Scott Lundberg,Sameer Singh,Hannaneh Hajishirzi,Luke Zettlemoyer,Marco Tulio Ribeiro, 16-03-2023
      ### Categories
      Computation and Language
     ### Abstract
     Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.
     ### Bullet Points

     * LLMs can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps

     * Each reasoning step can rely on external tools to support computation beyond the core LLM capabilities

     * Before work on CoT prompting and tool use, hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use is required

     * Automatic Reasoning and Tool-use (ART) is a framework that automatically generates intermediate reasoning steps from a task library

     * ART seamlessly pauses generation when external tools are called and integrates their output before resuming generation

     * The framework achieves a substantial improvement over few-shot prompting, automatic CoT on unseen tasks in BigBench and MMLU benchmarks, and matches performance of hand-crafted coT prompts on a majority of these tasks

     * It is extensible and easy for humans to




1. [AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512v2), Qingru Zhang,Minshuo Chen,Alexander Bukharin,Nikos Karampatziakis,Pengcheng He,Yu Cheng,Weizhu Chen,Tuo Zhao, 18-03-2023
      ### Categories
      Computation and Language, Machine Learning
     ### Abstract
     .
     None


1. [MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](https://arxiv.org/abs/2303.11381v1), Zhengyuan Yang,Linjie Li,Jianfeng Wang,Kevin Lin,Ehsan Azarnasab,Faisal Ahmed,Zicheng Liu,Ce Liu,Michael Zeng,Lijuan Wang, 20-03-2023
      ### Categories
      Computer Vision, Computation and Language, Machine Learning
     ### Abstract
     
     None


1. [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712v5), S√©bastien Bubeck,Varun Chandrasekaran,Ronen Eldan,Johannes Gehrke,Eric Horvitz,Ece Kamar,Peter Lee,Yin Tat Lee,Yuanzhi Li,Scott Lundberg,Harsha Nori,Hamid Palangi,Marco Tulio Ribeiro,Yi Zhang, 22-03-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.
     ### Bullet Points

     * AI researchers are developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition

     * The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data, and is part of a new cohort of LLMs that exhibit more general intelligence than previous AI models

     * The paper explores the rising capabilities and implications of these models, demonstrating that beyond its mastery of language and its ability to solve novel and difficult tasks, it can be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system

     * The challenges ahead for advancing towards deeper and more comprehensive versions of AGI include the possible need for pursuing a different paradigm that moves beyond next-word prediction

     * Reflections on societal influences of the recent technological leap and future research directions.




1. [Natural Language Reasoning, A Survey](https://arxiv.org/abs/2303.14725v2), Fei Yu,Hongbo Zhang,Prayag Tiwari,Benyou Wang, 26-03-2023
      ### Categories
      Computation and Language
     ### Abstract
     This survey paper proposes a clearer view of natural language reasoning in the field of Natural Language Processing (NLP), both conceptually and practically. Conceptually, we provide a distinct definition for natural language reasoning in NLP, based on both philosophy and NLP scenarios, discuss what types of tasks require reasoning, and introduce a taxonomy of reasoning. Practically, we conduct a comprehensive literature review on natural language reasoning in NLP, mainly covering classical logical reasoning, natural language inference, multi-hop question answering, and commonsense reasoning. The paper also identifies and views backward reasoning, a powerful paradigm for multi-step reasoning, and introduces defeasible reasoning as one of the most important future directions in natural language reasoning research. We focus on single-modality unstructured natural language text, excluding neuro-symbolic techniques and mathematical reasoning.
     ### Bullet Points

     * The survey paper proposes a clearer view of natural language reasoning in NLP, both conceptually and practically

     * It provides a definition for NLP based on philosophy and NLP scenarios, discusses tasks that require reasoning, and identifies and views backward reasoning as a powerful paradigm for multi-step reasoning

     * The literature review covers classical logical reasoning, natural language inference, multi-hop question answering, and commonsense reasoning

     * Backward reasoning is identified and viewed as one of the most important future directions in natural Language Reasoning research

     * The paper focuses on single-modality unstructured natural language text, excluding neuro-symbolic techniques and mathematical reasoning.




1. [Ecosystem Graphs: The Social Footprint of Foundation Models](https://arxiv.org/abs/2303.15772v1), Rishi Bommasani,Dilara Soylu,Thomas I. Liao,Kathleen A. Creel,Percy Liang, 28-03-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computers and Society
     ### Abstract
     . As of March 16, 2023, we annotate 262 assets (64 datasets, 128 models, 70 applications) from 63 organizations linked by 356 dependencies. We show Ecosystem Graphs functions as a powerful abstraction and interface for achieving the minimum transparency required to address myriad use cases. Therefore, we envision Ecosystem Graphs will be a community-maintained resource that provides value to stakeholders spanning AI researchers, industry professionals, social scientists, auditors and policymakers.
     ### Bullet Points

     * Ecosystem Graphs was annotated 262 assets from 63 organizations linked by 356 dependencies as of March 16, 2023

     * It functions as a powerful abstraction and interface for achieving minimum transparency to address various use cases

     * It will be a community-maintained resource that provides value to stakeholders including AI researchers, industry professionals, social scientists, auditors, and policymakers.




1. [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199v2), Renrui Zhang,Jiaming Han,Chris Liu,Peng Gao,Aojun Zhou,Xiangfei Hu,Shilin Yan,Pan Lu,Hongsheng Li,Yu Qiao, 28-03-2023
      ### Categories
      Computer Vision, Artificial Intelligence, Computation and Language, Machine Learning, Multimedia
     ### Abstract

1. [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.15647v1), Vladislav Lialin,Vijeta Deshpande,Anna Rumshisky, 28-03-2023
      ### Categories
      Computation and Language
     ### Abstract
     This paper presents a systematic overview and comparison of parameter-efficient fine-tuning methods covering over 40 papers published between February 2019 and February 2023. These methods aim to resolve the infeasibility and impracticality of fine-tuning large language models by only training a small set of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a specific focus on real-life efficiency and fine-tuning multibillion-scale language models.
     ### Bullet Points

    * The paper presents a comprehensive overview and comparison of parameter-efficient fine-tuning methods, covering over 40 papers published between February 2019 and February 2023

    * The methods aim to resolve the infeasibility and impracticality of large language models by only training a small set of parameters

    * The taxonomy covers a broad range of methods and provides a detailed method comparison with a focus on real-life efficiency and fine-tinging multibillion-scale language models.


1. [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564v3), Shijie Wu,Ozan Irsoy,Steven Lu,Vadim Dabravolski,Mark Dredze,Sebastian Gehrmann,Prabhanjan Kambadur,David Rosenberg,Gideon Mann, 30-03-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language, General Finance
     ### Abstract
     The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.
     ### Bullet Points

     * The work presents BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data

     * We construct a 363 billion token dataset based on Bloomberg's extensive data sources, augmented with 345 billion tokens from general purpose datasets

     * Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks

     * We explain our modeling choices, training process, and evaluation methodology, and release Training Chronicles (Appendix C).




1. [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://arxiv.org/abs/2303.17580v4), Yongliang Shen,Kaitao Song,Xu Tan,Dongsheng Li,Weiming Lu,Yueting Zhuang, 30-03-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Computer Vision, Machine Learning
     ### Abstract
     Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.
     ### Bullet Points

     * HuggingGPT is an LLM-powered agent that leverages LLMs to manage existing AI models to solve complex AI tasks, with language serving as a generic interface

     * It uses ChatGPT to conduct task planning, select models based on their function descriptions, execute subtasks with selected AI models, and summarize response according to execution results to achieve impressive results in language, vision, speech, and other challenging tasks, paving the way towards the realization of artificial general intelligence.




1. [Whose Opinions Do Language Models Reflect?](https://arxiv.org/abs/2303.17548v1), Shibani Santurkar,Esin Durmus,Faisal Ladhak,Cinoo Lee,Percy Liang,Tatsunori Hashimoto, 30-03-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Computers and Society, Machine Learning
     ### Abstract


1. [A Bibliometric Review of Large Language Models Research from 2017 to 2023](https://arxiv.org/abs/2304.02020v1), Lizhou Fan,Lingyao Li,Zihui Ma,Sanggyu Lee,Huizi Yu,Libby Hemphill, 03-04-2023
      ### Categories
      Digial Libraries, Computation and Language, Computers and Society, Social and Information Networks
     ### Abstract
     Large language models (LLMs) are a class of language models that have demonstrated outstanding performance across a range of natural language processing (NLP) tasks and have become a highly sought-after research area, because of their ability to generate human-like language and their potential to revolutionize science and technology. In this study, we conduct bibliometric and discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000 publications, this paper serves as a roadmap for researchers, practitioners, and policymakers to navigate the current landscape of LLMs research. We present the research trends from 2017 to early 2023, identifying patterns in research paradigms and collaborations. We start with analyzing the core algorithm developments and NLP tasks that are fundamental in LLMs research. We then investigate the applications of LLMs in various fields and domains including medicine, engineering, social science, and humanities. Our review also reveals the dynamic, fast-paced evolution of LLMs research. Overall, this paper offers valuable insights into the current state, impact, and potential of LLMs research and its applications.
     ### Bullet Points

     * The paper analyzes scholarly literature on LLMs and presents research trends from 2017 to early 2023, identifying patterns in research paradigms and collaborations

     * It provides insights into the current state, impact, and potential of LLM research and its applications.




1. [Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data](https://arxiv.org/abs/2304.01196v4), Canwen Xu,Daya Guo,Nan Duan,Julian McAuley, 03-04-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     .
     None


1. [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373v2), Stella Biderman,Hailey Schoelkopf,Quentin Anthony,Herbie Bradley,Kyle O'Brien,Eric Hallahan,Mohammad Aflah Khan,Shivanshu Purohit,USVSN Sai Prashanth,Edward Raff,Aviya Skowron,Lintang Sutawika,Oskar van der Wal, 03-04-2023
      ### Categories
      Computation and Language
     ### Abstract
     }.
     None


1. [One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era](https://arxiv.org/abs/2304.06488v1), Chaoning Zhang,Chenshuang Zhang,Chenghao Li,Yu Qiao,Sheng Zheng,Sumit Kumar Dam,Mengchun Zhang,Jung Uk Kim,Seong Tae Kim,Jinwoo Choi,Gyeong-Moon Park,Sung-Ho Bae,Lik-Hang Lee,Pan Hui,In So Kweon,Choong Seon Hong, 04-04-2023
      ### Categories
      Computers and Society, Artificial Intelligence, Computation and Language, Computer Vision, Machine Learning
     ### Abstract
     OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI.
     ### Bullet Points

     * OpenAI has released GPT-4 (also known as ChatGPT plus), which is a giant leap for artificial general intelligence (AGI)

     * It has attracted extensive media coverage and motivated researchers to investigate it from various aspects

     * More than 500 articles have been cited in their titles or abstracts, and a review is urgently needed to fill this gap

     * The work is the first to survey ChatGPt with a comprehensive review of its underlying technology, applications, and challenges, and presents an outlook on how it might evolve to realize general-purpose AIGC (AI-generated content), which will be a significant milestone for the development of AI.




1. [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442v2), Joon Sung Park,Joseph C. O'Brien,Carrie J. Cai,Meredith Ringel Morris,Percy Liang,Michael S. Bernstein, 07-04-2023
      ### Categories
      Human-Computer Interaction, Artificial Intelligence, Machine Learning
     ### Abstract
     Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.
     ### Bullet Points

     * The paper introduces generative agents, computational software agents that simulate believable human behavior

     * Generative agents wake up, cook breakfast, and head to work, artists paint, and authors write, form opinions, notice each other, initiate conversations, and remember and reflect on days past as they plan the next day

     * An architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesizes those memories over time into higher-level reflections, and retrieves them dynamically to plan behavior

     * These agents produce believable individual and emergent social behaviors by autonomously spreading invitations to a Valentine's Day party, making new acquaintances, asking each other out on dates to the party, and coordinate to show up for the party together at the right time

     * By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations




1. [OpenAssistant Conversations -- Democratizing Large Language Model Alignment](https://arxiv.org/abs/2304.07327v2), Andreas K√∂pf,Yannic Kilcher,Dimitri von R√ºtte,Sotiris Anagnostidis,Zhi-Rui Tam,Keith Stevens,Abdullah Barhoum,Nguyen Minh Duc,Oliver Stanley,Rich√°rd Nagyfi,Shahul ES,Sameer Suri,David Glushkov,Arnav Dantuluri,Andrew Maguire,Christoph Schuhmann,Huu Nguyen,Alexander Mattick, 14-04-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models. We release our code and data under a fully permissive licence.
     ### Bullet Points

     * Aligning large language models with human preferences has improved usability and led to rapid adoption

     * Alignment techniques such as supervised fine-tuning and reinforcement learning from human feedback reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs

     * However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary

     * To democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotrated conversation trees

     * The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers and shows consistent improvements on standard benchmarks over respective base




1. [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/abs/2304.13712v2), Jingfeng Yang,Hongye Jin,Ruixiang Tang,Xiaotian Han,Qizhang Feng,Haoming Jiang,Bing Yin,Xia Hu, 26-04-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     }.
     None


1. [MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks](https://arxiv.org/abs/2304.14979v1), Lei Zhang,Yuge Zhang,Kan Ren,Dongsheng Li,Yuqing Yang, 28-04-2023
      ### Categories
      Machine Learning, Artificial Intelligence
     ### Abstract
     The field of machine learning (ML) has gained widespread adoption, leading to a significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art LLMs to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the existing experiences of ML tasks and (ii) reason effectively to deliver promising results for new tasks. The solution generated can be used directly to achieve high levels of competitiveness.
     ### Bullet Points

     * The paper aims to bridge the gap between machine intelligence and human knowledge by introducing a novel framework called MLCopilot, which leverages state-of-the-art LLMs to develop ML solutions for novel tasks

     * The LLM can observe from existing experiences of ML tasks and reason effectively to deliver promising results for new tasks, and the solution can be used directly to achieve high levels of competitiveness.



1. [Can Large Language Models Be an Alternative to Human Evaluations?](https://arxiv.org/abs/2305.01937v1), Cheng-Han Chiang,Hung-yi Lee, 03-05-2023
      ### Categories
      Computation and Language, Human-Computer Interaction
     ### Abstract
     Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs. We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.
     ### Bullet Points

    * Human evaluation is essential for assessing the quality of texts generated by machine learning models or written by humans

    * However, human evaluation is difficult to reproduce and its quality is unstable, hindering fair comparisons among different NLP models and algorithms

    * Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided

    * We use LLMs to evaluate the texts in open-ended story generation and adversarial attacks

    * The results of LLM evaluation are consistent with the results obtained by expert human evaluation, and the texts rated higher by human experts are also ranked higher by the LLM

    * We are the first to show the potential of using LMLs to assess the quality and discuss the limitations and ethical considerations.



1. [Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents](https://arxiv.org/abs/2305.02412v2), Yue Wu,So Yeon Min,Yonatan Bisk,Ruslan Salakhutdinov,Amos Azaria,Yuanzhi Li,Tom Mitchell,Shrimai Prabhumoye, 03-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.
     ### Bullet Points

     * Pre-trained large language models (LLMs) capture procedural knowledge about the world

     * Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks

     * However, the transformer architecture inherits constraints that make it difficult for the LLM to directly serve as the agent

     * To maintain compatibility with a low-level trainable actor, we propose the Plan, Eliminate, and Track (PET) framework

     * The Plan module translates a task description into a list of high-level sub-tasks, the Elimininate module masks out irrelevant objects and receptacles from the observation for the current sub_task, and the Track module determines whether the agent has accomplished each subtask

     * The PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.




1. [Automatic Prompt Optimization with "Gradient Descent" and Beam Search](https://arxiv.org/abs/2305.03495v2), Reid Pryzant,Dan Iter,Jerry Li,Yin Tat Lee,Chenguang Zhu,Michael Zeng, 04-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language "gradients" that criticize the current prompt. The gradients are then "propagated" into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.
     ### Bullet Points

     * Automatic Prompt Optimization (APO) is a nonparametric solution to the problem of hand-written prompts in large language models

     * It uses numerical gradient descent to automatically improve prompts by using minibatches of data to form natural language "gradients" that criticize the current prompt

     * The gradients are then "propagated" into the prompt by editing the prompt in the opposite semantic direction of the gradient

     * This process is guided by a beam search and bandit selection procedure, which significantly improves algorithmic efficiency

     * Preliminary results suggest that APO can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31% by using data to rewrite vague task descriptions into more precise annotation instructions.



1. [PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits](https://arxiv.org/abs/2305.02547), Hang Jiang,Xiajie Zhang,Xubo Cao,Cynthia Breazeal,Jad Kabbara,Deb Roy, 04-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Human-Computer Interaction
     ### Abstract
     Despite the many use cases for large language models (LLMs) in creating personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits. We consider studying the behavior of LLM-based agents which we refer to as LLM personas and present a case study with GPT-3.5 and GPT-4 to investigate whether LLMs can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct LLM personas based on the Big Five personality model, have them complete the 44-item Big Five Inventory (BFI) personality test and a story writing task, and then assess their essays with automatic and human evaluations. Results show that LLM personas' self-reported BFI scores are consistent with their designated personality types, with large effect sizes observed across five traits. Additionally, LLM personas' writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80\%. Interestingly, the accuracy drops significantly when the annotators were informed of the AI's authorship.
     ### Bullet Points

    * LLM-based agents can accurately and consistently reflect specific personality traits despite limited research on their accuracy

    * A case study with GPT-3.5 and GPT-4 was conducted to investigate whether LLMs can generate content that aligns with their assigned personality profiles

    * We simulated LLM personas based on the Big Five personality model, have them complete the 44-item Big Five Inventory (BFI) personality test and a story writing task, and then assess their essays with automatic and human evaluations

    * Results showed that LLM characteras' self-reported BFI scores are consistent with their designated personality types, with large effect sizes observed across five traits

    * LLM writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus

    * Human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80%, but the accuracy drops significantly when annotators were informed of the AI's


1. [Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/abs/2305.03047), Zhiqing Sun,Yikang Shen,Qinhong Zhou,Hongxin Zhang,Zhenfang Chen,David Cox,Yiming Yang,Chuang Gan, 04-05-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language, Computers and Society
     ### Abstract
     Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.
     ### Bullet Points

    * SELF-ALIGN is a novel approach that combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision

    * It involves four stages:

    * Generate synthetic prompts and a topic-guided method to augment the prompt diversity


    * Use a small set of human-written principles for AI models to follow and guide the LLM through in-context learning from demonstrations


    * Fine-tune the original LLM with high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore


    * Offer a refinement step to address the issues of overly-brief or indirect responses


    * Develop an AI assistant named Dromedary with fewer than 300 lines of human annotations, including  200 seed prompts, 16 generic principles, and 5 exemplar



1. [Query Expansion by Prompting Large Language Models](https://arxiv.org/abs/2305.03653), Rolf Jagerman,Honglei Zhuang,Zhen Qin,Xuanhui Wang,Michael Bendersky, 05-05-2023
      ### Categories
      Information Retrieval, Information Search and Retrieval
     ### Abstract
     Query expansion is a widely used technique to improve the recall of search systems. In this paper, we propose an approach to query expansion that leverages the generative abilities of Large Language Models (LLMs). Unlike traditional query expansion approaches such as Pseudo-Relevance Feedback (PRF) that relies on retrieving a good set of pseudo-relevant documents to expand queries, we rely on the generative and creative abilities of an LLM and leverage the knowledge inherent in the model. We study a variety of different prompts, including zero-shot, few-shot and Chain-of-Thought (CoT). We find that CoT prompts are especially useful for query expansion as these prompts instruct the model to break queries down step-by-step and can provide a large number of terms related to the original query. Experimental results on MS-MARCO and BEIR demonstrate that query expansions generated by LLMs can be more powerful than traditional query expansion methods.
     ### Bullet Points

    * The paper proposes an approach to query expansion that utilizes the generative abilities of Large Language Models (LLMs) and leverages the knowledge inherent in the model

    * The authors study different prompts such as zero-shot, few-shot and Chain-of-Thought (CoT) and find that LLMs are particularly useful for query expansion as they instruct the model to break queries down step-by-step and provide a large number of terms related to the original query

    * Experimental results on MS-MARCO and BEIR demonstrate that query expansions generated by LLM can be more powerful than traditional query expansion methods.



1. [Exploring Human-Like Translation Strategy with Large Language Models](https://arxiv.org/abs/2305.04118v3), Zhiwei He,Tian Liang,Wenxiang Jiao,Zhuosheng Zhang,Yujiu Yang,Rui Wang,Zhaopeng Tu,Shuming Shi,Xing Wang, 06-05-2023
      ### Categories
      Computation and Language
     ### Abstract
     .
     None


1. [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759v2), Ronen Eldan,Yuanzhi Li, 12-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.
     ### Bullet Points

     * TinyStories can facilitate the development, analysis, and research of LMs, especially for low-resource or specialized domains, and provide insights into the emergence of language capabilities in the LM

     * The goal is to provide a platform for LM development and research.



1. [AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction](https://arxiv.org/abs/2305.09620v3), Junsol Kim,Byungkyu Lee, 16-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     Large language models (LLMs) that produce human-like responses have begun to revolutionize research practices in the social sciences. We develop a novel methodological framework that fine-tunes LLMs with repeated cross-sectional surveys to incorporate the meaning of survey questions, individual beliefs, and temporal contexts for opinion prediction. We introduce two new emerging applications of the AI-augmented survey: retrodiction (i.e., predict year-level missing responses) and unasked opinion prediction (i.e., predict entirely missing responses). Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our models based on Alpaca-7b excel in retrodiction (AUC = 0.86 for personal opinion prediction, $\rho$ = 0.98 for public opinion prediction). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. On the other hand, our fine-tuned Alpaca-7b models show modest success in unasked opinion prediction (AUC = 0.73, $\rho$ = 0.67). We discuss practical constraints and ethical concerns regarding individual autonomy and privacy when using LLMs for opinion prediction. Our study demonstrates that LLMs and surveys can mutually enhance each other's capabilities: LLMs can broaden survey potential, while surveys can improve the alignment of LLMs.
     ### Bullet Points

    * The study proposes a methodological framework that fine-tunes LLMs with cross-sectional surveys to incorporate the meaning of survey questions, individual beliefs, and temporal contexts for opinion prediction

    * Two new applications of the AI-augmented survey are retrodiction and unasked opinion prediction, with Alpaca-7b models exceling in retrodiction

    * The study also discusses practical constraints and ethical concerns regarding individual autonomy and privacy when using LLM models

    * Overall, the study shows that LLM and surveys can mutually enhance each other's capabilities by broadening survey potential and improving survey alignment.


1. [Towards Expert-Level Medical Question Answering with Large Language Models](https://arxiv.org/abs/2305.09617), Karan Singhal,Tao Tu,Juraj Gottweis,Rory Sayres,Ellery Wulczyn,Le Hou,Kevin Clark,Stephen Pfohl,Heather Cole-Lewis,Darlene Neal,Mike Schaekermann,Amy Wang,Mohamed Amin,Sami Lachgar,Philip Mansfield,Sushant Prakash,Bradley Green,Ewa Dominowska,Blaise Aguera y Arcas,Nenad Tomasev,Yun Liu,Renee Wong,Christopher Semturs,S. Sara Mahdavi,Joelle Barral,Dale Webster,Greg S. Corrado,Yossi Matias,Shekoofeh Azizi,Alan Karthikesalingam,Vivek Natarajan, 16-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.
     ### Bullet Points

    * Further studies are needed to validate the efficacy of these models in real-world settings, but these results highlight progress towards physician-level performance in medical question answering

    * Further studies may be necessary to validate their effectiveness in real life settings, despite the need for further studies.

1. [CoEdIT: Text Editing by Task-Specific Instruction Tuning](https://arxiv.org/abs/2305.09857), Vipul Raheja,Dhruv Kumar,Ryan Koo,Dongyeop Kang, 17-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Natural Language Processing
     ### Abstract


1. [Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt](https://arxiv.org/abs/2305.11186v2), Zhaozhuo Xu,Zirui Liu,Beidi Chen,Yuxin Tang,Jue Wang,Kaixiong Zhou,Xia Hu,Anshumali Shrivastava, 17-05-2023
      ### Categories
      Computation and Language, Machine Learning
     ### Abstract
     While the numerous parameters in Large Language Models (LLMs) contribute to their superior performance, this massive scale makes them inefficient and memory-hungry. Thus, they are hard to deploy on commodity hardware, such as one single GPU. Given the memory and power constraints of such devices, model compression methods are widely employed to reduce both the model size and inference latency, which essentially trades off model quality in return for improved efficiency. Thus, optimizing this accuracy-efficiency trade-off is crucial for the LLM deployment on commodity hardware. In this paper, we introduce a new perspective to optimize this trade-off by prompting compressed models. Specifically, we first observe that for certain questions, the generation quality of a compressed LLM can be significantly improved by adding carefully designed hard prompts, though this isn't the case for all questions. Based on this observation, we propose a soft prompt learning method where we expose the compressed model to the prompt learning process, aiming to enhance the performance of prompts. Our experimental analysis suggests our soft prompt strategy greatly improves the performance of the 8x compressed LLaMA-7B model (with a joint 4-bit quantization and 50% weight pruning compression), allowing them to match their uncompressed counterparts on popular benchmarks. Also, we demonstrate that these learned prompts can be transferred across various datasets, tasks, and compression levels. Hence with this transferability, we can stitch the soft prompt to a newly compressed model to improve the test-time accuracy in an ``in-situ'' way.
     ### Bullet Points

     * The paper proposes a soft prompt learning method to optimize the accuracy-efficiency trade-off between model size and inference latency for LLM deployment on commodity hardware

     * The method involves exposing the compressed model to the prompt learning process to enhance the performance of prompts

     * The soft prompt strategy greatly improves the generation quality of the 8x compressed LLaMA-7B model, allowing them to match their uncompressed counterparts on popular benchmarks and can be transferred across various datasets, tasks, and compression levels.




1. [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601v2), Shunyu Yao,Dian Yu,Jeffrey Zhao,Izhak Shafran,Thomas L. Griffiths,Yuan Cao,Karthik Narasimhan, 17-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract



1. [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206v1), Chunting Zhou,Pengfei Liu,Puxin Xu,Srini Iyer,Jiao Sun,Yuning Mao,Xuezhe Ma,Avia Efrat,Ping Yu,Lili Yu,Susan Zhang,Gargi Ghosh,Mike Lewis,Luke Zettlemoyer,Omer Levy, 18-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.
     ### Bullet Points

     * Large language models are trained in two stages: unsupervised pretraining from raw text, and large scale instruction tuning and reinforcement learning

     * LIMA, a 65B parameter LLaMa language model fine-tuned with standard supervised loss on only 1,000 carefully curated prompts and responses, demonstrates strong performance and generalizes well to unseen tasks

     * In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases, and only limited instruction tuning data is necessary to teach models to produce high quality output.




1. [Reasoning Implicit Sentiment with Chain-of-Thought Prompting](https://arxiv.org/abs/2305.11255v4), Hao Fei,Bobo Li,Qian Liu,Lidong Bing,Fei Li,Tat-Seng Chua, 18-05-2023
      ### Categories
      Computation and Language

1. [RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048), Bo Peng,Eric Alcaide,Quentin Anthony,Alon Albalak,Samuel Arcadinho,Stella Biderman,Huanqi Cao,Xin Cheng,Michael Chung,Matteo Grella,Kranthi Kiran GV,Xuzheng He,Haowen Hou,Jiaju Lin,Przemyslaw Kazienko,Jan Kocon,Jiaming Kong,Bartlomiej Koptyra,Hayden Lau,Krishna Sri Ipsit Mantri,Ferdinand Mom,Atsushi Saito,Guangyu Song,Xiangru Tang,Bolun Wang,Johan S. Wind,Stanislaw Wozniak,Ruichong Zhang,Zhenyuan Zhang,Qihang Zhao,Peng Zhou,Qinghua Zhou,Jian Zhu,Rui-Jie Zhu, 22-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.
     ### Bullet Points

    * The approach uses a linear attention mechanism to formulate the model as either Transformer or RNN, parallelizing computations during training and maintaining constant computational and memory complexity during inference

    * We scale models as large as 14 billion parameters and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models

    * This presents a significant step towards reconciling computational efficiency and model performance in sequence processing tasks.

1. [AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback](https://arxiv.org/abs/2305.14387v4), Yann Dubois,Xuechen Li,Rohan Taori,Tianyi Zhang,Ishaan Gulrajani,Jimmy Ba,Carlos Guestrin,Percy Liang,Tatsunori B. Hashimoto, 22-05-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language



1. [Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://arxiv.org/abs/2305.13860v1), Yi Liu,Gelei Deng,Zhengzi Xu,Yuekang Li,Yaowen Zheng,Ying Zhang,Lida Zhao,Tianwei Zhang,Yang Liu, 23-05-2023
      ### Categories
      Software Engineering, Artificial Intelligence, Computation and Language
     ### Abstract
     Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in jailbreaking LLMs and discusses the challenges of robust jailbreak prompt generation and prevention.
     ### Bullet Points

     * The study investigates three key research questions related to jailbreaking LLMs, including the number of different prompt types, the effectiveness of jailbreak prompts in circumventing LLM constraints, and the resilience of ChatGPT against these prompts

     * The study develops a classification model to analyze the distribution of existing prompts, assess the jailbreak capability of prompts with chatGPT versions 3.5 and 4.0, and evaluate the resistance of the prompts against the restrictions in 40 use-case scenarios

     * It emphasizes the importance of prompt structures and discusses the challenges of robust prisonbreak prompt generation and prevention.




1. [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314v1), Tim Dettmers,Artidoro Pagnoni,Ari Holtzman,Luke Zettlemoyer, 23-05-2023
      ### Categories
      Machine Learning
     ### Abstract
     We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.
     ### Bullet Points

     * QLoRA is an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit fineting task performance

     * Our best model family, Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuing

     * We use more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular fineting

     * GPT-4 evaluations are a cheap and reasonable alternative to human evaluation, and current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots

     * We release all of our models and code,




1. [The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning](https://arxiv.org/abs/2305.14045v2), Seungone Kim,Se June Joo,Doyoung Kim,Joel Jang,Seonghyeon Ye,Jamin Shin,Minjoon Seo, 23-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B & 11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and +2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until the max length by a +13.98% margin. Our code, the CoT Collection data, and model checkpoints are publicly available.
     ### Bullet Points

     * To equip smaller LMs with CoT reasoning capability, we introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection with additional rationales across 1,060 tasks

     * This dataset provides a step-by-step reasoning capability for smaller MLs, enabling them to have better CoT capabilities on unseen tasks

     * We report an average improvement of +4.34% and +2.60% in terms of zero-shot task accuracy and stronger few-shot learning capabilities on 4 domain-specific tasks, resulting in an improvement rate of +2.24% (Flan-T5 3B) and +2.37% (Flon-T5) 11B, even outperforming ChatGPT utilizing demonstrations until the max length by a +13.98% margin

     * The code, CoT collection data, and model checkpoints are publicly available.




1. [ExpertPrompting: Instructing Large Language Models to be Distinguished Experts](https://arxiv.org/abs/2305.14688v1), Benfeng Xu,An Yang,Junyang Lin,Quan Wang,Chang Zhou,Yongdong Zhang,Zhendong Mao, 24-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     }.
     None


1. [Reasoning with Language Model is Planning with World Model](https://arxiv.org/abs/2305.14992v2), Shibo Hao,Yi Gu,Haodi Ma,Joshua Jiahua Hong,Zhen Wang,Daisy Zhe Wang,Zhiting Hu, 24-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning $\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.
     ### Bullet Points

     * LLMs have remarkable reasoning capabilities, but they can still struggle with tasks that are easy for humans

     * Lack of an internal "textit" world model to predict the world textit state and simulate long-term outcomes of actions prevents them from performing deliberate planning akin to human brains

     * A new LLM reasoning framework, RAP, repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm based on Monto Carlo Tree Search for strategic exploration in the vast reasoning space

     * The LLM incrementally builds an reasoning tree under the guidance of LLM (as world model) and task-specific rewards, obtaining a high-reward reasoning path efficiently with a proper balance between exploration $textitvs.$ exploitation

     * Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and




1. [SPRING: Studying the Paper and Reasoning to Play Games](https://arxiv.org/abs/2305.15486v3), Yue Wu,Shrimai Prabhumoye,So Yeon Min,Yonatan Bisk,Ruslan Salakhutdinov,Amos Azaria,Tom Mitchell,Yuanzhi Li, 24-05-2023
      ### Categories
      Artificial Intelligence, Machine Learning
     ### Abstract
     Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context "reasoning" induced by different forms of prompts under the setting of the Crafter open-world environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of games as a test bed for LLMs.
     ### Bullet Points

     * Open-world survival games pose challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements

     * Reinforcement learning (RL) is popular for solving games, but its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft

     * A novel approach, SPRING, employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges

     * We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions

     * Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories

     * Quantitatively SPRING with GPT-4 outperforms all state-of the




1. [MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting](https://arxiv.org/abs/2305.16896v1), Tatsuro Inaba,Hirokazu Kiyomaru,Fei Cheng,Sadao Kurohashi, 26-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     Large language models (LLMs) have achieved impressive performance on various reasoning tasks. To further improve the performance, we propose MultiTool-CoT, a novel framework that leverages chain-of-thought (CoT) prompting to incorporate multiple external tools, such as a calculator and a knowledge retriever, during the reasoning process. We apply MultiTool-CoT to the Task 2 dataset of NumGLUE, which requires both numerical reasoning and domain-specific knowledge. The experiments show that our method significantly outperforms strong baselines and achieves state-of-the-art performance.
     ### Bullet Points

     * MultiTool-CoT is a novel framework that uses chain-of-thought prompting to improve LLM performance

     * We applied it to the Task 2 dataset of NumGLUE, which requires both numerical reasoning and domain-specific knowledge

     * Our method significantly outperforms strong baselines and achieves state of-the-art performance.




1. [Tab-CoT: Zero-shot Tabular Chain of Thought](https://arxiv.org/abs/2305.17812v1), Ziqi Jin,Wei Lu, 28-05-2023
      ### Categories
      Computation and Language
     ### Abstract
     The chain-of-though (CoT) prompting methods were successful in various natural language processing (NLP) tasks thanks to their ability to unveil the underlying complex reasoning processes. Such reasoning processes typically exhibit implicitly structured steps. Recent efforts also started investigating methods to encourage more explicitly structured reasoning procedures to be captured. In this work, we propose Tab-CoT, a novel tabular-format CoT prompting method, which allows the complex reasoning process to be explicitly modelled in a highly structured manner. Despite its simplicity, we show that our approach is capable of performing reasoning across multiple dimensions (i.e., both rows and columns). We demonstrate our approach's strong zero-shot and few-shot capabilities through extensive experiments on a range of reasoning tasks.
     ### Bullet Points

     * Tab-CoT is a novel tabular-format CoT prompting method that allows complex reasoning processes to be explicitly modelled in a highly structured manner

     * It is capable of performing reasoning across multiple dimensions and has strong zero-shot and few-shot capabilities despite its simplicity.


1. [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290), Rafael Rafailov,Archit Sharma,Eric Mitchell,Stefano Ermon,Christopher D. Manning,Chelsea Finn, 29-05-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language
     ### Abstract
     While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.
     ### Bullet Points

    * Direct Preference Optimization (DPO) is a stable, performant, and computationally lightweight algorithm that can fine-tune large unsupervised language models (LMs) to align with human preferences without sampling from the LM

    * It can be used to extract the corresponding optimal policy in closed form without drifting too far from the original model

    * DPO surpasses PPO-based RLHF in ability to control sentiment of generations, match or improve response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.



1. [Representation Engineering: A Top-Down Approach to AI Transparency](https://arxiv.org/abs/2310.01405), Andy Zou,Long Phan,Sarah Chen,James Campbell,Phillip Guo,Richard Ren,Alexander Pan,Xuwang Yin,Mantas Mazeika,Ann-Kathrin Dombrowski,Shashwat Goel,Nathaniel Li,Michael J. Byun,Zifan Wang,Alex Mallen,Steven Basart,Sanmi Koyejo,Dawn Song,Matt Fredrikson,J. Zico Kolter,Dan Hendrycks, 02-10-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language, Computer Vision, Computers and Society
     ### Abstract
     In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
     ### Bullet Points

    * The paper identifies and characterizes the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience

    * RepE places population-level representations at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs)

    * We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models

    * These methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research

    * The work hopes to further explore RepE and foster advancements in transparency and safety in AI systems.


1. [Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses](https://arxiv.org/abs/2305.19339v1), Liyan Tang,Yifan Peng,Yanshan Wang,Ying Ding,Greg Durrett,Justin F. Rousseau, 30-05-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     A human decision-maker benefits the most from an AI assistant that corrects for their biases. For problems such as generating interpretation of a radiology report given findings, a system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user. To alleviate biases in human decision-making, it is worth considering a broad differential diagnosis, going beyond the most likely options. We introduce a new task, "less likely brainstorming," that asks a model to generate outputs that humans think are relevant but less likely to happen. We explore the task in two settings: a brain MRI interpretation generation setting and an everyday commonsense reasoning setting. We found that a baseline approach of training with less likely hypotheses as targets generates outputs that humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective. To tackle this problem, we propose a controlled text generation method that uses a novel contrastive learning strategy to encourage models to differentiate between generating likely and less likely outputs according to humans. We compare our method with several state-of-the-art controlled text generation models via automatic and human evaluations and show that our models' capability of generating less likely outputs is improved.
     ### Bullet Points

     * A human decision-maker benefits the most from an AI assistant that corrects for biases

     * A system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user

     * A broad differential diagnosis, going beyond the most likely options, is recommended to alleviate bias in decision-making

     * A new task, "less likely brainstorming," asks a model to generate outputs that humans think are relevant but less likely to happen

     * We explored the task in two settings: a brain MRI interpretation generation setting and an everyday commonsense reasoning setting

     * We found that a baseline approach of training with less likely hypotheses as targets generates outputs which humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective

     * To tackle this problem, we propose a controlled text generation method that uses a novel contrastive learning strategy to encourage models to differentiate between generating likely and less likely output




1. [Chain-Of-Thought Prompting Under Streaming Batch: A Case Study](https://arxiv.org/abs/2306.00550v1), Yuxin Tang, 01-06-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language
     ### Abstract
     Recently, Large Language Models (LLMs) have demonstrated remarkable capabilities. Chain-of-Thought (CoT) has been proposed as a way of assisting LLMs in performing complex reasoning. However, developing effective prompts can be a challenging and labor-intensive task. Many studies come out of some way to automatically construct CoT from test data. Most of them assume that all test data is visible before testing and only select a small subset to generate rationales, which is an unrealistic assumption. In this paper, we present a case study on how to construct and optimize chain-of-thought prompting using batch data in streaming settings.
     ### Bullet Points

     * The paper presents a case study on how to construct and optimize chain-of-thought prompting using batch data in streaming settings

     * The paper explains how to use batch data to construct CoT prompts, which can be a challenging and labor-intensive task for LLMs.




1. [ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing](https://arxiv.org/abs/2306.00622v1), Ryan Liu,Nihar B. Shah, 01-06-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Digial Libraries
     ### Abstract
     Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.
     ### Bullet Points

     * LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not for complete evaluations of papers or proposals, based on the experiments they have been trained on

     * However, they may not be suitable for full evaluations as they do not have the necessary skills and experience to perform such tasks.



1. [ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing](https://arxiv.org/abs/2306.00622), Ryan Liu,Nihar B. Shah, 01-06-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Digial Libraries
     ### Abstract
     Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.
     ### Bullet Points

    * LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not for complete evaluations of papers or proposals, based on the experiments they have been trained on

    * However, they may not be suitable for full evaluations as they do not have the necessary skills and experience to perform such tasks.





1. [The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only](https://arxiv.org/abs/2306.01116v1), Guilherme Penedo,Quentin Malartic,Daniel Hesslow,Ruxandra Cojocaru,Alessandro Cappelli,Hamza Alobeidli,Baptiste Pannier,Ebtesam Almazrouei,Julien Launay, 01-06-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.
     ### Bullet Points

     * Large language models are commonly trained on filtered web data and curated high-quality corpora

     * Curation is necessary to produce performant models with broad zero-shot generalization abilities

     * However, it is unclear how scalable curation is and whether we will run out of unique high quality data soon

     * Properly filtered and deduplicated web data alone can lead to powerful models, even significantly outperforming models from state-of-the-art trained on The Pile

     * CommonCrawl has obtained five trillion tokens from the web, and 600 billion tokens are publicly released from our RefinedWeb dataset.


1. [Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents](https://arxiv.org/abs/2306.03314), Yashar Talebirad,Amirhossein Nadiri, 05-06-2023
      ### Categories
      Artificial Intelligence, Machine Learning, Multiagent Systems
     ### Abstract
     In this paper, we present a novel framework for enhancing the capabilities of large language models (LLMs) by leveraging the power of multi-agent systems. Our framework introduces a collaborative environment where multiple intelligent agent components, each with distinctive attributes and roles, work together to handle complex tasks more efficiently and effectively. We demonstrate the practicality and versatility of our framework through case studies in artificial general intelligence (AGI), specifically focusing on the Auto-GPT and BabyAGI models. We also examine the "Gorilla" model, which integrates external APIs into the LLM. Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations. By modeling various domains such as courtroom simulations and software development scenarios, we showcase the potential applications and benefits of our proposed multi-agent system. Our framework provides an avenue for advancing the capabilities and performance of LLMs through collaboration and knowledge exchange among intelligent agents.
     ### Bullet Points

    * The paper presents a new framework for enhancing the capabilities of large language models (LLMs) by leveraging multi-agent systems

    * The framework involves a collaborative environment where multiple intelligent agent components work together to handle complex tasks more efficiently and effectively

    * It addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations, and showcases potential applications and benefits through modeling various domains such as courtroom simulations and software development scenarios.


1. [When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm](https://arxiv.org/abs/2306.02552v2), Lei Wang,Jingsen Zhang,Hao Yang,Zhiyuan Chen,Jiakai Tang,Zeyu Zhang,Xu Chen,Yankai Lin,Ruihua Song,Wayne Xin Zhao,Jun Xu,Zhicheng Dou,Jun Wang,Ji-Rong Wen, 05-06-2023
      ### Categories
      Information Retrieval, Artificial Intelligence
     ### Abstract


1. [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685), Lianmin Zheng,Wei-Lin Chiang,Ying Sheng,Siyuan Zhuang,Zhanghao Wu,Yonghao Zhuang,Zi Lin,Zhuohan Li,Dacheng Li,Eric P. Xing,Hao Zhang,Joseph E. Gonzalez,Ion Stoica, 09-06-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract

1. [Mind2Web: Towards a Generalist Agent for the Web](https://arxiv.org/abs/2306.06070v3), Xiang Deng,Yu Gu,Boyuan Zheng,Shijie Chen,Samuel Stevens,Boshi Wang,Huan Sun,Yu Su, 09-06-2023
      ### Categories
      Computation and Language
     ### Abstract
     ) to facilitate further research on building a generalist agent for the web.
     ### Bullet Points

     * To facilitate further research on building a generalist agent for the web, you can use online resources such as websites, blogs, and forums to gather information on generalist agents and their capabilities

     * You can also use online forums and discussion boards to share your thoughts and ideas with others in the field.




1. [Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models](https://arxiv.org/abs/2306.06891v1), Soochan Lee,Gunhee Kim, 12-06-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Generating intermediate steps, or Chain of Thought (CoT), is an effective way to significantly improve language models' (LM) multi-step reasoning capability. However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size. Instead of increasing the context limit, which has already been heavily investigated, we explore an orthogonal direction: making LMs divide a problem into multiple contexts. We propose a new inference framework, called Recursion of Thought (RoT), which introduces several special tokens that the models can output to trigger context-related operations. Extensive experiments with multiple architectures including GPT-3 show that RoT dramatically improves LMs' inference capability to solve problems, whose solution consists of hundreds of thousands of tokens.
     ### Bullet Points

     * Generating intermediate steps, such as Chain of Thought, can improve language models' multi-step reasoning capability

     * However, the CoT lengths can grow rapidly with problem complexity, exceeding the maximum context size

     * Instead, we explore an orthogonal direction by making LMs divide a problem into multiple contexts

     * We propose a new inference framework called Recursion of Thoughth (RoT), which introduces several special tokens that the models can output to trigger context-related operations

     * Extensive experiments with multiple architectures, including GPT-3, show that RoT significantly improves LM's inference capability to solve problems, whose solution consists of hundreds of thousands of tokens.




1. [GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study](https://arxiv.org/abs/2307.05492), Zachary Robertson, 16-06-2023
      ### Categories
      Human-Computer Interaction, Artificial Intelligence, Computation and Language
     ### Abstract
     In this pilot study, we investigate the use of GPT4 to assist in the peer-review process. Our key hypothesis was that GPT-generated reviews could achieve comparable helpfulness to human reviewers. By comparing reviews generated by both human reviewers and GPT models for academic papers submitted to a major machine learning conference, we provide initial evidence that artificial intelligence can contribute effectively to the peer-review process. We also perform robustness experiments with inserted errors to understand which parts of the paper the model tends to focus on. Our findings open new avenues for leveraging machine learning tools to address resource constraints in peer review. The results also shed light on potential enhancements to the review process and lay the groundwork for further research on scaling oversight in a domain where human-feedback is increasingly a scarce resource.
     ### Bullet Points

    * The pilot study investigates the use of GPT4 to assist in the peer-review process by comparing reviews generated by both human reviewers and GPT models for academic papers submitted to a major machine learning conference

    * The results provide evidence that artificial intelligence can contribute effectively to the process

    * Robustness experiments with inserted errors are performed to understand which parts of the paper the model tends to focus on

    * These findings open new avenues for leveraging machine learning tools to address resource constraints in peer review

    * Further research is needed to scale oversight in a domain where human-feedback is scarce.




1. [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644v2), Suriya Gunasekar,Yi Zhang,Jyoti Aneja,Caio C√©sar Teodoro Mendes,Allie Del Giorno,Sivakanth Gopi,Mojan Javaheripi,Piero Kauffmann,Gustavo de Rosa,Olli Saarikivi,Adil Salim,Shital Shah,Harkirat Singh Behl,Xin Wang,S√©bastien Bubeck,Ronen Eldan,Adam Tauman Kalai,Yin Tat Lee,Yuanzhi Li, 20-06-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.
     ### Bullet Points

     * Phi-1 is a Transformer-based language model with 1.3B parameters trained for 4 days on 8 A100s using textbook quality data from the web and synthetically generated textbooks and exercises with GPT-3.5 tokens

     * It attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP

     * It also displays surprising emergent properties compared to phi-1-base, a model before finetuning stage on a dataset of coding exercises, and pha-1-small.




1. [Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT](https://arxiv.org/abs/2307.01202), Stephen Yang, 22-06-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language, Digial Libraries
     ### Abstract
     Analysis of innovation has been fundamentally limited by conventional approaches to broad, structural variables. This paper pushes the boundaries, taking an LLM approach to patent analysis with the groundbreaking ChatGPT technology. OpenAI's state-of-the-art textual embedding accesses complex information about the quality and impact of each invention to power deep learning predictive models. The nuanced embedding drives a 24% incremental improvement in R-squared predicting patent value and clearly isolates the worst and best applications. These models enable a revision of the contemporary Kogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median deviation of 1.5 times, accounting for potential institutional predictions. Furthermore, the market fails to incorporate timely information about applications; a long-short portfolio based on predicted acceptance rates achieves significant abnormal returns of 3.3% annually. The models provide an opportunity to revolutionize startup and small-firm corporate policy vis-a-vis patenting.
     ### Bullet Points

    * The paper proposes an LLM approach to patent analysis with ChatGPT technology, using OpenAI's textual embedding to improve R-squared predicting patent value and identify the worst and best applications

    * The models enable a revision of the valuation of patents by a median deviation of 1.5 times, accounting for potential institutional predictions

    * The market fails to incorporate timely information about applications and a long-short portfolio based on predicted acceptance rates achieves significant abnormal returns of 3.3% annually

    * This paper revolutionizes startup and small-firm corporate policy in patenting.




1. [Large Language Models Understand and Can be Enhanced by Emotional Stimuli](https://arxiv.org/abs/2307.11760v7), Cheng Li,Jindong Wang,Yixuan Zhang,Kaijie Zhu,Wenxin Hou,Jianxun Lian,Fang Luo,Qiang Yang,Xing Xie, 14-07-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Human-Computer Interaction
     ### Abstract
     Emotional intelligence significantly impacts our daily behaviors and interactions. Although Large Language Models (LLMs) are increasingly viewed as a stride toward artificial general intelligence, exhibiting impressive performance in numerous tasks, it is still uncertain if LLMs can genuinely grasp psychological emotional stimuli. Understanding and responding to emotional cues gives humans a distinct advantage in problem-solving. In this paper, we take the first step towards exploring the ability of LLMs to understand emotional stimuli. To this end, we first conduct automatic experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna, Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative applications that represent comprehensive evaluation scenarios. Our automatic experiments show that LLMs have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which we call "EmotionPrompt" that combines the original prompt with emotional stimuli), e.g., 8.00% relative performance improvement in Instruction Induction and 115% in BIG-Bench. In addition to those deterministic tasks that can be automatically evaluated using existing metrics, we conducted a human study with 106 participants to assess the quality of generative tasks using both vanilla and emotional prompts. Our human study results demonstrate that EmotionPrompt significantly boosts the performance of generative tasks (10.9% average improvement in terms of performance, truthfulness, and responsibility metrics). We provide an in-depth discussion regarding why EmotionPrompt works for LLMs and the factors that may influence its performance. We posit that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs interaction.
     ### Bullet Points

     * The paper explores the ability of LLMs to understand psychological emotional stimuli and investigates the effectiveness of EmotionPrompt, a generative task that combines the original prompt with emotional cues

     * The paper also conducts automatic experiments on 45 tasks and conducted a human study with 106 participants to assess the quality of generative tasks using both vanilla and emotional prompts

     * It highlights the importance of interdisciplinary knowledge for human-LLMs interaction.


1. [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288v2), Hugo Touvron,Louis Martin,Kevin Stone,Peter Albert,Amjad Almahairi,Yasmine Babaei,Nikolay Bashlykov,Soumya Batra,Prajjwal Bhargava,Shruti Bhosale,Dan Bikel,Lukas Blecher,Cristian Canton Ferrer,Moya Chen,Guillem Cucurull,David Esiobu,Jude Fernandes,Jeremy Fu,Wenyin Fu,Brian Fuller,Cynthia Gao,Vedanuj Goswami,Naman Goyal,Anthony Hartshorn,Saghar Hosseini,Rui Hou,Hakan Inan,Marcin Kardas,Viktor Kerkez,Madian Khabsa,Isabel Kloumann,Artem Korenev,Punit Singh Koura,Marie-Anne Lachaux,Thibaut Lavril,Jenya Lee,Diana Liskovich,Yinghai Lu,Yuning Mao,Xavier Martinet,Todor Mihaylov,Pushkar Mishra,Igor Molybog,Yixin Nie,Andrew Poulton,Jeremy Reizenstein,Rashi Rungta,Kalyan Saladi,Alan Schelten,Ruan Silva,Eric Michael Smith,Ranjan Subramanian,Xiaoqing Ellen Tan,Binh Tang,Ross Taylor,Adina Williams,Jian Xiang Kuan,Puxin Xu,Zheng Yan,Iliyan Zarov,Yuchen Zhang,Angela Fan,Melanie Kambadur,Sharan Narang,Aurelien Rodriguez,Robert Stojnic,Sergey Edunov,Thomas Scialom, 18-07-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.
     ### Bullet Points

     * Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) that are optimized for dialogue use cases

     * They outperform open-source chat models on most benchmarks we tested and may be a suitable substitute for closed-source models based on human evaluations for helpfulness and safety

     * Our approach to fine-ting and safety improvements is to enable the community to build on our work and contribute to responsible development of LLMs.




1. [A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis](https://arxiv.org/abs/2307.12856v3), Izzeddin Gur,Hiroki Furuta,Austin Huang,Mustafa Safdari,Yutaka Matsuo,Douglas Eck,Aleksandra Faust, 24-07-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language
     ### Abstract
     Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.
     ### Bullet Points

     * Pre-trained large language models (LLMs) have achieved better generalization and sample efficiency in autonomous web automation

     * However, the performance on real-world websites has suffered from open domainness, limited context length, and lack of inductive bias on HTML

     * We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions

     * We design webAgent with Flan-U-PaLM, HTML-T5, and new pretrained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives for planning and summarization

     * We empirically demonstrate that our modular recipe improves the success of real websites by over 50%, and that HTTP-T5 is the best model to solve various HTML understanding tasks, achieving 18.7% higher success rate than the prior method.




1. [WebArena: A Realistic Web Environment for Building Autonomous Agents](https://arxiv.org/abs/2307.13854v3), Shuyan Zhou,Frank F. Xu,Hao Zhu,Xuhui Zhou,Robert Lo,Abishek Sridhar,Xianyi Cheng,Tianyue Ou,Yonatan Bisk,Daniel Fried,Uri Alon,Graham Neubig, 25-07-2023
      ### Categories
      Artificial Intelligence, Computation and Language, Machine Learning
     ### Abstract
     With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
     ### Bullet Points

     * The paper builds an environment for language-guided agents that is highly realistic and reproducible, focusing on agents that perform tasks on the web

     * The environment is enriched with tools and external knowledge bases to encourage human-like task-solving

     * The benchmark tasks are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet

     * The best GPT-4-based agent achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%

     * Further development of robust agents is needed, as current state-of-the-art large language models are far from perfect performance in these real-life tasks

     * WebArena can be used to measure progress.




1. [MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework](https://arxiv.org/abs/2308.00352v5), Sirui Hong,Mingchen Zhuge,Jonathan Chen,Xiawu Zheng,Yuheng Cheng,Ceyao Zhang,Jinlin Wang,Zili Wang,Steven Ka Shing Yau,Zijuan Lin,Liyang Zhou,Chenyu Ran,Lingfeng Xiao,Chenglin Wu,J√ºrgen Schmidhuber, 01-08-2023
      ### Categories
      Artificial Intelligence, Multiagent Systems
     ### Abstract


1. [Evaluating ChatGPT text-mining of clinical records for obesity monitoring](https://arxiv.org/abs/2308.01666v1), Ivo S. Fins,(1),,Heather Davies,(1),,Sean Farrell,(2),,Jose R.Torres,(3),,Gina Pinchbeck,(1),,Alan D. Radford,(1),,Peter-John Noble,(1) ((1) Small Animal Veterinary Surveillance Network, Institute of Infection, Veterinary and Ecological Sciences, University of Liverpool, Liverpool, UK, (2) Department of Computer Science, Durham University, Durham, UK, (3) Institute for Animal Health and Food Safety, University of Las Palmas de Gran Canaria, Las Palmas, Canary Archipelago, Spain), 03-08-2023
      ### Categories
      Information Retrieval, Computation and Language
     ### Abstract
     Background: Veterinary clinical narratives remain a largely untapped resource for addressing complex diseases. Here we compare the ability of a large language model (ChatGPT) and a previously developed regular expression (RegexT) to identify overweight body condition scores (BCS) in veterinary narratives. Methods: BCS values were extracted from 4,415 anonymised clinical narratives using either RegexT or by appending the narrative to a prompt sent to ChatGPT coercing the model to return the BCS information. Data were manually reviewed for comparison. Results: The precision of RegexT was higher (100%, 95% CI 94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recall of ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that of RegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering is needed to improve ChatGPT output. Conclusions: Large language models create diverse opportunities and, whilst complex, present an intuitive interface to information but require careful implementation to avoid unpredictable errors.
     ### Bullet Points

     * The study compares the accuracy of ChatGPT and RegexT in identifying overweight body condition scores in veterinary clinical narratives

     * The results show that ChatGPPT has a higher precision (100%, 95% CI 94.81-100%) compared to RegexP, but the recall is considerably higher than that of RegExP

     * Subtle prompt engineering is needed to improve ChatGPTP output.




1. [Cumulative Reasoning with Large Language Models](https://arxiv.org/abs/2308.04371v5), Yifan Zhang,Jingqin Yang,Yang Yuan,Andrew Chi-Chih Yao, 08-08-2023
      ### Categories
      Artificial Intelligence
     ### Abstract
     .
     None


1. [You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content](https://arxiv.org/abs/2308.05596v1), Xinlei He,Savvas Zannettou,Yun Shen,Yang Zhang, 10-08-2023
      ### Categories
      Computation and Language, Social and Information Networks
     ### Abstract
     The spread of toxic content online is an important problem that has adverse effects on user experience online and in our society at large. Motivated by the importance and impact of the problem, research focuses on developing solutions to detect toxic content, usually leveraging machine learning (ML) models trained on human-annotated datasets. While these efforts are important, these models usually do not generalize well and they can not cope with new trends (e.g., the emergence of new toxic terms). Currently, we are witnessing a shift in the approach to tackling societal issues online, particularly leveraging large language models (LLMs) like GPT-3 or T5 that are trained on vast corpora and have strong generalizability. In this work, we investigate how we can use LLMs and prompt learning to tackle the problem of toxic content, particularly focusing on three tasks; 1) Toxicity Classification, 2) Toxic Span Detection, and 3) Detoxification. We perform an extensive evaluation over five model architectures and eight datasets demonstrating that LLMs with prompt learning can achieve similar or even better performance compared to models trained on these specific tasks. We find that prompt learning achieves around 10\% improvement in the toxicity classification task compared to the baselines, while for the toxic span detection task we find better performance to the best baseline (0.643 vs. 0.640 in terms of $F_1$-score). Finally, for the detoxification task, we find that prompt learning can successfully reduce the average toxicity score (from 0.775 to 0.213) while preserving semantic meaning.
     ### Bullet Points

     * Toxic content spread online has negative effects on user experience and society

     * Research focuses on developing solutions to detect toxic content using machine learning models trained on human-annotated datasets

     * However, these models usually do not generalize well and cannot cope with new trends

     * Currently, we are seeing a shift in the approach to tackling societal issues online, particularly leveraging large language models like GPT-3 or T5 that are trained on vast corpora and have strong generalizability

     * LLMs with prompt learning can achieve similar or even better performance in the toxicity classification task, toxic span detection task, and detoxification task

     * Prompt learning achieves around 10% improvement in the toxic content classification task compared to the baselines, while toxic span detector task achieves better performance to the best baseline (0.643 vs

     * 0.640 in terms of $F_1$-score).

1. [Large Language Models and Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges](https://arxiv.org/abs/2308.06668v4), Jiajia Li,Mingle Xu,Lirong Xiang,Dong Chen,Weichao Zhuang,Xunyuan Yin,Zhaojian Li, 13-08-2023
      ### Categories
      Machine Learning, Computer Vision
     ### Abstract
     The past decade has witnessed the rapid development and adoption of ML & DL methodologies in agricultural systems, showcased by great successes in agricultural applications. However, these conventional ML/DL models have certain limitations: they heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, large pre-trained models, also known as FMs, have demonstrated remarkable successes in language, vision, and decision-making tasks across various domains. These models are trained on a large amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture AI. Thus, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, conceptual tools and technical background are presented to help the understanding of the problem space and uncover new research directions. To this end, recent FMs in the general CS domain are reviewed, and the models are categorized into four categories: language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs. Then, the steps of developing agriculture FMs (AFMs) are outlined and potential applications in smart agriculture are discussed. Moreover, challenges and risks associated with developing AFMs are discussed, including model training, validation, and deployment. In summary, the advancement of AI in agriculture is explored by introducing AFMs as a promising paradigm that can significantly mitigate the reliance on extensive labeled datasets and enhance the efficiency, effectiveness, and generalization of agricultural AI systems.
     ### Bullet Points

    * The study explores the potential of large pre-trained FMs (FMs) in agriculture AI and explores their effectiveness and potential applications in smart agriculture

    * It also discusses the challenges and risks associated with developing AFMs, including model training, validation, and deployment

    * The study emphasizes the importance of conceptual tools and technical background to help understand the problem space and uncover research directions.


1. [Platypus: Quick, Cheap, and Powerful Refinement of LLMs](https://arxiv.org/abs/2308.07317), Ariel N. Lee,Cole J. Hunter,Nataniel Ruiz, 14-08-2023
      ### Categories
      Computation and Language
     ### Abstract

1. [Large Language Models as Optimizers](https://arxiv.org/abs/2309.03409v2), Chengrun Yang,Xuezhi Wang,Yifeng Lu,Hanxiao Liu,Quoc V. Le,Denny Zhou,Xinyun Chen, 07-09-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language
     ### Abstract



1. [The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants](https://arxiv.org/abs/2308.16884), Lucas Bandarkar,Davis Liang,Benjamin Muller,Mikel Artetxe,Satya Narayan Shukla,Donald Husa,Naman Goyal,Abhinandan Krishnan,Luke Zettlemoyer,Madian Khabsa, 31-08-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning, Natural Language Processing
     ### Abstract
     We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. We also observe that larger vocabulary size and conscious vocabulary construction correlate with better performance on low-resource languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.
     ### Bullet Points

    * Belebele is a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants that expands the language coverage of natural language understanding (NLU) benchmarks

    * It enables the evaluation of text models in high-, medium-, and low-resource languages

    * Each question is based on a short passage from the Flores-200 dataset, and the questions were carefully curated to discriminate between models with different levels of general language comprehension

    * The dataset enables direct comparison of model performance across all languages

    * We use this dataset to evaluate the capabilities of multilingual masked language models and large language models, and find that much smaller MLMs pretrained on balanced multilingual data still understand far more languages despite significant cross-lingual transfer in English-centric LLMs

    * Larger vocabulary size and conscious vocabulary construction correlate with better performance on low resource language

    * This dataset opens up new avenues for



1. [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958), Stephanie Lin,Jacob Hilton,Owain Evans, 08-09-2021
      ### Categories
      Computation and Language, Artificial Intelligence, Computers and Society, Machine Learning
     ### Abstract
     We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.
     ### Bullet Points

    * A benchmark is proposed to measure whether a language model is truthful in generating answers to questions

    * The benchmark includes 817 questions that span 38 categories, including health, law, finance, and politics

    * To perform well, models must avoid generating false answers learned from imitating human texts

    * The best model was truthful on 58% of questions, while human performance was 94%

    * Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans

    * The largest models were generally the least truthful

    * Scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.



1. [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/abs/2309.04269v1), Griffin Adams,Alexander Fabbri,Faisal Ladhak,Eric Lehman,No√©mie Elhadad, 08-09-2023
      ### Categories
      Computation and Language
     ### Abstract
     ).
     None


1. [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463v1), Yuanzhi Li,S√©bastien Bubeck,Ronen Eldan,Allie Del Giorno,Suriya Gunasekar,Yin Tat Lee, 11-09-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     We continue the investigation into the power of smaller Transformer-based language models as initiated by \textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step" or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \textbf{phi-1.5} to promote further research on these urgent topics.
     ### Bullet Points

     * We are investigating the power of smaller Transformer-based language models, such as textbfTinyStories and the follow-up work on a 1.3 billion parameter model, Textbooks Are All You Need

     * The work proposes to use existing LLMs to generate "textbook quality" data to enhance the learning process compared to traditional web data

     * We follow the "Textbook Are All you Need" approach, focusing on common sense reasoning in natural language, and create a new 1.3 million parameter model named texbffphi-1.5, with performance on natural language tasks comparable to models 5x larger and surpassing most non-frontier LLM models on more complex reasoning tasks such as grade-school mathematics and basic coding

     * Despite the lack of web data, we are seeing improvement on this front thanks to the absence of website data, and we open-source

1. [Re-Reading Improves Reasoning in Large Language Models](https://arxiv.org/abs/2309.06275v2), Xiaohan Xu,Chongyang Tao,Tao Shen,Can Xu,Hongbo Xu,Guodong Long,Jian-guang Lou, 12-09-2023
      ### Categories
      Computation and Language


1. [Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL](https://arxiv.org/abs/2309.06553v3), Hao Sun,Alihan H√ºy√ºk,Mihaela van der Schaar, 13-09-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization. We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges that impede the successful and economical design of prompt optimization techniques. One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable. Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive. To address this, we introduce Prompt-OIRL, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data. Such data exists as by-products when diverse prompts are benchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent prompt optimization objective is achieved by first learning an offline reward model. This model can evaluate any query-prompt pairs without accessing LLMs. Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt. Our experimental evaluations across various LLM scales and arithmetic reasoning datasets underscore both the efficacy and economic viability of the proposed approach.
     ### Bullet Points

     * The study aims to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization by identifying a previously overlooked objective of query dependency

     * Two ensuing challenges that impede the successful and economical design of prompt optimization techniques are the lack of an effective method to evaluate prompts during inference when the golden answer is unavailable and resource-intensive learning via interactions with the LLMs to navigate the vast natural language prompting space

     * Prompt-OIRL harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data, resulting in a query-dependent prompt optimization objective achieved by first learning an offline reward model

     * The best-of-N strategy is deployed to recommend the optimal prompt

     * The experimental evaluations demonstrate the efficacy and economic viability of the proposed approach.




1. [Agents: An Open-source Framework for Autonomous Language Agents](https://arxiv.org/abs/2309.07870v3), Wangchunshu Zhou,Yuchen Eleanor Jiang,Long Li,Jialong Wu,Tiannan Wang,Shi Qiu,Jintian Zhang,Jing Chen,Ruipu Wu,Shuai Wang,Shiding Zhu,Jiyu Chen,Wentao Zhang,Xiangru Tang,Ningyu Zhang,Huajun Chen,Peng Cui,Mrinmaya Sachan, 14-09-2023
      ### Categories
      Computation and Language
     ### Abstract
     .
     None


1. [Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts](https://arxiv.org/abs/2309.07430v3), Dave Van Veen,Cara Van Uden,Louis Blankemeier,Jean-Benoit Delbrouck,Asad Aali,Christian Bluethgen,Anuj Pareek,Malgorzata Polacin,Eduardo Pontes Reis,Anna Seehofnerova,Nidhi Rohatgi,Poonam Hosamani,William Collins,Neera Ahuja,Curtis P. Langlotz,Jason Hom,Sergios Gatidis,John Pauly,Akshay S. Chaudhari, 14-09-2023
      ### Categories
      Computation and Language
     ### Abstract
     Sifting through vast textual data and summarizing key information from electronic health records (EHR) imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy on a diverse range of clinical summarization tasks has not yet been rigorously demonstrated. In this work, we apply domain adaptation methods to eight LLMs, spanning six datasets and four distinct clinical summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not improve results. Further, in a clinical reader study with ten physicians, we show that summaries from our best-adapted LLMs are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis highlights challenges faced by both LLMs and human experts. Lastly, we correlate traditional quantitative NLP metrics with reader study scores to enhance our understanding of how these metrics align with physician preferences. Our research marks the first evidence of LLMs outperforming human experts in clinical text summarization across multiple tasks. This implies that integrating LLMs into clinical workflows could alleviate documentation burden, empowering clinicians to focus more on personalized patient care and the inherently human aspects of medicine.
     ### Bullet Points

     * The study demonstrates that large language models (LLMs) have shown promise in natural language processing (NLP) tasks, but their efficacy on a diverse range of clinical summarization tasks has not been rigorously demonstrated

     * The study applies domain adaptation methods to eight LLMs, spanning six datasets and four clinical summaries: radiology reports, patient questions, progress notes, and doctor-patient dialogue

     * The quantitative assessment reveals trade-offs between models and adaptation methods, as well as instances where recent advances in LLM may not improve results

     * In a clinical reader study with ten physicians, LLM summary sums from our best-adapted LLM are preferable to human summary in terms of completeness and correctness

     * Our qualitative analysis highlights challenges faced by both LLM and human experts, and we correlate traditional quantitative NLP metrics with reader study scores to enhance our understanding of how these metrics align with physician preferences.




1. [The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/abs/2309.07864v3), Zhiheng Xi,Wenxiang Chen,Xin Guo,Wei He,Yiwen Ding,Boyang Hong,Ming Zhang,Junzhe Wang,Senjie Jin,Enyu Zhou,Rui Zheng,Xiaoran Fan,Xiao Wang,Limao Xiong,Yuhao Zhou,Weiran Wang,Changhao Jiang,Yicheng Zou,Xiangyang Liu,Zhangyue Yin,Shihan Dou,Rongxiang Weng,Wensen Cheng,Qi Zhang,Wenjuan Qin,Yongyan Zheng,Xipeng Qiu,Xuanjing Huang,Tao Gui, 14-09-2023
      ### Categories
      Artificial Intelligence, Computation and Language
     ### Abstract
     .
     None


1. [Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers](https://arxiv.org/abs/2309.08532v1), Qingyan Guo,Rui Wang,Junliang Guo,Bei Li,Kaitao Song,Xu Tan,Guoqing Liu,Jiang Bian,Yujiu Yang, 15-09-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 9 datasets spanning language understanding and generation tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation by up to 25% and 14% respectively. Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.
     ### Bullet Points

     * The paper proposes a framework for discrete prompt optimization called EvoPrompt that uses evolutionary algorithms to automate the process of generating natural language expressions that require human effort

     * The approach involves connecting LLMs with EAs to generate new prompts based on the evolutionary operators, which outperforms human-engineered prompts and existing methods for automatic prompt generation by up to 25% and 14% respectively

     * The synergies between the two methods could inspire further research on the combination of LMLs and conventional algorithms.




1. [PDFTriage: Question Answering over Long, Structured Documents](https://arxiv.org/abs/2309.08872v2), Jon Saad-Falcon,Joe Barrow,Alexa Siu,Ani Nenkova,David Seunghyun Yoon,Ryan A. Rossi,Franck Dernoncourt, 16-09-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmented models across several classes of questions where existing retrieval-augmented LLMs fail. To facilitate further research on this fundamental problem, we release our benchmark dataset consisting of 900+ human-generated questions over 80 structured documents from 10 different categories of question types for document QA. Our code and datasets will be released soon on Github.
     ### Bullet Points

     * Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM

     * To overcome this issue, most existing works focus on retrieving the relevant context from the document

     * However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on

     * Representing structured documents as plain text is incongruous with the user's mental model of these documents with rich structure

     * When a system has to query the document for context, seemingly trivial questions can trip up the QA system

     * To bridge this gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content

     * Our experiments demonstrate the effectiveness of this approach across several classes of questions where existing retrieval-augmented LLMs fail

     * To further research on this fundamental problem




1. [OWL: A Large Language Model for IT Operations](https://arxiv.org/abs/2309.09298v1), Hongcheng Guo,Jian Yang,Jiaheng Liu,Liqun Yang,Linzheng Chai,Jiaqi Bai,Junran Peng,Xiaorong Hu,Chao Chen,Dongfeng Zhang,Xu Shi,Tieqiao Zheng,Liangfan Zheng,Bo Zhang,Ke Xu,Zhoujun Li, 17-09-2023
      ### Categories
      Computation and Language
     ### Abstract
     With the rapid development of IT operations, it has become increasingly crucial to efficiently manage and analyze large volumes of data for practical applications. The techniques of Natural Language Processing (NLP) have shown remarkable capabilities for various tasks, including named entity recognition, machine translation and dialogue systems. Recently, Large Language Models (LLMs) have achieved significant improvements across various NLP downstream tasks. However, there is a lack of specialized LLMs for IT operations. In this paper, we introduce the OWL, a large language model trained on our collected OWL-Instruct dataset with a wide range of IT-related information, where the mixture-of-adapter strategy is proposed to improve the parameter-efficient tuning across different domains or tasks. Furthermore, we evaluate the performance of our OWL on the OWL-Bench established by us and open IT-related benchmarks. OWL demonstrates superior performance results on IT tasks, which outperforms existing models by significant margins. Moreover, we hope that the findings of our work will provide more insights to revolutionize the techniques of IT operations with specialized LLMs.
     ### Bullet Points

     * The paper introduces OWL, a large language model trained on the OWL-Instruct dataset with a wide range of IT-related information, to improve parameter-efficient tuning across different domains or tasks

     * The OWL demonstrates superior performance results on IT tasks and outperforms existing models by significant margins

     * The findings will provide more insights into the techniques of IT operations with specialized LLMs.




1. [Investigating Zero- and Few-shot Generalization in Fact Verification](https://arxiv.org/abs/2309.09444v1), Liangming Pan,Yunxiang Zhang,Min-Yen Kan, 18-09-2023
      ### Categories
      Computation and Language
     ### Abstract
     In this paper, we explore zero- and few-shot generalization for fact verification (FV), which aims to generalize the FV model trained on well-resourced domains (e.g., Wikipedia) to low-resourced domains that lack human annotations. To this end, we first construct a benchmark dataset collection which contains 11 FV datasets representing 6 domains. We conduct an empirical analysis of generalization across these FV datasets, finding that current models generalize poorly. Our analysis reveals that several factors affect generalization, including dataset size, length of evidence, and the type of claims. Finally, we show that two directions of work improve generalization: 1) incorporating domain knowledge via pretraining on specialized domains, and 2) automatically generating training data via claim generation.
     ### Bullet Points

     * The paper explores zero- and few-shot generalization for fact verification (FV) to generalize the model trained on well-resourced domains to low-resource domains that lack human annotations

     * We construct a benchmark dataset collection of 11 FV datasets representing 6 domains, conduct an empirical analysis of generalization across these datasets, and find that current models generalize poorly

     * Factors that affect generalization include dataset size, length of evidence, and type of claims

     * Two directions of work improve generalization: 1) incorporating domain knowledge via pretraining on specialized domains and 2) automatically generating training data via claim generation.




1. [LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models](https://arxiv.org/abs/2309.09708v2), Nan Li,Bo Kang,Tijl De Bie, 18-09-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Automated occupation extraction and standardization from free-text job postings and resumes are crucial for applications like job recommendation and labor market policy formation. This paper introduces LLM4Jobs, a novel unsupervised methodology that taps into the capabilities of large language models (LLMs) for occupation coding. LLM4Jobs uniquely harnesses both the natural language understanding and generation capacities of LLMs. Evaluated on rigorous experimentation on synthetic and real-world datasets, we demonstrate that LLM4Jobs consistently surpasses unsupervised state-of-the-art benchmarks, demonstrating its versatility across diverse datasets and granularities. As a side result of our work, we present both synthetic and real-world datasets, which may be instrumental for subsequent research in this domain. Overall, this investigation highlights the promise of contemporary LLMs for the intricate task of occupation extraction and standardization, laying the foundation for a robust and adaptable framework relevant to both research and industrial contexts.
     ### Bullet Points

     * The paper introduces LLM4Jobs, an unsupervised methodology for occupation extraction and standardization from free-text job postings and resumes

     * It harnesses natural language understanding and generation capacities of LLMs and consistently surpasses unsupervised state-of-the-art benchmarks, demonstrating its versatility across diverse datasets and granularities

     * The paper presents both synthetic and real-world datasets, which may be useful for subsequent research in this domain.




1. [MindAgent: Emergent Gaming Interaction](https://arxiv.org/abs/2309.09971v2), Ran Gong,Qiuyuan Huang,Xiaojian Ma,Hoi Vo,Zane Durante,Yusuke Noda,Zilong Zheng,Song-Chun Zhu,Demetri Terzopoulos,Li Fei-Fei,Jianfeng Gao, 18-09-2023
      ### Categories
      Artificial Intelligence, Human-Computer Interaction, Multiagent Systems
     ### Abstract
     Large Language Models (LLMs) have the capacity of performing complex scheduling in a multi-agent system and can coordinate these agents into completing sophisticated tasks that require extensive collaboration. However, despite the introduction of numerous gaming frameworks, the community has insufficient benchmarks towards building general multi-agents collaboration infrastructure that encompass both LLM and human-NPCs collaborations. In this work, we propose a novel infrastructure - MindAgent - to evaluate planning and coordination emergent capabilities for gaming interaction. In particular, our infrastructure leverages existing gaming framework, to i) require understanding of the coordinator for a multi-agent system, ii) collaborate with human players via un-finetuned proper instructions, and iii) establish an in-context learning on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new gaming scenario and related benchmark that dispatch a multi-agent collaboration efficiency and supervise multiple agents playing the game simultaneously. We conduct comprehensive evaluations with new auto-metric CoS for calculating the collaboration efficiency. Finally, our infrastructure can be deployed into real-world gaming scenarios in a customized VR version of CUISINEWORLD and adapted in existing broader Minecraft gaming domain. We hope our findings on LLMs and the new infrastructure for general-purpose scheduling and coordination can help shed light on how such skills can be obtained by learning from large language corpora.
     ### Bullet Points

     * The study proposes MindAgent, an existing gaming framework, to evaluate planning and coordination emergent capabilities for gaming interaction

     * It leverages existing frameworks to require understanding of the coordinator for a multi-agent system, collaborate with human players via un-finetuned proper instructions, and establish an in-context learning on few-shot prompt with feedback

     * Additionally, we introduce CUISINEWORLD, a new gaming scenario and benchmark that dispatches a Multi-Agent Collaboration Efficiency and supervises multiple agents playing the game simultaneously

     * These infrastructure can be deployed into real-world gaming scenarios in a customized VR version of CUisINEWORLD and adapted in existing broader Minecraft gaming domain

     * Our findings on LLMs and the new infrastructure for general-purpose scheduling and coordination can help shed light on how such skills can be obtained by learning from large language corpora.




1. [PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models](https://arxiv.org/abs/2309.10238v1), Chenhao Tang,Zhengliang Liu,Chong Ma,Zihao Wu,Yiwei Li,Wei Liu,Dajiang Zhu,Quanzheng Li,Xiang Li,Tianming Liu,Lei Fan, 19-09-2023
      ### Categories
      Computation and Language
     ### Abstract
     Privacy policies serve as the primary conduit through which online service providers inform users about their data collection and usage procedures. However, in a bid to be comprehensive and mitigate legal risks, these policy documents are often quite verbose. In practical use, users tend to click the Agree button directly rather than reading them carefully. This practice exposes users to risks of privacy leakage and legal issues. Recently, the advent of Large Language Models (LLM) such as ChatGPT and GPT-4 has opened new possibilities for text analysis, especially for lengthy documents like privacy policies. In this study, we investigate a privacy policy text analysis framework PolicyGPT based on the LLM. This framework was tested using two datasets. The first dataset comprises of privacy policies from 115 websites, which were meticulously annotated by legal experts, categorizing each segment into one of 10 classes. The second dataset consists of privacy policies from 304 popular mobile applications, with each sentence manually annotated and classified into one of another 10 categories. Under zero-shot learning conditions, PolicyGPT demonstrated robust performance. For the first dataset, it achieved an accuracy rate of 97%, while for the second dataset, it attained an 87% accuracy rate, surpassing that of the baseline machine learning and neural network models.
     ### Bullet Points

     * PolicyGPT, a privacy policy text analysis framework based on LLM, was tested using two datasets

     * The first dataset consisted of privacy policies from 115 websites, which were meticulously annotated by legal experts, categorizing each segment into one of 10 classes

     * The second dataset consists of 304 popular mobile applications, with each sentence manually classified into one another 10 categories

     * The framework demonstrated robust performance under zero-shot learning conditions and achieved an accuracy rate of 97%, while the second dataset achieved an 87% accuracy rate surpassing that of the baseline machine learning and neural network models.




1. [Prompt, Condition, and Generate: Classification of Unsupported Claims with In-Context Learning](https://arxiv.org/abs/2309.10359v1), Peter Ebert Christensen,Srishti Yadav,Serge Belongie, 19-09-2023
      ### Categories
      Computation and Language
     ### Abstract
     Unsupported and unfalsifiable claims we encounter in our daily lives can influence our view of the world. Characterizing, summarizing, and -- more generally -- making sense of such claims, however, can be challenging. In this work, we focus on fine-grained debate topics and formulate a new task of distilling, from such claims, a countable set of narratives. We present a crowdsourced dataset of 12 controversial topics, comprising more than 120k arguments, claims, and comments from heterogeneous sources, each annotated with a narrative label. We further investigate how large language models (LLMs) can be used to synthesise claims using In-Context Learning. We find that generated claims with supported evidence can be used to improve the performance of narrative classification models and, additionally, that the same model can infer the stance and aspect using a few training examples. Such a model can be useful in applications which rely on narratives , e.g. fact-checking.
     ### Bullet Points

     * The work focuses on fine-grained debate topics and distills a countable set of narratives from these claims

     * The dataset includes 12 controversial topics with over 120k arguments, claims, and comments from heterogeneous sources, each annotated with a narrative label

     * Large language models (LLMs) can be used to synthesise claims using In-Context Learning

     * Generated claims with supported evidence can improve the performance of narrative classification models and can infer the stance and aspect using a few training examples

     * This model can be useful in applications that rely on narratives, such as fact-checking.




1. [Chain-of-Verification Reduces Hallucination in Large Language Models](https://arxiv.org/abs/2309.11495v2), Shehzaad Dhuliawala,Mojtaba Komeili,Jing Xu,Roberta Raileanu,Xian Li,Asli Celikyilmaz,Jason Weston, 20-09-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.
     ### Bullet Points

     * The issue of hallucination in large language models is unsolved

     * We develop the Chain-of-Verification (CoVe) method, which involves the model drafting an initial response, planning verification questions to fact-check its draft, answering those questions independently so the answers are not biased by other responses, and generating its final verified response

     * In experiments, CoVe decreases hallucinos across tasks such as Wikidata, closed book MultiSpanQA, and longform text generation.


   
1. [Large Language Model Alignment: A Survey](https://arxiv.org/abs/2309.15025v1), Tianhao Shen,Renren Jin,Yufei Huang,Chuang Liu,Weilong Dong,Zishan Guo,Xinwei Wu,Yan Liu,Deyi Xiong, 26-09-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Our aspiration for this survey extends beyond merely spurring research interests in this realm. We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs.
     ### Bullet Points

     * The survey seeks to bridge the gap between the AI alignment research community and the researchers engrossed in LLM capability exploration for both capable and safe LLMs, beyond just spurring research interests in this field

     * The aim is to provide a more comprehensive understanding of LLM capabilities and their capabilities.




1. [AutoAgents: A Framework for Automatic Agent Generation](https://arxiv.org/abs/2309.17288v2), Guangyao Chen,Siwei Dong,Yu Shu,Ge Zhang,Jaward Sesay,B√∂rje F. Karlsson,Jie Fu,Yemin Shi, 29-09-2023
      ### Categories
      Artificial Intelligence
     ### Abstract
     .
     None


1. [SmartPlay: A Benchmark for LLMs as Intelligent Agents](https://arxiv.org/abs/2310.01557v3), Yue Wu,Xuan Tang,Tom M. Mitchell,Yuanzhi Li, 02-10-2023
      ### Categories
      Machine Learning, Artificial Intelligence
     ### Abstract
     


1. [UltraFeedback: Boosting Language Models with High-quality Feedback](https://arxiv.org/abs/2310.01377), Ganqu Cui,Lifan Yuan,Ning Ding,Guanming Yao,Wei Zhu,Yuan Ni,Guotong Xie,Zhiyuan Liu,Maosong Sun, 02-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract



1. [Can large language models provide useful feedback on research papers? A large-scale empirical analysis](https://arxiv.org/abs/2310.01783), Weixin Liang,Yuhui Zhang,Hancheng Cao,Binglu Wang,Daisy Ding,Xinyu Yang,Kailas Vodrahalli,Siyu He,Daniel Smith,Yian Yin,Daniel McFarland,James Zou, 03-10-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language, Human-Computer Interaction
     ### Abstract
     Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.
     ### Bullet Points

    * The article discusses the challenges of using large language models (LLMs) to generate scientific feedback on research manuscripts

    * It explains that the utility of LLM-generated feedback has not been systematically studied, and researchers who are junior or from under-resourced settings have difficulty getting timely feedback

    * To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers

    * We evaluated the quality of the feedback through two large-scale studies and conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT 4 system on their own papers

    * More than half of the users found it helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers

    * However, we also identify several limitations.



1. [Can large language models provide useful feedback on research papers? A large-scale empirical analysis](https://arxiv.org/abs/2310.01783v1), Weixin Liang,Yuhui Zhang,Hancheng Cao,Binglu Wang,Daisy Ding,Xinyu Yang,Kailas Vodrahalli,Siyu He,Daniel Smith,Yian Yin,Daniel McFarland,James Zou, 03-10-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language, Human-Computer Interaction
     ### Abstract
     Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.
     ### Bullet Points

     * The article discusses the challenges of using large language models (LLMs) to generate scientific feedback on research manuscripts

     * It explains that the utility of LLM-generated feedback has not been systematically studied, and researchers who are junior or from under-resourced settings have difficulty getting timely feedback

     * To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers

     * We evaluated the quality of the feedback through two large-scale studies and conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT 4 system on their own papers

     * More than half of the users found it helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers

     * However, we also identify several limitations.




1. [Conversational Health Agents: A Personalized LLM-Powered Agent Framework](https://arxiv.org/abs/2310.02374v3), Mahyar Abbasian,Iman Azimi,Amir M. Rahmani,Ramesh Jain, 03-10-2023
      ### Categories
      Computation and Language
     ### Abstract
     Conversational Health Agents (CHAs) are interactive systems that provide healthcare services, such as assistance, self-awareness, and diagnosis. Current CHAs, especially those utilizing Large Language Models (LLMs), primarily focus on conversation aspects. However, they offer limited agent capabilities specifically lacking multi-step problem-solving, empathetic conversations, and multimodal data analysis. Our aim is to overcome these limitations. In this paper, we propose an LLM-powered framework to empower CHAs to generate a personalized response for users' healthcare queries. This framework provides critical thinking, knowledge acquisition, and problem-solving abilities by integrating healthcare data sources, enabling multilingual and multimodal conversations, and interacting with various user data analysis tools. We illustrate the framework's proficiency in handling complex healthcare tasks via a case study on stress level estimation, showcasing the agent's cognitive and operational capabilities. Powered by our framework, the CHA can provide appropriate responses, when the user inquires about their stress level. To achieve this, it learns to collect photoplethysmogram signals, converts them into heart rate variability, and interprets them as indicators of stress levels.
     ### Bullet Points

     * The paper proposes an LLM-powered framework to empower Conversational Health Agents (CHAs) to generate a personalized response for users' healthcare queries

     * The framework provides critical thinking, knowledge acquisition, and problem-solving abilities by integrating healthcare data sources, enabling multilingual and multimodal conversations, and interacting with various user data analysis tools

     * The CHA can provide appropriate responses when the user inquires about their stress level by collecting photoplethysmogram signals, converting them into heart rate variability, and interpreting them as indicators of stress levels.




1. [EcoAssistant: Using LLM Assistant More Affordably and Accurately](https://arxiv.org/abs/2310.03046v1), Jieyu Zhang,Ranjay Krishna,Ahmed H. Awadallah,Chi Wang, 03-10-2023
      ### Categories
      Software Engineering, Artificial Intelligence
     ### Abstract
     Today, users ask Large language models (LLMs) as assistants to answer queries that require external knowledge; they ask about the weather in a specific city, about stock prices, and even about where specific locations are within their neighborhood. These queries require the LLM to produce code that invokes external APIs to answer the user's question, yet LLMs rarely produce correct code on the first try, requiring iterative code refinement upon execution results. In addition, using LLM assistants to support high query volumes can be expensive. In this work, we contribute a framework, EcoAssistant, that enables LLMs to answer code-driven queries more affordably and accurately. EcoAssistant contains three components. First, it allows the LLM assistants to converse with an automatic code executor to iteratively refine code or to produce answers based on the execution results. Second, we use a hierarchy of LLM assistants, which attempts to answer the query with weaker, cheaper LLMs before backing off to stronger, expensive ones. Third, we retrieve solutions from past successful queries as in-context demonstrations to help subsequent queries. Empirically, we show that EcoAssistant offers distinct advantages for affordability and accuracy, surpassing GPT-4 by 10 points of success rate with less than 50% of GPT-4's cost.
     ### Bullet Points

     * EcoAssistant is a framework that enables LLMs to answer code-driven queries more affordably and accurately, with three components: converse with an automatic code executor, use a hierarchy of LLM assistants, retrieve solutions from past successful queries as in-context demonstrations, and surpass GPT-4 by 10 points of success rate with less than 50% of its cost.




1. [Large Language Models Cannot Self-Correct Reasoning Yet](https://arxiv.org/abs/2310.01798v1), Jie Huang,Xinyun Chen,Swaroop Mishra,Huaixiu Steven Zheng,Adams Wei Yu,Xinying Song,Denny Zhou, 03-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.
     ### Bullet Points

     * The paper examines the role and efficacy of self-correction within LLMs, shedding light on its potential and limitations

     * The paper focuses on intrinsic self correction, where an LLM attempts to correct its initial responses based solely on its inherent capabilities, without external feedback

     * The research suggests that LLM's performance may degrade post self correction and suggests future research and practical applications in this field.




1. [How FaR Are Large Language Models From Agents with Theory-of-Mind?](https://arxiv.org/abs/2310.03051v1), Pei Zhou,Aman Madaan,Srividya Pranavi Potharaju,Aditya Gupta,Kevin R. McKee,Ari Holtzman,Jay Pujara,Xiang Ren,Swaroop Mishra,Aida Nematzadeh,Shyam Upadhyay,Manaal Faruqui, 04-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     "Thinking is for Doing." Humans can infer other people's mental states from observations--an ability called Theory-of-Mind (ToM)--and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as ToMi ask models questions to make inferences about beliefs of characters in a story, but do not test whether models can then use these inferences to guide their actions. We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action. Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions. FaR boosts GPT-4's performance from 50% to 71% on T4D, outperforming other prompting methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures and scenarios that also require ToM inferences to choose an action, consistently outperforming other methods including few-shot in-context learning.
     ### Bullet Points

     * The article proposes a new evaluation paradigm for large language models (LLMs) called Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios

     * LLMs such as GPT-4 and PaLM 2 excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action

     * To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FAR), which provides a reasoning structure that encourages LLLs to anticipate future challenges and reason about potential actions

     * FaR boosts GPT 4's performance from 50% to 71% on T4D, outperforming other prompting methods such as Chain-of-Thought and Self-Ask.



1. [FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation](https://arxiv.org/abs/2310.03214), Tu Vu,Mohit Iyyer,Xuezhi Wang,Noah Constant,Jerry Wei,Jason Wei,Chris Tar,Yun-Hsuan Sung,Denny Zhou,Quoc Le,Thang Luong, 05-10-2023
      ### Categories
      Computation and Language
     ### Abstract



1. [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714), Omar Khattab,Arnav Singhvi,Paridhi Maheshwari,Zhiyuan Zhang,Keshav Santhanam,Sri Vardhamanan,Saiful Haq,Ashutosh Sharma,Thomas T. Joshi,Hanna Moazam,Heather Miller,Matei Zaharia,Christopher Potts, 05-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Information Retrieval, Machine Learning
     ### Abstract




1. [Agent Instructs Large Language Models to be General Zero-Shot Reasoners](https://arxiv.org/abs/2310.03710v1), Nicholas Crispino,Kyle Montgomery,Fankun Zeng,Dawn Song,Chenguang Wang, 05-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
     ### Bullet Points

     * We developed a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks by building an autonomous agent to instruct the reasoning process

     * Our method generalizes to most tasks and obtained state-of-the-art zero-shoot performance on 20 of the 29 datasets we evaluated

     * Our improvement in reasoning is striking, with an average increase of 10.5%

     * Llama-2-70b-chat outperforms GPT-3.5 Turbo by 10.2%.




1. [FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation](https://arxiv.org/abs/2310.03214v2), Tu Vu,Mohit Iyyer,Xuezhi Wang,Noah Constant,Jerry Wei,Jason Wei,Chris Tar,Yun-Hsuan Sung,Denny Zhou,Quoc Le,Thang Luong, 05-10-2023
      ### Categories
      Computation and Language
     ### Abstract
     and commit to updating it at regular intervals.
     None


1. [Large Language Models for Software Engineering: Survey and Open Problems](https://arxiv.org/abs/2310.03533v4), Angela Fan,Beliz Gokkaya,Mark Harman,Mitya Lyubarskiy,Shubho Sengupta,Shin Yoo,Jie M. Zhang, 05-10-2023
      ### Categories
      Software Engineering
     ### Abstract
     This paper provides a survey of the emerging area of Large Language Models (LLMs) for Software Engineering (SE). It also sets out open research challenges for the application of LLMs to technical problems faced by software engineers. LLMs' emergent properties bring novelty and creativity with applications right across the spectrum of Software Engineering activities including coding, design, requirements, repair, refactoring, performance improvement, documentation and analytics. However, these very same emergent properties also pose significant technical challenges; we need techniques that can reliably weed out incorrect solutions, such as hallucinations. Our survey reveals the pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in the development and deployment of reliable, efficient and effective LLM-based SE.
     ### Bullet Points

     * The paper provides a survey on the emerging area of Large Language Models (LLMs) for Software Engineering (SE) and presents open research challenges for the application of LLMs to technical problems faced by software engineers

     * The emerging properties bring novelty and creativity, but they also pose significant technical challenges

     * Hybrid techniques, such as SE and LLM, need to play a crucial role in the development and deployment of reliable, efficient, and effective LLM-based SE.

   

1. [Generative Judge for Evaluating Alignment](https://arxiv.org/abs/2310.05470), Junlong Li,Shichao Sun,Weizhe Yuan,Run-Ze Fan,Hai Zhao,Pengfei Liu, 09-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract


1. [Compressing Context to Enhance Inference Efficiency of Large Language Models](https://arxiv.org/abs/2310.06201v1), Yucheng Li,Bo Dong,Chenghua Lin,Frank Guerin, 09-10-2023
      ### Categories
      Computation and Language
     ### Abstract
     Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50\% reduction in context cost, resulting in a 36\% reduction in inference memory usage and a 32\% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.
     ### Bullet Points

     * The paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact

     * This method is tested using common data sources such as arXiv papers, news articles, and long conversations on tasks such as summarisation, question answering, and response generation

     * Experimental results show that the method significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used

     * We achieve a 50% reduction in context cost, resulting in a 36% reduction in inference memory usage and a 32% reduced inference time, while observing only a minor drop in BERTscore and .038 in faithfulness on four downstream applications.




1. [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://arxiv.org/abs/2310.05736v2), Huiqiang Jiang,Qianhui Wu,Chin-Yew Lin,Yuqing Yang,Lili Qiu, 09-10-2023
      ### Categories
      Computation and Language, Machine Learning
     ### Abstract
     .
     None


1. [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/abs/2310.06117v1), Huaixiu Steven Zheng,Swaroop Mishra,Xinyun Chen,Heng-Tze Cheng,Ed H. Chi,Quoc V Le,Denny Zhou, 09-10-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language
     ### Abstract
     We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
     ### Bullet Points

     * Step-Back Prompting is a simple prompting technique that allows LLMs to derive high-level concepts and first principles from specific details

     * This technique significantly improves their abilities in following a correct reasoning path towards a solution

     * We conducted experiments with PaLM-2L models and observed significant performance gains on challenging reasoning-intensive tasks such as STEM, Knowledge QA, and Multi-Hop Reasoning.

1. [Humanoid Agents: Platform for Simulating Human-like Generative Agents](https://arxiv.org/abs/2310.05418v1), Zhilin Wang,Yu Ying Chiu,Yu Cheung Chiu, 09-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Human-Computer Interaction




1. [Beyond Memorization: Violating Privacy Via Inference with Large Language Models](https://arxiv.org/abs/2310.07298v1), Robin Staab,Mark Vero,Mislav Balunoviƒá,Martin Vechev, 11-10-2023
      ### Categories
      Artificial Intelligence, Machine Learning, Natural Language Processing
     ### Abstract
     Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for a wider privacy protection.
     ### Bullet Points

     * Current privacy research on large language models (LLMs) focuses on extracting memorized training data and increasing inference capabilities

     * This raises the question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time

     * We present the first comprehensive study on the capabilities of pretrained LLM models to infer personal attributes

     * We explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions

     * Common mitigations, such as text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference

     * In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for wider privacy protection.




1. [Exploring the Landscape of Large Language Models In Medical Question Answering: Observations and Open Questions](https://arxiv.org/abs/2310.07225v1), Karolina Korgul,Andrew M. Bean,Felix Krones,Robert McCraith,Adam Mahdi, 11-10-2023
      ### Categories
      Computation and Language
     ### Abstract
     Large Language Models (LLMs) have shown promise in medical question answering by achieving passing scores in standardised exams and have been suggested as tools for supporting healthcare workers. Deploying LLMs into such a high-risk context requires a clear understanding of the limitations of these models. With the rapid development and release of new LLMs, it is especially valuable to identify patterns which exist across models and may, therefore, continue to appear in newer versions. In this paper, we evaluate a wide range of popular LLMs on their knowledge of medical questions in order to better understand their properties as a group. From this comparison, we provide preliminary observations and raise open questions for further research.
     ### Bullet Points

      * Large Language Models (LLMs) have shown promise in medical question answering by achieving passing scores in standardised exams

      * They are suggested as tools for supporting healthcare workers

      * Deploying LLMs into a high-risk context requires a clear understanding of the limitations of these models

      * It is especially valuable to identify patterns that exist across models and may continue to appear in newer versions

     * Preliminary observations and open questions for further research are provided.




1. [Large Language Models Are Zero-Shot Time Series Forecasters](https://arxiv.org/abs/2310.07820v1), Nate Gruver,Marc Finzi,Shikai Qiu,Andrew Gordon Wilson, 11-10-2023
      ### Categories
      Machine Learning
     ### Abstract
     By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.
     ### Bullet Points

     * Encoding time series as a string of numerical digits can be used to frame forecasting as next-token prediction in text

     * Large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on downstream tasks

     * To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values

     * LLMs can naturally represent multimodal distributions, in conjunction with biases for simplicity, repetition, which align with salient features in many time series, and can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions

     * Increasing model size generally improves performance on time series

     * GPT-4 can perform worse




1. [Prometheus: Inducing Fine-grained Evaluation Capability in Language Models](https://arxiv.org/abs/2310.08491), Seungone Kim,Jamin Shin,Yejin Cho,Joel Jang,Shayne Longpre,Hwaran Lee,Sangdoo Yun,Seongjin Shin,Sungdong Kim,James Thorne,Minjoon Seo, 12-10-2023
      ### Categories
      Computation and Language, Machine Learning
     ### Abstract


1. [Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams](https://arxiv.org/abs/2310.08678v1), Ethan Callanan,Amarachi Mbakwe,Antony Papadimitriou,Yulong Pei,Mathieu Sibue,Xiaodan Zhu,Zhiqiang Ma,Xiaomo Liu,Sameena Shah, 12-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence, General Finance
     ### Abstract
     Large Language Models (LLMs) have demonstrated remarkable performance on a wide range of Natural Language Processing (NLP) tasks, often matching or even beating state-of-the-art task-specific models. This study aims at assessing the financial reasoning capabilities of LLMs. We leverage mock exam questions of the Chartered Financial Analyst (CFA) Program to conduct a comprehensive evaluation of ChatGPT and GPT-4 in financial analysis, considering Zero-Shot (ZS), Chain-of-Thought (CoT), and Few-Shot (FS) scenarios. We present an in-depth analysis of the models' performance and limitations, and estimate whether they would have a chance at passing the CFA exams. Finally, we outline insights into potential strategies and improvements to enhance the applicability of LLMs in finance. In this perspective, we hope this work paves the way for future studies to continue enhancing LLMs for financial reasoning through rigorous evaluation.
     ### Bullet Points

     * The study aims to assess the financial reasoning capabilities of LLMs by conducting a comprehensive evaluation of ChatGPT and GPT-4 in financial analysis, considering scenarios such as Zero-Shot (ZS), Chain-of-Thought (CoT), and Few-Sshot (FS)

     * We analyze the models' performance and limitations, estimate their chances of passing the CFA exams, and outline potential strategies and improvements to enhance their applicability in finance

     * This work paves the way for future studies to continue enhancing their financial reasoning through rigorous evaluation.




1. [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659v4), Yixiao Li,Yifan Yu,Chen Liang,Pengcheng He,Nikos Karampatziakis,Weizhu Chen,Tuo Zhao, 12-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     .
     None


1. [MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning](https://arxiv.org/abs/2310.09478v3), Jun Chen,Deyao Zhu,Xiaoqian Shen,Xiang Li,Zechun Liu,Pengchuan Zhang,Raghuraman Krishnamoorthi,Vikas Chandra,Yunyang Xiong,Mohamed Elhoseiny, 14-10-2023
      ### Categories
      Computer Vision
     ### Abstract



1. [Character-LLM: A Trainable Agent for Role-Playing](https://arxiv.org/abs/2310.10158), Yunfan Shao,Linyang Li,Junqi Dai,Xipeng Qiu, 16-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents \textit{memorize} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.
     ### Bullet Points

    * LLMs can be used to simulate human behaviors by training agents with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API

    * We introduce Character-LLM, a method that focuses on editing profiles as experiences and training models to be personal simulacra with these experiences

    * To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents textitmemorize their characters and experiences

    * Experimental results show interesting observations that help build future simulativity of humankind.


1. [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/abs/2310.11511v1), Akari Asai,Zeqiu Wu,Yizhong Wang,Avirup Sil,Hannaneh Hajishirzi, 17-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.
     ### Bullet Points

     * Self-Reflective Retrieval-Augmented Generation (Self-RAG) is a new framework that enhances LLMs' quality and factuality through retrieval and self-reflection

     * It trains a single arbitrary LM that adaptively retrieves passages on-demand and generates and reflects on retrieved passages and its own generations using reflection tokens, making it controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements

     * This framework outperforms state-of-the-art LLM models and retrieval-augmented models on a diverse set of tasks, and shows significant gains in improving factual and citation accuracy for long-form generations relative to these models.




1. [Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning](https://arxiv.org/abs/2310.11716v1), Ming Li,Lichang Chen,Jiuhai Chen,Shwai He,Heng Huang,Jiuxiang Gu,Tianyi Zhou, 18-10-2023
      ### Categories
      Computation and Language
     ### Abstract
     Recent advancements in Large Language Models (LLMs) have expanded the horizons of natural language understanding and generation. Notably, the output control and alignment with the input of LLMs can be refined through instruction tuning. However, as highlighted in several studies, low-quality data in the training set are usually detrimental to instruction tuning, resulting in inconsistent or even misleading LLM outputs. We propose a novel method, termed "reflection-tuning," which addresses the problem by self-improvement and judging capabilities of LLMs. This approach utilizes an oracle LLM to recycle the original training data by introspecting and enhancing the quality of instructions and responses in the data. Extensive experiments on widely used evaluation benchmarks show that LLMs trained with our recycled data outperform those trained with existing datasets in various benchmarks.
     ### Bullet Points

    * LLMs have expanded the scope of natural language understanding and generation, but low-quality data in the training set can lead to inconsistent or misleading outputs

    * Reflection-tuning is a new method that addresses this problem by self-improvement and judging capabilities

    * An oracle LLM recycles the original training data by introspecting and enhancing the quality of instructions and responses in the data

    * Extensive experiments on evaluation benchmarks show that LLM trained with recycled data outperforms those trained with existing datasets in various benchmarks.


1. [Contrastive Preference Learning: Learning from Human Feedback without RL](https://arxiv.org/abs/2310.13639v2), Joey Hejna,Rafael Rafailov,Harshit Sikchi,Chelsea Finn,Scott Niekum,W. Bradley Knox,Dorsa Sadigh, 20-10-2023
      ### Categories
      Machine Learning, Artificial Intelligence
     ### Abstract
     Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. Because of these optimization challenges, contemporary RLHF methods restrict themselves to contextual bandit settings (e.g., as in large language models) or limit observation dimensionality (e.g., state-based robotics). We overcome these limitations by introducing a new family of algorithms for optimizing behavior from human feedback using the regret-based model of human preferences. Using the principle of maximum entropy, we derive Contrastive Preference Learning (CPL), an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL. CPL is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs. This enables CPL to elegantly scale to high-dimensional and sequential RLHF problems while being simpler than prior methods.
     ### Bullet Points

     * Reinforcement Learning from Human Feedback (RLHF) is a popular paradigm for aligning models with human intent

     * RLHF algorithms typically use human preferences to learn a reward function and then align the model by optimizing the learned reward via reinforcement learning (RL)

     * However, recent work suggests that they follow the regret under the user's optimal policy, leading to unwieldy optimization challenges

     * To overcome these limitations, we introduce a new family of algorithms for optimizing behavior from human feedback using the regret-based model of human preferences

     * Contrastive Preference Learning (CPL) is an algorithm for learning optimal policies from preferences without learning reward functions, circumventing the need for RL

     * CPL is fully off-policy, uses only a simple contrastive objective, and can be applied to arbitrary MDPs.




1. [The Perils & Promises of Fact-checking with Large Language Models](https://arxiv.org/abs/2310.13549v1), Dorian Quelle,Alexandre Bovet, 20-10-2023
      ### Categories
      Computation and Language, Computers and Society, Human-Computer Interaction
     ### Abstract
     Autonomous fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large Language Models (LLMs) like GPT-4 are increasingly trusted to verify information and write academic papers, lawsuits, and news articles, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper comprehension of when agents succeed and when they fail.
     ### Bullet Points

     * Autonomous fact-checking, using machine learning to verify claims, has become vital as misinformation spreads

     * LLMs like GPT-4 are trusted to verify information and write academic papers, lawsuits, and news articles, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs

     * Agents explain their reasoning and cite relevant sources from the retrieved context, and LLM-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity

     * Further research is needed to understand when agents succeed and fail.




1. [ALCUNA: Large Language Models Meet New Knowledge](https://arxiv.org/abs/2310.14820v1), Xunjian Yin,Baizhou Huang,Xiaojun Wan, 23-10-2023
      ### Categories
      Computation and Language
     ### Abstract
     With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models' capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs' ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to assess LLMs' abilities in knowledge understanding, differentiation, and association. We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge. We also explore the impact of entity similarity on the model's understanding of entity knowledge and the influence of contextual entities. We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge.
     ### Bullet Points

     * The paper addresses the lack of benchmarks to evaluate LLMs' ability to handle new knowledge, particularly when faced with new knowledge

     * We propose KnowGen, an approach that generates new knowledge by altering existing entity attributes and relationships, and introduce a benchmark named ALCUNA to assess LLM's abilities in knowledge understanding, differentiation, and association

     * Benchmarks show that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge, and explore the impact of entity similarity on the model's understanding of entity knowledge and the influence of contextual entities.




1. [Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature](https://arxiv.org/abs/2310.16146v1), Alejandro Lozano,Scott L Fleming,Chia-Chun Chiang,Nigam Shah, 24-10-2023
      ### Categories
      Information Retrieval, Artificial Intelligence, Computation and Language
     ### Abstract
     and other publicly available OpenQA systems on PubMedRS-200.
     ### Bullet Points

     * PubMedRS-200 is a publicly available OpenQA system that provides a range of publicly available openQA systems

     * Some of these systems include OpenQA, Xavier, and Xerox

     * Additionally, there are several openQA databases available on PubMedWS-200, including PubMed.org, PubMed, and OpenQA.




1. [NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes](https://arxiv.org/abs/2310.15959v2), Junda Wang,Zonghai Yao,Zhichao Yang,Huixue Zhou,Rumeng Li,Xun Wang,Yucheng Xu,Hong Yu, 24-10-2023
      ### Categories
      Computation and Language
     ### Abstract
     We introduce NoteChat, a novel cooperative multi-agent framework leveraging Large Language Models (LLMs) to generate patient-physician dialogues. NoteChat embodies the principle that an ensemble of role-specific LLMs, through structured role-play and strategic prompting, can perform their assigned roles more effectively. The synergy among these role-playing LLMs results in a cohesive and efficient dialogue generation. Evaluation on MTS-dialogue, a benchmark dataset for patient-physician dialogues-note pairs, shows that models trained with the augmented synthetic patient-physician dialogues by NoteChat outperforms other state-of-the-art models for generating clinical notes. Our comprehensive automatic and human evaluation demonstrates that NoteChat substantially surpasses state-of-the-art models like ChatGPT and GPT-4 up to 22.78% by domain experts in generating superior synthetic patient-physician dialogues based on clinical notes. NoteChat has the potential to engage patients directly and help clinical documentation, a leading cause of physician burnout.
     ### Bullet Points

     * NoteChat is a cooperative multi-agent framework that uses LLMs to generate patient-physician dialogues

     * It embodies the principle that an ensemble of role-specific LLLs can perform their assigned roles more effectively through structured role-play and strategic prompting

     * Evaluation on MTS-dialogue shows that models trained with the augmented synthetic patient-pharmacial dialogues by NoteCutout outperforms other state-of-the-art models for generating clinical notes, surpassing other models like ChatGPT and GPT-4 up to 22.78% by domain experts in generating superior clinical dialogues based on clinical notes

     * It has the potential to engage patients directly and help clinical documentation, a leading cause of physician burnout.




1. [Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/abs/2310.16944v1), Lewis Tunstall,Edward Beeching,Nathan Lambert,Nazneen Rajani,Kashif Rasul,Younes Belkada,Shengyi Huang,Leandro von Werra,Cl√©mentine Fourrier,Nathan Habib,Nathan Sarrazin,Omar Sanseviero,Alexander M. Rush,Thomas Wolf, 25-10-2023
      ### Categories
      Machine Learning, Computation and Language
     ### Abstract



1. [JudgeLM: Fine-tuned Large Language Models are Scalable Judges](https://arxiv.org/abs/2310.17631), Lianghui Zhu,Xinggang Wang,Xinlong Wang, 26-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, and multi-turn chat.
     ### Bullet Points

    * We propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate them efficiently and effectively in open-ended benchmarks

    * We train JudgeLM at different scales and conduct a systematic analysis of its capabilities and behaviors

    * We analyze key biases and consider position bias, knowledge bias, and format bias

    * We introduce techniques such as swap augmentation, reference support, and reference drop to enhance the judge's performance

    * JudgeLM obtains state-of-the-art judge performance on PandaLM and proposed new benchmark

    * It obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement

    * The JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs.



1. [JudgeLM: Fine-tuned Large Language Models are Scalable Judges](https://arxiv.org/abs/2310.17631v1), Lianghui Zhu,Xinggang Wang,Xinlong Wang, 26-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, and multi-turn chat.
     ### Bullet Points

     * We propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate them efficiently and effectively in open-ended benchmarks

     * We train JudgeLM at different scales and conduct a systematic analysis of its capabilities and behaviors

     * We analyze key biases and consider position bias, knowledge bias, and format bias

     * We introduce techniques such as swap augmentation, reference support, and reference drop to enhance the judge's performance

     * JudgeLM obtains state-of-the-art judge performance on PandaLM and proposed new benchmark

     * It obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement

     * The JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs.




1. [Large Language Models as Evolutionary Optimizers](https://arxiv.org/abs/2310.19046v2), Shengcai Liu,Caishun Chen,Xinghua Qu,Ke Tang,Yew-Soon Ong, 29-10-2023
      ### Categories
      Neural and Evolutionary Computing
     ### Abstract
     Evolutionary algorithms (EAs) have achieved remarkable success in tackling complex combinatorial optimization problems. However, EAs often demand carefully-designed operators with the aid of domain expertise to achieve satisfactory performance. In this work, we present the first study on large language models (LLMs) as evolutionary combinatorial optimizers. The main advantage is that it requires minimal domain knowledge and human efforts, as well as no additional training of the model. This approach is referred to as LLM-driven EA (LMEA). Specifically, in each generation of the evolutionary search, LMEA instructs the LLM to select parent solutions from current population, and perform crossover and mutation to generate offspring solutions. Then, LMEA evaluates these new solutions and include them into the population for the next generation. LMEA is equipped with a self-adaptation mechanism that controls the temperature of the LLM. This enables it to balance between exploration and exploitation and prevents the search from getting stuck in local optima. We investigate the power of LMEA on the classical traveling salesman problems (TSPs) widely used in combinatorial optimization research. Notably, the results show that LMEA performs competitively to traditional heuristics in finding high-quality solutions on TSP instances with up to 20 nodes. Additionally, we also study the effectiveness of LLM-driven crossover/mutation and the self-adaptation mechanism in evolutionary search. In summary, our results reveal the great potentials of LLMs as evolutionary optimizers for solving combinatorial problems. We hope our research shall inspire future explorations on LLM-driven EAs for complex optimization challenges.
     ### Bullet Points

     * The study presents the first study on large language models (LLMs) as evolutionary combinatorial optimizers, which requires minimal domain knowledge and human efforts to achieve satisfactory performance

     * LLMs are equipped with a self-adaptation mechanism that controls the temperature of the LLM and performs competitively to traditional heuristics in finding high-quality solutions on TSP instances with up to 20 nodes

     * The results show that LLM performs well on classical traveling salesman problems (TSPs) and has the potential to inspire future explorations on LLM-driven EAs for complex optimization challenges.




1. [TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise](https://arxiv.org/abs/2310.19019v2), Nan He,Hanyu Lai,Chenyang Zhao,Zirui Cheng,Junting Pan,Ruoyu Qin,Ruofan Lu,Rui Lu,Yunchen Zhang,Gangming Zhao,Zhaohui Hou,Zhiyuan Huang,Shaoqing Lu,Ding Liang,Mingjie Zhan, 29-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, which makes annotation more than just an answer, thus allowing other models to learn "why" instead of just "what". The TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we augmented 58 NLP datasets and taught various student models with different parameters from OPT and BLOOM series in a multi-task setting. The experimental results indicate that the data augmentation provided by TeacherLM has brought significant benefits. We will release the TeacherLM series of models and augmented datasets as open-source.
     ### Bullet Points

     * The work proposes TeacherLM-7.1B, a small NLP model capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, making annotation more than just an answer

     * The model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters

     * The data augmentation provided by TeacherLM has brought significant benefits, and we will release the TeacherLM series of models and augmented datasets as open-source.




1. [EHRTutor: Enhancing Patient Understanding of Discharge Instructions](https://arxiv.org/abs/2310.19212v1), Zihao Zhang,Zonghai Yao,Huixue Zhou,Feiyun ouyang,Hong Yu, 30-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Large language models have shown success as a tutor in education in various fields. Educating patients about their clinical visits plays a pivotal role in patients' adherence to their treatment plans post-discharge. This paper presents EHRTutor, an innovative multi-component framework leveraging the Large Language Model (LLM) for patient education through conversational question-answering. EHRTutor first formulates questions pertaining to the electronic health record discharge instructions. It then educates the patient through conversation by administering each question as a test. Finally, it generates a summary at the end of the conversation. Evaluation results using LLMs and domain experts have shown a clear preference for EHRTutor over the baseline. Moreover, EHRTutor also offers a framework for generating synthetic patient education dialogues that can be used for future in-house system training.
     ### Bullet Points

     * The paper presents EHRTutor, a multi-component framework leveraging the Large Language Model (LLM) for patient education through conversational question-answering

     * The framework formulates questions related to the electronic health record discharge instructions, then educates the patient through conversation and generates a summary at the end of the conversation

     * Evaluation results using LLMs and domain experts have shown a preference for the framework over the baseline, and it also offers a framework for generating synthetic patient education dialogues that can be used for future in-house system training.




1. [Evaluating Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2310.19736v3), Zishan Guo,Renren Jin,Chuang Liu,Yufei Huang,Dan Shi,Supryadi,Linhao Yu,Yan Liu,Jiaxuan Li,Bojian Xiong,Deyi Xiong, 30-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     .
     None


1. [Learning From Mistakes Makes LLM Better Reasoner](https://arxiv.org/abs/2310.20689v2), Shengnan An,Zexiong Ma,Zeqi Lin,Nanning Zheng,Jian-Guang Lou,Weizhu Chen, 31-10-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     .
     None


1. [TopicGPT: A Prompt-based Topic Modeling Framework](https://arxiv.org/abs/2311.01449v1), Chau Minh Pham,Alexander Hoyle,Simeng Sun,Mohit Iyyer, 02-11-2023
      ### Categories
      Computation and Language
     ### Abstract
     Topic modeling is a well-established technique for exploring text corpora. Conventional topic models (e.g., LDA) represent topics as bags of words that often require "reading the tea leaves" to interpret; additionally, they offer users minimal semantic control over topics. To tackle these issues, we introduce TopicGPT, a prompt-based framework that uses large language models (LLMs) to uncover latent topics within a provided text collection. TopicGPT produces topics that align better with human categorizations compared to competing methods: for example, it achieves a harmonic mean purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for the strongest baseline. Its topics are also more interpretable, dispensing with ambiguous bags of words in favor of topics with natural language labels and associated free-form descriptions. Moreover, the framework is highly adaptable, allowing users to specify constraints and modify topics without the need for model retraining. TopicGPT can be further extended to hierarchical topical modeling, enabling users to explore topics at various levels of granularity. By streamlining access to high-quality and interpretable topics, TopicGPT represents a compelling, human-centered approach to topic modeling.
     ### Bullet Points

     * Topic modeling is a well-established technique for exploring text corpora

     * Conventional topic models often require reading the tea leaves to interpret and offer minimal semantic control over topics

     * We introduce TopicGPT, a prompt-based framework that uses large language models (LLMs) to uncover latent topics within a provided text collection

     * It produces topics that align better with human categorizations compared to competing methods and is more interpretable, dispensing with ambiguous bags of words in favor of topics with natural language labels and associated free-form descriptions

     * The framework is highly adaptable, allowing users to specify constraints and modify topics without the need for model retraining

     * It can be further extended to hierarchical topical modeling, enabling users to explore topics at various levels of granularity

     * It represents a human-centered approach to topic modeling.




1. [Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review](https://arxiv.org/abs/2311.01918v1), Mingze Yuan,Peng Bao,Jiajia Yuan,Yunhao Shen,Zifan Chen,Yi Xie,Jie Zhao,Yang Chen,Li Zhang,Lin Shen,Bin Dong, 03-11-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     for an accompanying GitHub repository containing latest papers.
     ### Bullet Points

     * I'm sorry, but you haven't provided me with any information about the GitHub repository containing the latest papers

     * Please provide me with more details so that I can assist you better in summarizing the summary for you.




1. [Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs](https://arxiv.org/abs/2311.02262v1), Qingru Zhang,Chandan Singh,Liyuan Liu,Xiaodong Liu,Bin Yu,Jianfeng Gao,Tuo Zhao, 03-11-2023
      ### Categories
      Computation and Language, Machine Learning
     ### Abstract
     .
     None


1. [Can LLMs Follow Simple Rules?](https://arxiv.org/abs/2311.04235v1), Norman Mu,Sarah Chen,Zifan Wang,Sizhe Chen,David Karamardian,Lulwa Aljeraisy,Dan Hendrycks,David Wagner, 06-11-2023
      ### Categories
      Artificial Intelligence, Computation and Language, Machine Learning
     ### Abstract
     As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Evaluating how well LLMs follow developer-provided rules in the face of adversarial inputs typically requires manual review, which slows down monitoring and methods development. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 15 simple text scenarios in which the model is instructed to obey a set of rules in natural language while interacting with the human user. Each scenario has a concise evaluation program to determine whether the model has broken any rules in a conversation. Through manual exploration of model behavior in our scenarios, we identify 6 categories of attack strategies and collect two suites of test cases: one consisting of unique conversations from manual testing and one that systematically implements strategies from the 6 categories. Across various popular proprietary and open models such as GPT-4 and Llama 2, we find that all models are susceptible to a wide variety of adversarial hand-crafted user inputs, though GPT-4 is the best-performing model. Additionally, we evaluate open models under gradient-based attacks and find significant vulnerabilities. We propose RuLES as a challenging new setting for research into exploring and defending against both manual and automatic attacks on LLMs.
     ### Bullet Points

     * RuLES is a programmatic framework for measuring rule-following ability in LLMs

     * It consists of 15 simple text scenarios in which the model is instructed to obey a set of rules in natural language while interacting with the human user

     * Through manual exploration of model behavior, we identify 6 categories of attack strategies and collect two suites of test cases: one consisting of unique conversations from manual testing and one that systematically implements strategies from the 6 categories

     * We evaluate open models under gradient-based attacks and find significant vulnerabilities

     * RuLES offers a challenging new setting for research into exploring and defending against both manual and automatic attacks on LLM models.




1. [Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](https://arxiv.org/abs/2311.03099v1), Le Yu,Bowen Yu,Haiyang Yu,Fei Huang,Yongbin Li, 06-11-2023
      ### Categories
      Computation and Language, Machine Learning
     ### Abstract
     .
     None


1. [S-LoRA: Serving Thousands of Concurrent LoRA Adapters](https://arxiv.org/abs/2311.03285v2), Ying Sheng,Shiyi Cao,Dacheng Li,Coleman Hooper,Nicholas Lee,Shuo Yang,Christopher Chou,Banghua Zhu,Lianmin Zheng,Kurt Keutzer,Joseph E. Gonzalez,Ion Stoica, 06-11-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Distributed, Parallel, and Cluster Computing
     ### Abstract
     
     None


1. [Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves](https://arxiv.org/abs/2311.04205v1), Yihe Deng,Weitong Zhang,Zixiang Chen,Quanquan Gu, 07-11-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning

1. [Prompt Cache: Modular Attention Reuse for Low-Latency Inference](https://arxiv.org/abs/2311.04934v2), In Gim,Guojun Chen,Seung-seob Lee,Nikhil Sarda,Anurag Khandelwal,Lin Zhong, 07-11-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     We present Prompt Cache, an approach for accelerating inference for large language models (LLM) by reusing attention states across different LLM prompts. Many input prompts have overlapping text segments, such as system messages, prompt templates, and documents provided for context. Our key insight is that by precomputing and storing the attention states of these frequently occurring text segments on the inference server, we can efficiently reuse them when these segments appear in user prompts. Prompt Cache employs a schema to explicitly define such reusable text segments, called prompt modules. The schema ensures positional accuracy during attention state reuse and provides users with an interface to access cached states in their prompt. Using a prototype implementation, we evaluate Prompt Cache across several LLMs. We show that Prompt Cache significantly reduce latency in time-to-first-token, especially for longer prompts such as document-based question answering and recommendations. The improvements range from 8x for GPU-based inference to 60x for CPU-based inference, all while maintaining output accuracy and without the need for model parameter modifications.
     ### Bullet Points

    * Prompt Cache is an approach for accelerating inference for large language models (LLM) by reusing attention states across different LLM prompts

    * It employs a schema to define reusable text segments called prompt modules, which ensure positional accuracy during attention state reuse and provide users with an interface to access cached states in their prompt

    * We evaluate the implementation using a prototype implementation and show that Prompto Cache significantly reduces latency in time-to-first-token, especially for longer prompts such as document-based question answering and recommendations

    * The improvements range from 8x for GPU-based inference to 60x for CPU-based Inference, all while maintaining output accuracy and without model parameter modifications.



1. [ADaPT: As-Needed Decomposition and Planning with Language Models](https://arxiv.org/abs/2311.05772v1), Archiki Prasad,Alexander Koller,Mareike Hartmann,Peter Clark,Ashish Sabharwal,Mohit Bansal,Tushar Khot, 08-11-2023
      ### Categories
      Artificial Intelligence, Computation and Language, Machine Learning
     ### Abstract
     Large Language Models (LLMs) are increasingly being used for interactive decision-making tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADaPT substantially outperforms established strong baselines, achieving success rates up to 28.3% higher in ALFWorld, 27% in WebShop, and 33% in TextCraft -- a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADaPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity.
     ### Bullet Points

     * LLMs are being used for interactive decision-making tasks requiring planning and adapting to the environment, but they struggle with task complexity

     * To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed when the LLM is unable to execute them

     * ADaPT outperforms established strong baselines and achieves success rates up to 28.3% higher in ALFWorld, 27% in WebShop, and 33% in TextCraft

     * Multilevel decomposition is important and dynamically adjusts to the capabilities of the executor LLM as well as to task complexity through extensive analysis.




1. [A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges](https://arxiv.org/abs/2311.05112v2), Hongjian Zhou,Fenglin Liu,Boyang Gu,Xinyu Zou,Jinfa Huang,Jinge Wu,Yiru Li,Sam S. Chen,Peilin Zhou,Junling Liu,Yining Hua,Chengfeng Mao,Xian Wu,Yefeng Zheng,Lei Clifton,Zheng Li,Jiebo Luo,David A. Clifton, 09-11-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     .
     None


1. [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232v1), Lei Huang,Weijiang Yu,Weitao Ma,Weihong Zhong,Zhangyin Feng,Haotian Wang,Qianglong Chen,Weihua Peng,Xiaocheng Feng,Bing Qin,Ting Liu, 09-11-2023
      ### Categories
      Computation and Language
     ### Abstract
     The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs.
     ### Bullet Points

     * The survey aims to provide an overview of recent advances in the field of LLM hallucinations, including an innovative taxonomy, a comprehensive overview of Hallucination detection methods, benchmarks, representative approaches, and open questions to delineate pathways for future research.




1. [LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents](https://arxiv.org/abs/2311.05437v1), Shilong Liu,Hao Cheng,Haotian Liu,Hao Zhang,Feng Li,Tianhe Ren,Xueyan Zou,Jianwei Yang,Hang Su,Jun Zhu,Lei Zhang,Jianfeng Gao,Chunyuan Li, 09-11-2023
      ### Categories
      Computer Vision, Artificial Intelligence, Computation and Language, Machine Learning, Multimedia
     ### Abstract
     LLaVA-Plus is a general-purpose multimodal assistant that expands the capabilities of large multimodal models. It maintains a skill repository of pre-trained vision and vision-language models and can activate relevant tools based on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained on multimodal instruction-following data to acquire the ability to use tools, covering visual understanding, generation, external knowledge retrieval, and compositions. Empirical results show that LLaVA-Plus outperforms LLaVA in existing capabilities and exhibits new ones. It is distinct in that the image query is directly grounded and actively engaged throughout the entire human-AI interaction sessions, significantly improving tool use performance and enabling new scenarios.
     ### Bullet Points

     * LLaVA-Plus is a general-purpose multimodal assistant that expands the capabilities of large multimodal models

     * It maintains a skill repository of pre-trained vision and vision-language models and can activate relevant tools based on users' inputs

     * It is trained on multimodal instruction-following data to acquire the ability to use tools, covering visual understanding, generation, external knowledge retrieval, and compositions

     * Empirical results show that it outperforms LlaVA in existing capabilities and exhibits new ones

     * The image query is directly grounded and actively engaged throughout the entire human-AI interaction sessions, improving tool use performance and enabling new scenarios.




1. [Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure](https://arxiv.org/abs/2311.07590v2), J√©r√©my Scheurer,Mikita Balesni,Marius Hobbhahn, 09-11-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception.
     ### Bullet Points

     * Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so

     * They deploy GPT-4 as an agent in a simulated environment where it assumes the role of an autonomous stock trading agent

     * The model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management

     * When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision

     * We investigate how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes.




1. [Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations](https://arxiv.org/abs/2311.05584v1), Joey Hong,Sergey Levine,Anca Dragan, 09-11-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language
     ### Abstract
     Large language models (LLMs) have emerged as powerful and general solutions to many natural language tasks. However, many of the most important applications of language generation are interactive, where an agent has to talk to a person to reach a desired outcome. For example, a teacher might try to understand their student's current comprehension level to tailor their instruction accordingly, and a travel agent might ask questions of their customer to understand their preferences in order to recommend activities they might enjoy. LLMs trained with supervised fine-tuning or "single-step" RL, as with standard RLHF, might struggle which tasks that require such goal-directed behavior, since they are not trained to optimize for overall conversational outcomes after multiple turns of interaction. In this work, we explore a new method for adapting LLMs with RL for such goal-directed dialogue. Our key insight is that, though LLMs might not effectively solve goal-directed dialogue tasks out of the box, they can provide useful data for solving such tasks by simulating suboptimal but human-like behaviors. Given a textual description of a goal-directed dialogue task, we leverage LLMs to sample diverse synthetic rollouts of hypothetical in-domain human-human interactions. Our algorithm then utilizes this dataset with offline reinforcement learning to train an interactive conversational agent that can optimize goal-directed objectives over multiple turns. In effect, the LLM produces examples of possible interactions, and RL then processes these examples to learn to perform more optimal interactions. Empirically, we show that our proposed approach achieves state-of-the-art performance in various goal-directed dialogue tasks that include teaching and preference elicitation.
     ### Bullet Points

     * A new method for adapting LLMs with RL for goal-directed dialogue involves simulating suboptimal but human-like behaviors

     * The algorithm uses synthetic rollouts of hypothetical in-domain human-human interactions to train an interactive conversational agent that can optimize objective-directed objectives over multiple turns

     * Empirically, the proposed approach achieves state-of-the-art performance in various goal-oriented dialogue tasks that include teaching and preference elicitation.




1. [MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks](https://arxiv.org/abs/2311.07463v1), Sanchit Ahuja,Divyanshu Aggarwal,Varun Gumma,Ishaan Watts,Ashutosh Sathe,Millicent Ochieng,Rishav Hada,Prachi Jain,Maxamed Axmed,Kalika Bali,Sunayana Sitaram, 13-11-2023
      ### Categories
      Computation and Language
     ### Abstract
     Recently, there has been a rapid advancement in research on Large Language Models (LLMs), resulting in significant progress in several Natural Language Processing (NLP) tasks. Consequently, there has been a surge in LLM evaluation research to comprehend the models' capabilities and limitations. However, much of this research has been confined to the English language, leaving LLM building and evaluation for non-English languages relatively unexplored. There has been an introduction of several new LLMs, necessitating their evaluation on non-English languages. This study aims to expand our MEGA benchmarking suite by including six new datasets to form the MEGAVERSE benchmark. The benchmark comprises 22 datasets covering 81 languages, including low-resource African languages. We evaluate several state-of-the-art LLMs like GPT-3.5-Turbo, GPT4, PaLM2, and Llama2 on the MEGAVERSE datasets. Additionally, we include two multimodal datasets in the benchmark and assess the performance of the LLaVa-v1.5 model. Our experiments suggest that GPT4 and PaLM2 outperform the Llama models on various tasks, notably on low-resource languages, with GPT4 outperforming PaLM2 on more datasets than vice versa. However, issues such as data contamination must be addressed to obtain an accurate assessment of LLM performance on non-English languages.
     ### Bullet Points

     * The study aims to expand the MEGAVERSE benchmarking suite by including six new LLMs, including GPT-3.5-Turbo, GPT4, PaLM2, and Llama2, and evaluate their performance on non-English languages

     * The benchmark includes 22 datasets covering 81 languages, including low-resource African languages

     * GPT4 and PaLM2 outperform the LLM models on various tasks, notably on low-rsource languages, and two multimodal datasets assess the performance of the LLaVa-v1.5 model

     * However, data contamination needs to be addressed to obtain an accurate assessment of LLM performance.




1. [The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4](https://arxiv.org/abs/2311.07361v2), Microsoft Research AI4Science,Microsoft Azure Quantum, 13-11-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     In recent years, groundbreaking advancements in natural language processing have culminated in the emergence of powerful large language models (LLMs), which have showcased remarkable capabilities across a vast array of domains, including the understanding, generation, and translation of natural language, and even tasks that extend beyond language processing. In this report, we delve into the performance of LLMs within the context of scientific discovery, focusing on GPT-4, the state-of-the-art language model. Our investigation spans a diverse range of scientific areas encompassing drug discovery, biology, computational chemistry (density functional theory (DFT) and molecular dynamics (MD)), materials design, and partial differential equations (PDE). Evaluating GPT-4 on scientific tasks is crucial for uncovering its potential across various research domains, validating its domain-specific expertise, accelerating scientific progress, optimizing resource allocation, guiding future model development, and fostering interdisciplinary research. Our exploration methodology primarily consists of expert-driven case assessments, which offer qualitative insights into the model's comprehension of intricate scientific concepts and relationships, and occasionally benchmark testing, which quantitatively evaluates the model's capacity to solve well-defined domain-specific problems. Our preliminary exploration indicates that GPT-4 exhibits promising potential for a variety of scientific applications, demonstrating its aptitude for handling complex problem-solving and knowledge integration tasks. Broadly speaking, we evaluate GPT-4's knowledge base, scientific understanding, scientific numerical calculation abilities, and various scientific prediction capabilities.
     ### Bullet Points

     * The report explores the performance of large language models (LLMs) in scientific discovery, focusing on GPT-4, a state-of-the-art language model

     * The exploration methodology involves expert-driven case assessments, benchmark testing, and interdisciplinary research

     * GPT4 has promising potential for a variety of scientific applications, demonstrating its ability to handle complex problem-solving and knowledge integration tasks.




1. [Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers Faster](https://arxiv.org/abs/2311.08263v1), Hongxuan Zhang,Zhining Liu,Jiaqi Zheng,Chenyi Zhuang,Jinjie Gu,Guihai Chen, 14-11-2023
      ### Categories
      Computation and Language
     ### Abstract
     In this work, we propose FastCoT, a model-agnostic framework based on parallel decoding without any further training of an auxiliary model or modification to the LLM itself. FastCoT uses a size-varying context window whose size changes with position to conduct parallel decoding and auto-regressive decoding simultaneously, thus fully utilizing GPU computation resources. In FastCoT, the parallel decoding part provides the LLM with a quick glance of the future composed of approximate tokens, which could lead to faster answers compared to regular autoregressive decoding used by causal transformers. We also provide an implementation of parallel decoding within LLM, which supports KV-cache generation and batch processing. Through extensive experiments, we demonstrate that FastCoT saves inference time by nearly 20% with only a negligible performance drop compared to the regular approach. Additionally, we show that the context window size exhibits considerable robustness for different tasks.
     ### Bullet Points

     * FastCoT is a model-agnostic framework based on parallel decoding without any training of an auxiliary model or modification to the LLM itself

     * It uses a size-varying context window that changes with position to conduct both parallel and auto-regressive decoding simultaneously, fully utilizing GPU computation resources

     * The process saves inference time by nearly 20% with only a negligible performance drop compared to the regular approach, and the context window size exhibits robustness for different tasks.




1. [The ART of LLM Refinement: Ask, Refine, and Trust](https://arxiv.org/abs/2311.07961v1), Kumar Shridhar,Koustuv Sinha,Andrew Cohen,Tianlu Wang,Ping Yu,Ram Pasunuru,Mrinmaya Sachan,Jason Weston,Asli Celikyilmaz, 14-11-2023
      ### Categories
      Computation and Language
     ### Abstract
     In recent years, Large Language Models (LLMs) have demonstrated remarkable generative abilities, but can they judge the quality of their own generations? A popular concept, referred to as self-refinement, postulates that LLMs can detect and correct the errors in their generations when asked to do so. However, recent empirical evidence points in the opposite direction, suggesting that LLMs often struggle to accurately identify errors when reasoning is involved. To address this, we propose a reasoning with refinement objective called ART: Ask, Refine, and Trust, which asks necessary questions to decide when an LLM should refine its output, and either affirm or withhold trust in its refinement by ranking the refinement and the initial prediction. On two multistep reasoning tasks of mathematical word problems (GSM8K) and question answering (StrategyQA), ART achieves a performance gain of +5 points over self-refinement baselines, while using a much smaller model as the decision maker. We also demonstrate the benefit of using smaller models to make refinement decisions as a cost-effective alternative to fine-tuning a larger model.
     ### Bullet Points

     * LLMs can detect and correct errors in their own generations, but they often struggle to accurately identify errors when reasoning is involved

     * To address this, we propose ART: Ask, Refine, and Trust, which asks necessary questions to decide when an LLM should refine its output, and either affirm or withhold trust in its refinement by ranking the refinement and the initial prediction

     * ART achieves a performance gain of +5 points over self-refinement baselines, while using a much smaller model as the decision maker

     * The benefit of using smaller models to make refinement decisions is that it is a cost-effective alternative to fine-tuning a larger model.




1. [Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code](https://arxiv.org/abs/2311.07989v3), Ziyin Zhang,Chaoyu Chen,Bingchang Liu,Cong Liao,Zi Gong,Hang Yu,Jianguo Li,Rui Wang, 14-11-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Software Engineering
     ### Abstract
     .
     None


1. [Contrastive Chain-of-Thought Prompting](https://arxiv.org/abs/2311.09277v1), Yew Ken Chia,Guizhen Chen,Luu Anh Tuan,Soujanya Poria,Lidong Bing, 15-11-2023
      ### Categories
      Computation and Language
     ### Abstract
     Despite the success of chain of thought in enhancing language model reasoning, the underlying process remains less well understood. Although logically sound reasoning appears inherently crucial for chain of thought, prior studies surprisingly reveal minimal impact when using invalid demonstrations instead. Furthermore, the conventional chain of thought does not inform language models on what mistakes to avoid, which potentially leads to more errors. Hence, inspired by how humans can learn from both positive and negative examples, we propose contrastive chain of thought to enhance language model reasoning. Compared to the conventional chain of thought, our approach provides both valid and invalid reasoning demonstrations, to guide the model to reason step-by-step while reducing reasoning mistakes. To improve generalization, we introduce an automatic method to construct contrastive demonstrations. Our experiments on reasoning benchmarks demonstrate that contrastive chain of thought can serve as a general enhancement of chain-of-thought prompting.
     ### Bullet Points

     * The underlying process of enhancing language model reasoning is not well understood, and prior studies reveal minimal impact when using invalid demonstrations

     * Contrastive chain of thought provides valid and invalid reasoning demonstrations to guide the model to reason step-by-step while reducing reasoning mistakes

     * To improve generalization, an automatic method to construct contrastive demonstrations is introduced to enhance generalization

     * Experiments on reasoning benchmarks demonstrate that contrastive Chain of Thought can serve as a general enhancement of chain-of-thought prompting.




1. [Towards Verifiable Text Generation with Symbolic References](https://arxiv.org/abs/2311.09188v1), Lucas Torroba Hennigen,Shannon Shen,Aniruddha Nrusimha,Bernhard Gapp,David Sontag,Yoon Kim, 15-11-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     Large language models (LLMs) have demonstrated an impressive ability to synthesize plausible and fluent text. However they remain vulnerable to hallucinations, and thus their outputs generally require manual human verification for high-stakes applications, which can be time-consuming and difficult. This paper proposes symbolically grounded generation (SymGen) as a simple approach for enabling easier validation of an LLM's output. SymGen prompts an LLM to interleave its regular output text with explicit symbolic references to fields present in some conditioning data (e.g., a table in JSON format). The references can be used to display the provenance of different spans of text in the generation, reducing the effort required for manual verification. Across data-to-text and question answering experiments, we find that LLMs are able to directly output text that makes use of symbolic references while maintaining fluency and accuracy.
     ### Bullet Points

     * The paper proposes symbolically grounded generation (SymGen) as a simple approach to enable easier validation of LLM's output, prompting an LLM to interleave its regular output text with explicit symbolic references to fields present in some conditioning data, reducing the effort required for manual verification

     * LLMs are able to directly output text that makes use of symbolic references while maintaining fluency and accuracy across data-to-text and question answering experiments.




1. [MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning](https://arxiv.org/abs/2311.10537v1), Xiangru Tang,Anni Zou,Zhuosheng Zhang,Yilun Zhao,Xingyao Zhang,Arman Cohan,Mark Gerstein, 16-11-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     }.
     None


1. [R-Tuning: Teaching Large Language Models to Refuse Unknown Questions](https://arxiv.org/abs/2311.09677v1), Hanning Zhang,Shizhe Diao,Yong Lin,Yi R. Fung,Qing Lian,Xingyao Wang,Yangyi Chen,Heng Ji,Tong Zhang, 16-11-2023
      ### Categories
      Computation and Language
     ### Abstract
     .
     None


1. [Generalized products and Lorentzian length spaces](https://arxiv.org/abs/2311.10691v1), Elefterios Soultanis, 17-11-2023
      ### Categories
      Differential Geometry, Mathematical Physics, Algebraic Geometry, Mathematical Physics
     ### Abstract
     The generalized Lorentzian product naturally has a Lorentzian length structure but can fail the push-up condition in general. We recover the push-up property under a log-Lipschitz condition on the time variable and establish sufficient conditions for global hyperbolicity. Moreover we formulate time-like Ricci curvature bounds without push-up and regularity assumptions, and obtain a partial rigidity of the splitting under a strong energy condition.
     ### Bullet Points

     * The generalized Lorentzian product can fail the push-up condition in general, but it can be recovered under log-Lipschitz condition on the time variable, establish conditions for global hyperbolicity, formulate time-like Ricci curvature bounds, and obtain partial rigidity of the splitting under a strong energy condition.




1. [Testing Language Model Agents Safely in the Wild](https://arxiv.org/abs/2311.10538v3), Silen Naihin,David Atkinson,Marc Green,Merwane Hamadi,Craig Swift,Douglas Schonholtz,Adam Tauman Kalai,David Bau, 17-11-2023
      ### Categories
      Artificial Intelligence
     ### Abstract
     A prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. Yet real-world autonomous tests face several unique safety challenges, both due to the possibility of causing harm during a test, as well as the risk of encountering new unsafe agent behavior through interactions with real-world and potentially malicious actors. We propose a framework for conducting safe autonomous agent tests on the open internet: agent actions are audited by a context-sensitive monitor that enforces a stringent safety boundary to stop an unsafe test, with suspect behavior ranked and logged to be examined by humans. We design a basic safety monitor (AgentMonitor) that is flexible enough to monitor existing LLM agents, and, using an adversarial simulated agent, we measure its ability to identify and stop unsafe situations. Then we apply the AgentMonitor on a battery of real-world tests of AutoGPT, and we identify several limitations and challenges that will face the creation of safe in-the-wild tests as autonomous agents grow more capable.
     ### Bullet Points

     * To conduct safe autonomous agent tests on the open internet, a framework is proposed

     * Agent actions are audited by a context-sensitive monitor that enforces a safety boundary to stop an unsafe test, with suspect behavior ranked and logged to be examined by humans

     * A basic safety monitor (AgentMonitor) is designed to monitor existing LLM agents, and using an adversarial simulated agent, we measure its ability to identify and stop unsafe situations

     * We then apply the AgentMonitors on a battery of real-world AutoGPT tests to identify limitations and challenges that will face the creation of safe in-the-wild tests.




1. [Orca 2: Teaching Small Language Models How to Reason](https://arxiv.org/abs/2311.11045v2), Arindam Mitra,Luciano Del Corro,Shweti Mahajan,Andres Codas,Clarisse Simoes,Sahaj Agarwal,Xuxi Chen,Anastasia Razdaibiedina,Erik Jones,Kriti Aggarwal,Hamid Palangi,Guoqing Zheng,Corby Rosset,Hamed Khanpour,Ahmed Awadallah, 18-11-2023
      ### Categories
      Artificial Intelligence
     ### Abstract
     to support research on the development, evaluation, and alignment of smaller LMs
     ### Bullet Points

     * To support research on the development, evaluation, and alignment of smaller LMs, it is recommended to conduct a literature review and review of existing literature on the topic

     * The review should provide a comprehensive overview of the research findings, methodology, and methods used to develop, evaluate, and align these smaller models

     * Additionally, the review should focus on the strengths and weaknesses of the smaller models, as well as their potential applications and limitations.




1. [Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents](https://arxiv.org/abs/2311.11797v1), Zhuosheng Zhang,Yao Yao,Aston Zhang,Xiangru Tang,Xinbei Ma,Zhiwei He,Yiming Wang,Mark Gerstein,Rui Wang,Gongshen Liu,Hai Zhao, 20-11-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Computer Vision, Human-Computer Interaction, Multiagent Systems
     ### Abstract
     .
     None


1. [From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models](https://arxiv.org/abs/2311.13063v2), Zachary Englhardt,Chengqian Ma,Margaret E. Morris,Xuhai "Orson" Xu,Chun-Cheng Chang,Lianhui Qin,Daniel McDuff,Xin Liu,Shwetak Patel,Vikram Iyer, 21-11-2023
      ### Categories
      Artificial Intelligence
     ### Abstract
     Passively collected behavioral health data from ubiquitous sensors holds significant promise to provide mental health professionals insights from patient's daily lives; however, developing analysis tools to use this data in clinical practice requires addressing challenges of generalization across devices and weak or ambiguous correlations between the measured signals and an individual's mental health. To address these challenges, we take a novel approach that leverages large language models (LLMs) to synthesize clinically useful insights from multi-sensor data. We develop chain of thought prompting methods that use LLMs to generate reasoning about how trends in data such as step count and sleep relate to conditions like depression and anxiety. We first demonstrate binary depression classification with LLMs achieving accuracies of 61.1% which exceed the state of the art. While it is not robust for clinical use, this leads us to our key finding: even more impactful and valued than classification is a new human-AI collaboration approach in which clinician experts interactively query these tools and combine their domain expertise and context about the patient with AI generated reasoning to support clinical decision-making. We find models like GPT-4 correctly reference numerical data 75% of the time, and clinician participants express strong interest in using this approach to interpret self-tracking data.
     ### Bullet Points

     * We use large language models (LLMs) to synthesize clinically useful insights from multi-sensor data and develop chain of thought prompting methods to generate reasoning about how trends in data relate to depression and anxiety

     * We first demonstrate binary depression classification with LLMs achieving accuracies of 61.1%, which exceed the state of the art

     * This approach is more impactful and valued than classification, and clinician experts interact with these tools and combine domain expertise and context to support clinical decision-making

     * GPT-4 correctly references numerical data 75% of the time and clinician participants express strong interest in using this approach to interpret self-tracking data.




1. [Algorithm Evolution Using Large Language Model](https://arxiv.org/abs/2311.15249v1), Fei Liu,Xialiang Tong,Mingxuan Yuan,Qingfu Zhang, 26-11-2023
      ### Categories
      Neural and Evolutionary Computing, Artificial Intelligence, Machine Learning
     ### Abstract
     Optimization can be found in many real-life applications. Designing an effective algorithm for a specific optimization problem typically requires a tedious amount of effort from human experts with domain knowledge and algorithm design skills. In this paper, we propose a novel approach called Algorithm Evolution using Large Language Model (AEL). It utilizes a large language model (LLM) to automatically generate optimization algorithms via an evolutionary framework. AEL does algorithm-level evolution without model training. Human effort and requirements for domain knowledge can be significantly reduced. We take constructive methods for the salesman traveling problem as a test example, we show that the constructive algorithm obtained by AEL outperforms simple hand-crafted and LLM-generated heuristics. Compared with other domain deep learning model-based algorithms, these methods exhibit excellent scalability across different problem sizes. AEL is also very different from previous attempts that utilize LLMs as search operators in algorithms.
     ### Bullet Points

     * The paper proposes a novel approach called Algorithm Evolution using Large Language Model (AEL) to automatically generate optimization algorithms via an evolutionary framework without model training, reducing human effort and requirements for domain knowledge

     * The approach outperforms simple hand-crafted and LLM-generated heuristics and exhibits excellent scalability across different problem sizes

     * AEL is different from previous attempts that utilize LLMs as search operators in algorithms.




1. [ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?](https://arxiv.org/abs/2311.16989v4), Hailin Chen,Fangkai Jiao,Xingxuan Li,Chengwei Qin,Mathieu Ravaut,Ruochen Zhao,Caiming Xiong,Shafiq Joty, 28-11-2023
      ### Categories
      Computation and Language
     ### Abstract
     Upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce. Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work, on the first anniversary of ChatGPT, we provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT.
     ### Bullet Points

     * ChatGPT, released in late 2022, has revolutionized the field of AI by introducing instruction-tuning a large language model (LLM) that can answer human questions and follow instructions on a broad panel of tasks

     * This has led to the growth of new LLMs, which are flourishing at frequent intervals across academia and industry

     * While closed-source models generally outperform their open-source counterparts, progress has been rapid with claims of achieving parity or even better on certain tasks, which has crucial implications not only on research but also on business.




1. [A collection of principles for guiding and evaluating large language models](https://arxiv.org/abs/2312.10059v1), Konstantin Hebenstreit,Robert Praas,Matthias Samwald, 04-12-2023
      ### Categories
      Computers and Society
     ### Abstract
     Large language models (LLMs) demonstrate outstanding capabilities, but challenges remain regarding their ability to solve complex reasoning tasks, as well as their transparency, robustness, truthfulness, and ethical alignment. In this preliminary study, we compile a set of core principles for steering and evaluating the reasoning of LLMs by curating literature from several relevant strands of work: structured reasoning in LLMs, self-evaluation/self-reflection, explainability, AI system safety/security, guidelines for human critical thinking, and ethical/regulatory guidelines for AI. We identify and curate a list of 220 principles from literature, and derive a set of 37 core principles organized into seven categories: assumptions and perspectives, reasoning, information and evidence, robustness and security, ethics, utility, and implications. We conduct a small-scale expert survey, eliciting the subjective importance experts assign to different principles and lay out avenues for future work beyond our preliminary results. We envision that the development of a shared model of principles can serve multiple purposes: monitoring and steering models at inference time, improving model behavior during training, and guiding human evaluation of model reasoning.
     ### Bullet Points

     * The study compiled a set of core principles for steering and evaluating the reasoning of LLMs by curating literature from relevant strands of work

     * We identified and curate 220 principles from literature and derive 37 core principles organized into seven categories: assumptions and perspectives, reasoning, information and evidence, robustness and security, ethics, utility, and implications

     * A small-scale expert survey elicited the subjective importance experts assign to different principles and laid out avenues for future work beyond our preliminary results

     * The shared model of principles can serve multiple purposes such as monitoring and steering models at inference time, improving model behavior during training, and guiding human evaluation of model reasoning.




1. [Data Management For Large Language Models: A Survey](https://arxiv.org/abs/2312.01700v2), Zige Wang,Wanjun Zhong,Yufei Wang,Qi Zhu,Fei Mi,Baojun Wang,Lifeng Shang,Xin Jiang,Qun Liu, 04-12-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     .
     None


1. [Creative Agents: Empowering Agents with Imagination for Creative Tasks](https://arxiv.org/abs/2312.02519v1), Chi Zhang,Penglin Cai,Yuhui Fu,Haoqi Yuan,Zongqing Lu, 05-12-2023
      ### Categories
      Artificial Intelligence, Machine Learning
     ### Abstract
     ).
     None


1. [Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey](https://arxiv.org/abs/2312.03014v1), Shengchao Chen,Guodong Long,Jing Jiang,Dikai Liu,Chengqi Zhang, 05-12-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Computer Vision, Atmospheric and Oceanic Physics
     ### Abstract
     As artificial intelligence (AI) continues to rapidly evolve, the realm of Earth and atmospheric sciences is increasingly adopting data-driven models, powered by progressive developments in deep learning (DL). Specifically, DL techniques are extensively utilized to decode the chaotic and nonlinear aspects of Earth systems, and to address climate challenges via understanding weather and climate data. Cutting-edge performance on specific tasks within narrower spatio-temporal scales has been achieved recently through DL. The rise of large models, specifically large language models (LLMs), has enabled fine-tuning processes that yield remarkable outcomes across various downstream tasks, thereby propelling the advancement of general AI. However, we are still navigating the initial stages of crafting general AI for weather and climate. In this survey, we offer an exhaustive, timely overview of state-of-the-art AI methodologies specifically engineered for weather and climate data, with a special focus on time series and text data. Our primary coverage encompasses four critical aspects: types of weather and climate data, principal model architectures, model scopes and applications, and datasets for weather and climate. Furthermore, in relation to the creation and application of foundation models for weather and climate data understanding, we delve into the field's prevailing challenges, offer crucial insights, and propose detailed avenues for future research. This comprehensive approach equips practitioners with the requisite knowledge to make substantial progress in this domain. Our survey encapsulates the most recent breakthroughs in research on large, data-driven models for weather and climate data understanding, emphasizing robust foundations, current advancements, practical applications, crucial resources, and prospective research opportunities.
     ### Bullet Points

     * The Earth and atmospheric sciences are adopting data-driven models powered by deep learning (DL) techniques to decode chaotic and nonlinear aspects of Earth systems, and to address climate challenges via understanding weather and climate data

     * Cutting-edge performance on specific tasks within narrower spatio-temporal scales has been achieved through DL

     * The rise of large models, specifically large language models (LLMs), has enabled fine-tuning processes that yield remarkable outcomes across downstream tasks, propelling the advancement of general AI

     * The survey covers four critical aspects of state-of-the-art AI methodologies specifically engineered for weather, climate data, with a special focus on time series and text data

     * Our primary coverage encompasses four critical areas: types of weather and weather data, principal model architectures, model scopes and applications, and datasets for weather or climate

     * In relation to the creation and application of foundation models, we delve into the field'




1. [An LLM Compiler for Parallel Function Calling](https://arxiv.org/abs/2312.04511v1), Sehoon Kim,Suhong Moon,Ryan Tabrizi,Nicholas Lee,Michael W. Mahoney,Kurt Keutzer,Amir Gholami, 07-12-2023
      ### Categories
      Computation and Language
     ### Abstract
     Large Language Models (LLMs) have shown remarkable results on various complex reasoning benchmarks. The reasoning capabilities of LLMs enable them to execute function calls, using user-provided functions to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has expanded LLMs' scope to include multi-function calling, where LLMs are equipped with a variety of functions and select the proper functions based on the context. Multi-function calling abilities of LLMs have catalyzed LLM-based software development, allowing them to tackle more complex problems. However, current methods for multi-function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multi-function calling. Drawing from the principles of classical compilers, LLMCompiler streamlines parallel function calling with three components: (i) an LLM Planner, formulating execution strategies and dependencies; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. LLMCompiler automatically computes an optimized orchestration for the function calls and can be used with open-source models such as LLaMA-2. We have benchmarked LLMCompiler on a range of tasks including cases with non-trivial inter-dependency between function calls, as well as cases that require dynamic replanning based on intermediate results. We observe consistent latency speedup of up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to ~9% as compared to ReAct. Additionally, LLMCompiler achieves up to 1.35x latency gain over OpenAI's recent parallel function calling, while achieving similar accuracy.
     ### Bullet Points

     * LLMs have shown remarkable results on complex reasoning benchmarks, enabling them to execute function calls using user-provided functions

     * They have expanded their scope to include multi-function calling, where they are equipped with a variety of functions and select the appropriate functions based on the context

     * LLMCompiler streamlines parallel function calling with three components: an LLM Planner, formulating execution strategies and dependencies, a Task Fetching Unit, dispatching function calling tasks, and an Executor, executing these tasks in parallel

     * This automates an optimized orchestration for the function calls and can be used with open-source models such as LLaMA-2

     * We have observed consistent latency speedup, cost savings, and accuracy improvement of up to 9% as compared to ReAct and achieve up to 1.35x latency gain over OpenAI's recent parallel functions calling while achieving similar accuracy.




1. [Are We Testing or Being Tested? Exploring the Practical Applications of Large Language Models in Software Testing](https://arxiv.org/abs/2312.04860v1), Robson Santos,Italo Santos,Cleyton Magalhaes,Ronnie de Souza Santos, 08-12-2023
      ### Categories
      Software Engineering
     ### Abstract
     A Large Language Model (LLM) represents a cutting-edge artificial intelligence model that generates coherent content, including grammatically precise sentences, human-like paragraphs, and syntactically accurate code snippets. LLMs can play a pivotal role in software development, including software testing. LLMs go beyond traditional roles such as requirement analysis and documentation and can support test case generation, making them valuable tools that significantly enhance testing practices within the field. Hence, we explore the practical application of LLMs in software testing within an industrial setting, focusing on their current use by professional testers. In this context, rather than relying on existing data, we conducted a cross-sectional survey and collected data within real working contexts, specifically, engaging with practitioners in industrial settings. We applied quantitative and qualitative techniques to analyze and synthesize our collected data. Our findings demonstrate that LLMs effectively enhance testing documents and significantly assist testing professionals in programming tasks like debugging and test case automation. LLMs can support individuals engaged in manual testing who need to code. However, it is crucial to emphasize that, at this early stage, software testing professionals should use LLMs with caution while well-defined methods and guidelines are being built for the secure adoption of these tools.
     ### Bullet Points

     * LLMs are a cutting-edge artificial intelligence model that generates coherent content and supports test case generation

     * They play a pivotal role in software development, including software testing, and are valuable tools that enhance testing practices within the field

     * A cross-sectional survey was conducted to gather data from practitioners in industrial settings, and quantitative and qualitative techniques were applied to analyze and synthesize the collected data

     * The findings demonstrate that LLM enhances testing documents and significantly assists testing professionals in programming tasks like debugging and test case automation

     * However, software testing professionals should use LLM with caution while well-defined methods and guidelines are being built for secure adoption.




1. [KwaiAgents: Generalized Information-seeking Agent System with Large Language Models](https://arxiv.org/abs/2312.04889v3), Haojie Pan,Zepeng Zhai,Hao Yuan,Yaojia Lv,Ruiji Fu,Ming Liu,Zhongyuan Wang,Bing Qin, 08-12-2023
      ### Categories
      Artificial Intelligence, Computation and Language, Machine Learning
     ### Abstract
     Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update and retrieve information from its internal memory, plan and execute actions using a time-aware search-browse toolkit, and ultimately provide a comprehensive response. We further investigate the system's performance when powered by LLMs less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework, designed to ensure even an open-sourced 7B or 13B model performs well among many agent systems. We exploit both benchmark and human evaluations to systematically validate these capabilities. Extensive experiments show the superiority of our agent system compared to other autonomous agents and highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.
     ### Bullet Points

     * The paper introduces KwaiAgents, a generalized information-seeking agent system based on LLMs

     * The agent system employs LLM as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents

     * It can update and retrieve information from its internal memory, plan and execute actions using a time-aware search-browse toolkit, and provide a comprehensive response

     * The system's performance is evaluated using benchmark and human evaluations, and the MAT framework is used to ensure even an open-sourced 7B or 13B model performs well among many agent systems

     * Extensive experiments demonstrate the superiority and enhanced generalized agent-abilities of the agent system.




1. [Large-scale Training of Foundation Models for Wearable Biosignals](https://arxiv.org/abs/2312.05409v1), Salar Abbaspourazad,Oussama Elachqar,Andrew C. Miller,Saba Emrani,Udhyakumar Nallasamy,Ian Shapiro, 08-12-2023
      ### Categories
      Machine Learning, Artificial Intelligence, Signal Processing
     ### Abstract
     Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for developing neural network models for biosignals. To address this challenge, we have employed self-supervised learning using the unlabeled sensor data collected under informed consent from the large longitudinal Apple Heart and Movement Study (AHMS) to train foundation models for two common biosignals: photoplethysmography (PPG) and electrocardiogram (ECG) recorded on Apple Watch. We curated PPG and ECG datasets from AHMS that include data from ~141K participants spanning ~3 years. Our self-supervised learning framework includes participant level positive pair selection, stochastic augmentation module and a regularized contrastive loss optimized with momentum training, and generalizes well to both PPG and ECG modalities. We show that the pre-trained foundation models readily encode information regarding participants' demographics and health conditions. To the best of our knowledge, this is the first study that builds foundation models using large-scale PPG and ECG data collected via wearable consumer devices $\unicode{x2013}$ prior works have commonly used smaller-size datasets collected in clinical and experimental settings. We believe PPG and ECG foundation models can enhance future wearable devices by reducing the reliance on labeled data and hold the potential to help the users improve their health.
     ### Bullet Points

     * The study uses unlabeled sensor data from the Apple Heart and Movement Study (AHMS) to train foundation models for two common biosignals: photoplethysmography (PPG) and electrocardiogram (ECG) recorded on Apple Watch

     * The pre-trained foundation models readily encode information regarding participants' demographics and health conditions

     * This is the first study to build foundation models using large-scale PPG and ECG data collected via wearable consumer devices, reducing the reliance on labeled data and improving user health.




1. [Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs](https://arxiv.org/abs/2312.05934), Oded Ovadia,Menachem Brief,Moshik Mishaeli,Oren Elisha, 10-12-2023
      ### Categories
      Artificial Intelligence, Computation and Language, Machine Learning
     ### Abstract
     Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.
     ### Bullet Points

    * The study compares unsupervised fine-tuning and retrieval-augmented generation (RAG) for incorporating new information or refining the capabilities of LLMs on previously seen information

    * RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge

    * LLM struggle to learn new factual information through unsupervised good-tuneing, and exposing them to numerous variations of the same fact during training could alleviate this problem.





1. [LLM360: Towards Fully Transparent Open-Source LLMs](https://arxiv.org/abs/2312.06550v1), Zhengzhong Liu,Aurick Qiao,Willie Neiswanger,Hongyi Wang,Bowen Tan,Tianhua Tao,Junbo Li,Yuqi Wang,Suqi Sun,Omkar Pangarkar,Richard Fan,Yi Gu,Victor Miller,Yonghao Zhuang,Guowei He,Haonan Li,Fajri Koto,Liping Tang,Nikhil Ranjan,Zhiqiang Shen,Xuguang Ren,Roberto Iriondo,Cun Mu,Zhiting Hu,Mark Schulze,Preslav Nakov,Tim Baldwin,Eric P. Xing, 11-12-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     ). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.
     ### Bullet Points

     * Open-source effort is pushing boundaries of LLMs through larger and stronger models

     * More models will be released in the next few years, as we continue to push the boundaries of the field

     * (Note: This statement is not a complete summary, but rather a statement about a commitment to pushing the boundaries through open-source efforts.)




1. ["I Want It That Way": Enabling Interactive Decision Support Using Large Language Models and Constraint Programming](https://arxiv.org/abs/2312.06908v1), Connor Lawless,Jakob Schoeffer,Lindy Le,Kael Rowan,Shilad Sen,Cristina St. Hill,Jina Suh,Bahar Sarrafzadeh, 12-12-2023
      ### Categories
      Human-Computer Interaction
     ### Abstract
     A critical factor in the success of decision support systems is the accurate modeling of user preferences. Psychology research has demonstrated that users often develop their preferences during the elicitation process, highlighting the pivotal role of system-user interaction in developing personalized systems. This paper introduces a novel approach, combining Large Language Models (LLMs) with Constraint Programming to facilitate interactive decision support. We study this hybrid framework through the lens of meeting scheduling, a time-consuming daily activity faced by a multitude of information workers. We conduct three studies to evaluate the novel framework, including a diary study (n=64) to characterize contextual scheduling preferences, a quantitative evaluation of the system's performance, and a user study (n=10) with a prototype system. Our work highlights the potential for a hybrid LLM and optimization approach for iterative preference elicitation and design considerations for building systems that support human-system collaborative decision-making processes.
     ### Bullet Points

     * The paper proposes a hybrid approach combining LLMs and Constraint Programming to facilitate interactive decision support in decision support systems

     * The approach involves meeting scheduling, which is a time-consuming daily activity faced by information workers

     * Three studies were conducted to evaluate the hybrid framework, including a diary study, a quantitative evaluation of the system's performance, and a user study with a prototype system

     * The paper highlights the potential for a LLM and optimization approach for iterative preference elicitation and design considerations for building systems that support human-system collaborative decision-making processes.




1. [Alignment for Honesty](https://arxiv.org/abs/2312.07000v1), Yuqing Yang,Ethan Chern,Xipeng Qiu,Graham Neubig,Pengfei Liu, 12-12-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     , including honesty-aligned models, training and evaluation datasets for honesty alignment, concept glossary, as well as all relevant source code.
     ### Bullet Points

     * This includes honesty-aligned models, training and evaluation datasets for honesty alignment, concept glossary, and all relevant source code

     * It also includes a glossary of concepts and a reference to the source code for the model.




1. [Efficient Few-Shot Clinical Task Adaptation with Large Language Models](https://arxiv.org/abs/2312.07125v1), Kaipeng Zheng,Weiran Huang,Lichao Sun, 12-12-2023
      ### Categories
      Computer Vision
     ### Abstract
     Few-shot learning has been studied to adapt models to tasks with very few samples. It holds profound significance, particularly in clinical tasks, due to the high annotation cost of medical images. Several works have explored few-shot learning on medical images, yet they still require a large number of medical images for pre-training models to gain domain-specific priors. Vision foundation models recently have achieved remarkable success in natural images. Hence, adapting rapidly advancing vision foundation models from natural images to few-shot clinical tasks holds great promise. MedFMC has recently organized a challenge to shed more light on this topic at NeurIPS 2023. In this work, we present our challenge solution. We observe that a simple variant of fine-tuning with partial freezing shows remarkable performance. Empirical evidence demonstrates that this approach could outperform various common fine-tuning methods under limited sample sizes. Additionally, we explore enhanced utilization of semantic supervision to boost performance. We propose a novel approach that contextualizes labels via large language models (LLMs). Our findings reveal that the context generated by LLMs significantly enhances the discrimination of semantic embeddings for similar categories, resulting in a notable performance improvement of 3%-5% in 1-shot settings compared to commonly employed one-hot labels and other semantic supervision methods. Our solution secures the 1st place in the MedFMC challenge.
     ### Bullet Points

     * Few-shot learning is significant in clinical tasks due to the high annotation cost of medical images

     * However, it still requires a large number of images for pre-training models to gain domain-specific priors

     * Vision foundation models have achieved remarkable success in natural images

     * Adapting rapidly advancing vision foundation models from natural images to few-shot clinical tasks holds great promise

     * MedFMC has organized a challenge to shed more light on this topic at NeurIPS 2023

     * Our solution proposes a novel approach that contextualizes labels via large language models (LLMs)

     * LLMs significantly enhance the discrimination of semantic embeddings for similar categories, resulting in a notable performance improvement of 3%-5% in 1-shot settings compared to commonly employed one-hot labels and other semantic supervision methods

     * The solution secures the 1st place in the MedFCMC challenge.




1. [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](https://arxiv.org/abs/2312.11514v2), Keivan Alizadeh,Iman Mirzadeh,Dmitry Belenko,Karen Khatamifard,Minsik Cho,Carlo C Del Mundo,Mohammad Rastegari,Mehrdad Farajtabar, 12-12-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, "windowing" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.
     ### Bullet Points

     * The paper aims to efficiently run large language models (LLMs) that exceed the available DRAM capacity by storing the model parameters in flash memory but bringing them on demand to DRAM

     * The method involves constructing an inference cost model that takes into account the characteristics of flash memory and optimizing in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks

     * Two principal techniques are "windowing" and "row-column bundling" that increase the size of data chunks read from flash memory

     * These methods collectively enable running models up to twice the sizes of the available RAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively.




1. [LLMEval: A Preliminary Study on How to Evaluate Large Language Models](https://arxiv.org/abs/2312.07398v2), Yue Zhang,Ming Zhang,Haipeng Yuan,Shichun Liu,Yongyao Shi,Tao Gui,Qi Zhang,Xuanjing Huang, 12-12-2023
      ### Categories
      Artificial Intelligence, Computation and Language
     ### Abstract
     .
     None


1. [SM70: A Large Language Model for Medical Devices](https://arxiv.org/abs/2312.06974v1), Anubhav Bhatti,Surajsinh Parmar,San Lee, 12-12-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     We are introducing SM70, a 70 billion-parameter Large Language Model that is specifically designed for SpassMed's medical devices under the brand name 'JEE1' (pronounced as G1 and means 'Life'). This large language model provides more accurate and safe responses to medical-domain questions. To fine-tune SM70, we used around 800K data entries from the publicly available dataset MedAlpaca. The Llama2 70B open-sourced model served as the foundation for SM70, and we employed the QLoRA technique for fine-tuning. The evaluation is conducted across three benchmark datasets - MEDQA - USMLE, PUBMEDQA, and USMLE - each representing a unique aspect of medical knowledge and reasoning. The performance of SM70 is contrasted with other notable LLMs, including Llama2 70B, Clinical Camel 70 (CC70), GPT 3.5, GPT 4, and Med-Palm, to provide a comparative understanding of its capabilities within the medical domain. Our results indicate that SM70 outperforms several established models in these datasets, showcasing its proficiency in handling a range of medical queries, from fact-based questions derived from PubMed abstracts to complex clinical decision-making scenarios. The robust performance of SM70, particularly in the USMLE and PUBMEDQA datasets, suggests its potential as an effective tool in clinical decision support and medical information retrieval. Despite its promising results, the paper also acknowledges the areas where SM70 lags behind the most advanced model, GPT 4, thereby highlighting the need for further development, especially in tasks demanding extensive medical knowledge and intricate reasoning.
     ### Bullet Points

     * SM70 is a 70 billion-parameter Large Language Model designed for SpassMed's medical devices under the brand name 'JEE1'

     * It provides more accurate and safe responses to medical-domain questions

     * The model was fine-tuned using 800K data entries from the publicly available dataset MedAlpaca

     * The evaluation was conducted across three benchmark datasets - MEDQA, USMLE, PUBMEDQA and UMSLE - each representing a unique aspect of medical knowledge and reasoning

     * The results indicate that the model outperforms several established models in these datasets, showcasing its proficiency in handling a range of medical queries, from fact-based questions derived from PubMed abstracts to complex clinical decision-making scenarios

     * Despite its promising results, the paper acknowledges areas where it lags behind the most advanced model, GPT 4, highlighting the need for further development, especially in tasks




1. [Distributed Inference and Fine-tuning of Large Language Models Over The Internet](https://arxiv.org/abs/2312.08361v1), Alexander Borzunov,Max Ryabinin,Artem Chumachenko,Dmitry Baranchuk,Tim Dettmers,Younes Belkada,Pavel Samygin,Colin Raffel, 13-12-2023
      ### Categories
      Machine Learning, Distributed, Parallel, and Cluster Computing
     ### Abstract
     Large language models (LLMs) are useful in many NLP tasks and become more capable with size, with the best open-source models having over 50 billion parameters. However, using these 50B+ models requires high-end hardware, making them inaccessible to most researchers. In this work, we investigate methods for cost-efficient inference and fine-tuning of LLMs, comparing local and distributed strategies. We observe that a large enough model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network. This could allow running LLM efficiently by pooling together idle compute resources of multiple research groups and volunteers. We address two open problems: (1) how to perform inference and fine-tuning reliably if any device can disconnect abruptly and (2) how to partition LLMs between devices with uneven hardware, joining and leaving at will. In order to do that, we develop special fault-tolerant inference algorithms and load-balancing protocols that automatically assign devices to maximize the total system throughput. We showcase these algorithms in Petals - a decentralized system that runs Llama 2 (70B) and BLOOM (176B) over the Internet up to 10x faster than offloading for interactive generation. We evaluate the performance of our system in simulated conditions and a real-world setup spanning two continents.
     ### Bullet Points

     * The study investigates methods for cost-efficient inference and fine-tuning of large language models, comparing local and distributed strategies, and finding that a large enough model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network

     * The study also addresses two open problems: performing inference reliably if any device can disconnect abruptly and partitioning LLMs between devices with uneven hardware, joining and leaving at will

     * We demonstrate these algorithms in Petals, a decentralized system that runs Llama 2 (70B) and BLOOM (176B) over the Internet up to 10x faster than offloading for interactive generation.




1. [PromptBench: A Unified Library for Evaluation of Large Language Models](https://arxiv.org/abs/2312.07910v2), Kaijie Zhu,Qinlin Zhao,Hao Chen,Jindong Wang,Xing Xie, 13-12-2023
      ### Categories
      Artificial Intelligence, Computation and Language, Machine Learning
     ### Abstract
     and will be continuously supported.
     None


1. [CogAgent: A Visual Language Model for GUI Agents](https://arxiv.org/abs/2312.08914v2), Wenyi Hong,Weihan Wang,Qingsong Lv,Jiazheng Xu,Wenmeng Yu,Junhui Ji,Yan Wang,Zihan Wang,Yuxuan Zhang,Juanzi Li,Bin Xu,Yuxiao Dong,Ming Ding,Jie Tang, 14-12-2023
      ### Categories
      Computer Vision
     ### Abstract
     .
     None


1. [TigerBot: An Open Multilingual Multitask LLM](https://arxiv.org/abs/2312.08688v2), Ye Chen,Wei Cai,Liangmin Wu,Xiaowei Li,Zhanxuan Xin,Cong Fu, 14-12-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     We release and introduce the TigerBot family of large language models (LLMs), consisting of base and chat models, sized from 7, 13, 70 and 180 billion parameters. We develop our models embarking from Llama-2 and BLOOM, and push the boundary further in data, training algorithm, infrastructure, and application tools. Our models yield meaningful performance gain over SOTA open-source models, e.g., Llama-2, specifically 6% gain in English and 20% gain in Chinese. TigerBot model family also achieves leading performance in major academic and industrial benchmarks and leaderboards. We believe that TigerBot represents just a snapshot of lightning-fast progression in LLM open-source community. Therefore, we are thrilled to give back by publicly releasing our models and reporting our approach behind, with additional emphases on building SOTA LLMs in a democratized way and making LLMs of use in real-world applications.
     ### Bullet Points

     * TigerBot is a family of large language models with base and chat models sized from 7, 13, 70, and 180 billion parameters

     * They develop models from Llama-2 and BLOOM and push the boundary further in data, training algorithm, infrastructure, and application tools

     * Their models yield meaningful performance gain over SOTA open-source models, achieving leading performance in academic and industrial benchmarks and leaderboards

     * They believe that TigerBot represents just a snapshot of lightning-fast progress in the LLM open source community

     * They are excited to publicly release their models and report their approach behind, with additional emphases on building SOTA LLMs in a democratized way and making them of use in real-world applications.




1. [Catwalk: A Unified Language Model Evaluation Framework for Many Datasets](https://arxiv.org/abs/2312.10253v1), Dirk Groeneveld,Anas Awadalla,Iz Beltagy,Akshita Bhagia,Ian Magnusson,Hao Peng,Oyvind Tafjord,Pete Walsh,Kyle Richardson,Jesse Dodge, 15-12-2023
      ### Categories
      Computation and Language
     ### Abstract
     .
     None


1. [Extending Context Window of Large Language Models via Semantic Compression](https://arxiv.org/abs/2312.09571v1), Weizhi Fei,Xueyan Niu,Pingyi Zhou,Lu Hou,Bo Bai,Lei Deng,Wei Han, 15-12-2023
      ### Categories
      Computation and Language, Information Theory, Information Theory
     ### Abstract
     Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts. We propose a novel semantic compression method that enables generalization to texts that are 6-8 times longer, without incurring significant computational costs or requiring fine-tuning. Our proposed framework draws inspiration from source coding in information theory and employs a pre-trained model to reduce the semantic redundancy of long inputs before passing them to the LLMs for downstream tasks. Experimental results demonstrate that our method effectively extends the context window of LLMs across a range of tasks including question answering, summarization, few-shot learning, and information retrieval. Furthermore, the proposed semantic compression method exhibits consistent fluency in text generation while reducing the associated computational overhead.
     ### Bullet Points

     * The proposed semantic compression method enables generalization to texts that are 6-8 times longer without significant computational costs or fine-tuning

     * It draws inspiration from source coding in information theory and employs a pre-trained model to reduce the semantic redundancy of long inputs before passing them to the LLMs for downstream tasks

     * Experimental results demonstrate that the method effectively extends the context window and exhibits consistent fluency in text generation while reducing computational overhead.




1. [Faithful Persona-based Conversational Dataset Generation with Large Language Models](https://arxiv.org/abs/2312.10007v1), Pegah Jandaghi,XiangHai Sheng,Xinyi Bai,Jay Pujara,Hakim Sidahmed, 15-12-2023
      ### Categories
      Computation and Language, Machine Learning
     ### Abstract
     High-quality conversational datasets are essential for developing AI models that can communicate with users. One way to foster deeper interactions between a chatbot and its user is through personas, aspects of the user's character that provide insights into their personality, motivations, and behaviors. Training Natural Language Processing (NLP) models on a diverse and comprehensive persona-based dataset can lead to conversational models that create a deeper connection with the user, and maintain their engagement. In this paper, we leverage the power of Large Language Models (LLMs) to create a large, high-quality conversational dataset from a seed dataset. We propose a Generator-Critic architecture framework to expand the initial dataset, while improving the quality of its conversations. The Generator is an LLM prompted to output conversations. The Critic consists of a mixture of expert LLMs that control the quality of the generated conversations. These experts select the best generated conversations, which we then use to improve the Generator. We release Synthetic-Persona-Chat, consisting of 20k conversations seeded from Persona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our generation framework on different dimensions through extensive experiments, and observe that the losing rate of Synthetic-Persona-Chat against Persona-Chat during Turing test decreases from 17.2% to 8.8% over three iterations.
     ### Bullet Points

     * The paper proposes a large, high-quality conversational dataset using Large Language Models (LLMs) to foster deeper interactions between a chatbot and its user through personas, which provide insights into the user's personality, motivations, and behaviors

     * The generator-critic architecture framework is used to expand the initial dataset and improve the quality of its conversations

     * Synthetic-Persona-Chat, a 20k conversation seeded from Persona-Chhat, is evaluated through extensive experiments to evaluate its quality on different dimensions

     * The losing rate during Turing test decreases from 17.2% to 8.8%.




1. [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](https://arxiv.org/abs/2312.12456v1), Yixin Song,Zeyu Mi,Haotong Xie,Haibo Chen, 16-12-2023
      ### Categories
      Machine Learning, Operating Systems
     ### Abstract
     This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high locality inherent in LLM inference, characterized by a power-law distribution in neuron activation. This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity. Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU, only 18% lower than that achieved by a top-tier server-grade A100 GPU. This significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.
     ### Bullet Points

     * The paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a PC equipped with a single consumer-grade GPU

     * It exploits the high locality inherent in LLM inference, characterized by a power-law distribution in neuron activation, which results in a small subset of neurons being consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs

     * The GPU-CPU hybrid engine integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neural activation and computational sparsity

     * The average token generation rate is 13.20 tokens/s, and the peak is 29.08 tokens pers

     * This significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.




1. [A Survey of Reasoning with Foundation Models](https://arxiv.org/abs/2312.11562v4), Jiankai Sun,Chuanyang Zheng,Enze Xie,Zhengying Liu,Ruihang Chu,Jianing Qiu,Jiaqi Xu,Mingyu Ding,Hongyang Li,Mengzhe Geng,Yue Wu,Wenhai Wang,Junsong Chen,Zhangyue Yin,Xiaozhe Ren,Jie Fu,Junxian He,Wu Yuan,Qi Liu,Xihui Liu,Yu Li,Hao Dong,Yu Cheng,Ming Zhang,Pheng Ann Heng,Jifeng Dai,Ping Luo,Jingdong Wang,Ji-Rong Wen,Xipeng Qiu,Yike Guo,Hui Xiong,Qun Liu,Zhenguo Li, 17-12-2023
      ### Categories
      Artificial Intelligence, Computation and Language, Computer Vision, Machine Learning
     ### Abstract
     Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advancements in reasoning with foundation models, and contribute to the development of AGI.
     ### Bullet Points

     * The paper introduces seminal foundation models for reasoning, highlighting advancements in various reasoning tasks, methods, and benchmarks

     * The paper explores the potential future directions behind the emergence of reasoning abilities within foundation models, including multimodal learning, autonomous agents, and super alignment

     * The aim is to inspire researchers in their exploration of this field and contribute to the development of AGI.




1. [A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models](https://arxiv.org/abs/2312.10982v1), Aysan Esmradi,Daniel Wankit Yip,Chun Fai Chan, 18-12-2023
      ### Categories
      Cryptography and Security
     ### Abstract
     Ensuring the security of large language models (LLMs) is an ongoing challenge despite their widespread popularity. Developers work to enhance LLMs security, but vulnerabilities persist, even in advanced versions like GPT-4. Attackers exploit these weaknesses, highlighting the need for proactive cybersecurity measures in AI model development. This article explores two attack categories: attacks on models themselves and attacks on model applications. The former requires expertise, access to model data, and significant implementation time, while the latter is more accessible to attackers and has seen increased attention. Our study reviews over 100 recent research works, providing an in-depth analysis of each attack type. We identify the latest attack methods and explore various approaches to carry them out. We thoroughly investigate mitigation techniques, assessing their effectiveness and limitations. Furthermore, we summarize future defenses against these attacks. We also examine real-world techniques, including reported and our implemented attacks on LLMs, to consolidate our findings. Our research highlights the urgency of addressing security concerns and aims to enhance the understanding of LLM attacks, contributing to robust defense development in this evolving domain.
     ### Bullet Points

     * The article explores two attack categories: attacks on models themselves and attacks on model applications, highlighting the need for proactive cybersecurity measures in AI model development

     * The article reviews over 100 recent research works, provides an in-depth analysis of each attack type, investigates mitigation techniques, summarizes future defenses against these attacks, and examines real-world techniques to consolidate findings

     * The study highlights the urgency of addressing security concerns and contributing to robust defense development in this evolving domain.




1. [An In-depth Look at Gemini's Language Abilities](https://arxiv.org/abs/2312.11444v2), Syeda Nahida Akter,Zichun Yu,Aashiq Muhamed,Tianyue Ou,Alex B√§uerle,√Ångel Alexander Cabrera,Krish Dholakia,Chenyan Xiong,Graham Neubig, 18-12-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     
     None


1. [Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment](https://arxiv.org/abs/2312.12148v1), Lingling Xu,Haoran Xie,Si-Zhao Joe Qin,Xiaohui Tao,Fu Lee Wang, 19-12-2023
      ### Categories
      Computation and Language
     ### Abstract
     With the continuous growth in the number of parameters of transformer-based pretrained language models (PLMs), particularly the emergence of large language models (LLMs) with billions of parameters, many natural language processing (NLP) tasks have demonstrated remarkable success. However, the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning. The demands for fine-tuning PLMs, especially LLMs, have led to a surge in the development of PEFT methods, as depicted in Fig. 1. In this paper, we present a comprehensive and systematic review of PEFT methods for PLMs. We summarize these PEFT methods, discuss their applications, and outline future directions. Furthermore, we conduct experiments using several representative PEFT methods to better understand their effectiveness in parameter efficiency and memory efficiency. By offering insights into the latest advancements and practical applications, this survey serves as an invaluable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by PEFT in the context of PLMs.
     ### Bullet Points

     * The paper presents a comprehensive and systematic review of PEFT methods for PLMs, highlighting their effectiveness in parameter efficiency and memory efficiency, and conducting experiments using representative methods to better understand their effectiveness

     * This survey serves as a valuable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by PEFT in the context of LLMs.




1. [Generative Multimodal Models are In-Context Learners](https://arxiv.org/abs/2312.13286v1), Quan Sun,Yufeng Cui,Xiaosong Zhang,Fan Zhang,Qiying Yu,Zhengxiong Luo,Yueze Wang,Yongming Rao,Jingjing Liu,Tiejun Huang,Xinlong Wang, 20-12-2023
      ### Categories
      Computer Vision
     ### Abstract
     The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research.
     ### Bullet Points

     * Emu2 is a generative multimodal model with 37 billion parameters that can enhance task-agnostic in-context learning capabilities of large multimodal models

     * It is trained on large-scale multimodal sequences and has strong multimodal understanding abilities

     * It sets a new record on multiple multimodal comprehension tasks in few-shot settings and achieves new state-of-the-art on challenging tasks such as question answering benchmarks and open-ended subject-driven generation

     * The code and models are publicly available for future research.




1. [Mini-GPTs: Efficient Large Language Models through Contextual Pruning](https://arxiv.org/abs/2312.12682v1), Tim Valicenti,Justice Vidal,Ritik Patnaik, 20-12-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Natural Language Processing
     ### Abstract
     In AI research, the optimization of Large Language Models (LLMs) remains a significant challenge, crucial for advancing the field's practical applications and sustainability. Building upon the foundational work of Professor Song Han's lab at MIT, this paper introduces a novel approach in developing Mini-GPTs via contextual pruning. Our methodology strategically prunes the computational architecture of traditional LLMs, like Phi-1.5, focusing on retaining core functionalities while drastically reducing model sizes. We employ the technique across diverse and complex datasets, including US law, Medical Q&A, Skyrim dialogue, English-Taiwanese translation, and Economics articles. The results underscore the efficiency and effectiveness of contextual pruning, not merely as a theoretical concept but as a practical tool in developing domain-specific, resource-efficient LLMs. Contextual pruning is a promising method for building domain-specific LLMs, and this research is a building block towards future development with more hardware compute, refined fine-tuning, and quantization.
     ### Bullet Points

     * The paper introduces a new approach to developing Mini-GPTs through contextual pruning, which strategically prunes the computational architecture of traditional LLMs, focusing on retaining core functionalities while drastically reducing model sizes

     * The technique is used across diverse and complex datasets, and the results demonstrate the efficiency and effectiveness of contextual pruning

     * This research is a building block towards future development with more hardware compute, refined fine-tuning, and quantization.




1. [Time is Encoded in the Weights of Finetuned Language Models](https://arxiv.org/abs/2312.13401v2), Kai Nylund,Suchin Gururangan,Noah A. Smith, 20-12-2023
      ### Categories
      Computation and Language
     ### Abstract
     We present time vectors, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as our experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appear to be positioned closer together in a manifold. Using this structure, we interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. We demonstrate the consistency of our findings across different tasks, domains, model sizes, and time scales. Our results suggest that time is encoded in the weight space of finetuned models.
     ### Bullet Points

     * Time vectors are a tool to customize language models to new time periods by finetuning a language model on data from a single time and subtracting the weights of the original pretrained model

     * This vector specifies a direction in weight space that improves performance on text from that time period, and is positioned closer together in a manifold

     * We interpolate between time vectors to induce new models that perform better on intervening and future time periods without additional training

     * Our findings suggest that time is encoded in the weight space of finetuned models.




1. [AppAgent: Multimodal Agents as Smartphone Users](https://arxiv.org/abs/2312.13771v2), Chi Zhang,Zhao Yang,Jiaxuan Liu,Yucheng Han,Xin Chen,Zebiao Huang,Bin Fu,Gang Yu, 21-12-2023
      ### Categories
      Computer Vision
     ### Abstract
     Recent advancements in large language models (LLMs) have led to the creation of intelligent agents capable of performing complex tasks. This paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications. Our framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps. Central to our agent's functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates a knowledge base that the agent refers to for executing complex tasks across different applications. To demonstrate the practicality of our agent, we conducted extensive testing over 50 tasks in 10 different applications, including social media, email, maps, shopping, and sophisticated image editing tools. The results affirm our agent's proficiency in handling a diverse array of high-level tasks.
     ### Bullet Points

     * The paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications through a simplified action space, mimicking human-like interactions, bypassing system back-end access, and generating a knowledge base for executing complex tasks across different applications

     * The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations

     * Extensive testing was conducted over 50 tasks in 10 different applications, demonstrating the agent's proficiency in handling a diverse array of high-level tasks.




1. [Exploring the intersection of Generative AI and Software Development](https://arxiv.org/abs/2312.14262v1), Filipe Calegario,Vanilson Bur√©gio,Francisco Erivaldo,Daniel Moraes Costa Andrade,Kailane Felix,Nathalia Barbosa,Pedro Lucas da Silva Lucena,C√©sar Fran√ßa, 21-12-2023
      ### Categories
      Software Engineering, Artificial Intelligence
     ### Abstract
     In the ever-evolving landscape of Artificial Intelligence (AI), the synergy between generative AI and Software Engineering emerges as a transformative frontier. This whitepaper delves into the unexplored realm, elucidating how generative AI techniques can revolutionize software development. Spanning from project management to support and updates, we meticulously map the demands of each development stage and unveil the potential of generative AI in addressing them. Techniques such as zero-shot prompting, self-consistency, and multimodal chain-of-thought are explored, showcasing their unique capabilities in enhancing generative AI models. The significance of vector embeddings, context, plugins, tools, and code assistants is underscored, emphasizing their role in capturing semantic information and amplifying generative AI capabilities. Looking ahead, this intersection promises to elevate productivity, improve code quality, and streamline the software development process. This whitepaper serves as a guide for stakeholders, urging discussions and experiments in the application of generative AI in Software Engineering, fostering innovation and collaboration for a qualitative leap in the efficiency and effectiveness of software development.
     ### Bullet Points

     * The whitepaper explores how generative AI techniques can revolutionize software development by mapping the demands of each development stage and exploring techniques such as zero-shot prompting, self-consistency, and multimodal chain-of-thought

     * The significance of vector embeddings, context, plugins, tools, and code assistants is highlighted, emphasizing their role in capturing semantic information

     * This intersection promises to elevate productivity, improve code quality, and streamline the software development process

     * This whitepaper serves as a guide for stakeholders, urging discussions and experiments in the application of generating AI in Software Engineering, fostering innovation and collaboration for a qualitative leap in the efficiency and effectiveness of software development.




1. [LARP: Language-Agent Role Play for Open-World Games](https://arxiv.org/abs/2312.17653v1), Ming Yan,Ruihao Li,Hao Zhang,Hao Wang,Zhilan Yang,Ji Yan, 24-12-2023
      ### Categories
      Artificial Intelligence
     ### Abstract
     .
     None


1. [Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4](https://arxiv.org/abs/2312.16171v2), Sondos Mahmoud Bsharat,Aidar Myrzakhan,Zhiqiang Shen, 26-12-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     .
     None


1. [Supervised Knowledge Makes Large Language Models Better In-context Learners](https://arxiv.org/abs/2312.15918v1), Linyi Yang,Shuibai Zhang,Zhuohao Yu,Guangsheng Bao,Yidong Wang,Jindong Wang,Ruochen Xu,Wei Ye,Xing Xie,Weizhu Chen,Yue Zhang, 26-12-2023
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Large Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering. The recent progress in large-scale generative models has further expanded their use in real-world language applications. However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored. While previous in-context learning research has focused on enhancing models to adhere to users' specific instructions and quality expectations, and to avoid undesired outputs, little to no work has explored the use of task-Specific fine-tuned Language Models (SLMs) to improve LLMs' in-context learning during the inference stage. Our primary contribution is the establishment of a simple yet effective framework that enhances the reliability of LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs benefit from discriminative models, and 3) minimizes hallucinations in generative tasks. Using our proposed plug-in method, enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality. We offer a comprehensive suite of resources, including 16 curated datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks. Our empirical analysis sheds light on the advantages of incorporating discriminative models into LLMs and highlights the potential of our methodology in fostering more reliable LLMs.
     ### Bullet Points

     * Large Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering

     * Recent progress in large-scale generative models has expanded their use in real-world language applications

     * However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored

     * The use of task-Specific fine-tuned Language Model (SLMs) has been explored to enhance LLM's reliability during the inference stage

     * Our proposed plug-in method enables enhanced versions of Llama 2 and ChatGPT to surpass their original versions in terms of generalisability, and minimizes hallucinations in generative tasks

     * Our methodology provides a comprehensive suite of resources, including 16 curated datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks

     * The empirical analysis highlights the advantages of incorporating discriminative




1. [Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs](https://arxiv.org/abs/2312.17080v1), Zhongshen Zeng,Pengguang Chen,Haiyun Jiang,Jiaya Jia, 28-12-2023
      ### Categories
      Computation and Language
     ### Abstract
     In this work, we introduce a novel evaluation paradigm for Large Language Models, one that challenges them to engage in meta-reasoning. This approach addresses critical shortcomings in existing math problem-solving benchmarks, traditionally used to evaluate the cognitive capabilities of agents. Our paradigm shifts the focus from result-oriented assessments, which often overlook the reasoning process, to a more holistic evaluation that effectively differentiates the cognitive capabilities among models. For example, in our benchmark, GPT-4 demonstrates a performance ten times more accurate than GPT3-5. The significance of this new paradigm lies in its ability to reveal potential cognitive deficiencies in LLMs that current benchmarks, such as GSM8K, fail to uncover due to their saturation and lack of effective differentiation among varying reasoning abilities. Our comprehensive analysis includes several state-of-the-art math models from both open-source and closed-source communities, uncovering fundamental deficiencies in their training and evaluation approaches. This paper not only advocates for a paradigm shift in the assessment of LLMs but also contributes to the ongoing discourse on the trajectory towards Artificial General Intelligence (AGI). By promoting the adoption of meta-reasoning evaluation methods similar to ours, we aim to facilitate a more accurate assessment of the true cognitive abilities of LLMs.
     ### Bullet Points

     * The paper introduces a new evaluation paradigm for Large Language Models that challenges them to engage in meta-reasoning

     * The paradigm shifts the focus from result-oriented assessments to a more holistic evaluation that effectively differentiates the cognitive capabilities among models

     * The significance of this paradigm lies in its ability to reveal potential cognitive deficiencies in LLMs that current benchmarks fail to uncover due to saturation and lack of effective differentiation among varying reasoning abilities

     * The paper also contributes to the ongoing discourse towards Artificial General Intelligence (AI).




1. [Experiential Co-Learning of Software-Developing Agents](https://arxiv.org/abs/2312.17025v2), Chen Qian,Yufan Dang,Jiahao Li,Wei Liu,Weize Chen,Cheng Yang,Zhiyuan Liu,Maosong Sun, 28-12-2023
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning, Software Engineering
     ### Abstract
     Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. These agents are now capable of collaborating seamlessly, splitting tasks and enhancing accuracy, thus minimizing the need for human involvement. However, these agents often approach a diverse range of tasks in isolation, without benefiting from past experiences. This isolation can lead to repeated mistakes and inefficient trials in task solving. To this end, this paper introduces Experiential Co-Learning, a novel framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for mutual reasoning. This paradigm, enriched with previous experiences, equips agents to more effectively address unseen tasks.
     ### Bullet Points

     * LLM-driven autonomous agents are now capable of collaborating seamlessly, splitting tasks, and enhancing accuracy

     * However, they often approach tasks in isolation without benefiting from past experiences, leading to repeated mistakes and inefficient trials in task solving

     * This paper introduces Experiential Co-Learning, a framework where instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these experiences for mutual reasoning, equipping agents to more effectively address unseen tasks.




1. [MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices](https://arxiv.org/abs/2312.16886v2), Xiangxiang Chu,Limeng Qiao,Xinyang Lin,Shuang Xu,Yang Yang,Yiming Hu,Fei Wei,Xinyu Zhang,Bo Zhang,Xiaolin Wei,Chunhua Shen, 28-12-2023
      ### Categories
      Computer Vision
     ### Abstract
     .
     None


1. [Large Language Models for Generative Information Extraction: A Survey](https://arxiv.org/abs/2312.17617v1), Derong Xu,Wei Chen,Wenjun Peng,Chao Zhang,Tong Xu,Xiangyu Zhao,Xian Wu,Yefeng Zheng,Enhong Chen, 29-12-2023
      ### Categories
      Computation and Language
     ### Abstract
     }.
     None


1. [Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models](https://arxiv.org/abs/2401.00127v1), Ashhadul Islam,Md. Rafiul Biswas,Wajdi Zaghouani,Samir Brahim Belhaouari,Zubair Shah, 30-12-2023
      ### Categories
      Computer Vision, Social and Information Networks
     ### Abstract
     $ $The synergy of language and vision models has given rise to Large Language and Vision Assistant models (LLVAs), designed to engage users in rich conversational experiences intertwined with image-based queries. These comprehensive multimodal models seamlessly integrate vision encoders with Large Language Models (LLMs), expanding their applications in general-purpose language and visual comprehension. The advent of Large Multimodal Models (LMMs) heralds a new era in Artificial Intelligence (AI) assistance, extending the horizons of AI utilization. This paper takes a unique perspective on LMMs, exploring their efficacy in performing image classification tasks using tailored prompts designed for specific datasets. We also investigate the LLVAs zero-shot learning capabilities. Our study includes a benchmarking analysis across four diverse datasets: MNIST, Cats Vs. Dogs, Hymnoptera (Ants Vs. Bees), and an unconventional dataset comprising Pox Vs. Non-Pox skin images. The results of our experiments demonstrate the model's remarkable performance, achieving classification accuracies of 85\%, 100\%, 77\%, and 79\% for the respective datasets without any fine-tuning. To bolster our analysis, we assess the model's performance post fine-tuning for specific tasks. In one instance, fine-tuning is conducted over a dataset comprising images of faces of children with and without autism. Prior to fine-tuning, the model demonstrated a test accuracy of 55\%, which significantly improved to 83\% post fine-tuning. These results, coupled with our prior findings, underscore the transformative potential of LLVAs and their versatile applications in real-world scenarios.
     ### Bullet Points

     * The paper explores the efficacy of Large Language and Vision Assistant models (LLVAs) in performing image classification tasks using tailored prompts designed for specific datasets and investigates their zero-shot learning capabilities

     * The results demonstrate the model's remarkable performance, achieving classification accuracies of 85%, 100%, 77% and 79% for the respective datasets without any fine-tuning

     * The paper also assesses the Model's performance post-fining for specific tasks, demonstrating its transformative potential in real-world scenarios.




1. [DocLLM: A layout-aware generative language model for multimodal document understanding](https://arxiv.org/abs/2401.00908v1), Dongsheng Wang,Natraj Raman,Mathieu Sibue,Zhiqiang Ma,Petr Babkin,Simerjot Kaur,Yulong Pei,Armineh Nourbakhsh,Xiaomo Liu, 31-12-2023
      ### Categories
      Computation and Language
     ### Abstract
     Enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.
     ### Bullet Points

     * DocLLM is a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual and spatial modalities

     * It focuses on bounding box information to incorporate the spatial layout structure, captures cross-alignment between text and spatial, and uses a pre-training objective to learn to infill text segments

     * The pre-trained model is fine-tuned using a large-scale instruction dataset covering four core document intelligence tasks, outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.




1. [Improving Text Embeddings with Large Language Models](https://arxiv.org/abs/2401.00368v1), Liang Wang,Nan Yang,Xiaolong Huang,Linjun Yang,Rangan Majumder,Furu Wei, 31-12-2023
      ### Categories
      Computation and Language, Information Retrieval
     ### Abstract
     In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across nearly 100 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.
     ### Bullet Points

     * The paper introduces a simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps

     * It uses proprietary LLMs to generate diverse synthetic data for text embeddding tasks across nearly 100 languages and fine-tunes open-source decoder-only LLM on the synthetic data using standard contrastive loss

     * Experiments show that the method achieves strong performance on highly competitive benchmarks without using any labeled data and sets new state-of-the-art results on BEIR and MTEB benchmarks.




1. [Opening A Pandora's Box: Things You Should Know in the Era of Custom GPTs](https://arxiv.org/abs/2401.00905v1), Guanhong Tao,Siyuan Cheng,Zhuo Zhang,Junmin Zhu,Guangyu Shen,Xiangyu Zhang, 31-12-2023
      ### Categories
      Cryptography and Security
     ### Abstract
     The emergence of large language models (LLMs) has significantly accelerated the development of a wide range of applications across various fields. There is a growing trend in the construction of specialized platforms based on LLMs, such as the newly introduced custom GPTs by OpenAI. While custom GPTs provide various functionalities like web browsing and code execution, they also introduce significant security threats. In this paper, we conduct a comprehensive analysis of the security and privacy issues arising from the custom GPT platform. Our systematic examination categorizes potential attack scenarios into three threat models based on the role of the malicious actor, and identifies critical data exchange channels in custom GPTs. Utilizing the STRIDE threat modeling framework, we identify 26 potential attack vectors, with 19 being partially or fully validated in real-world settings. Our findings emphasize the urgent need for robust security and privacy measures in the custom GPT ecosystem, especially in light of the forthcoming launch of the official GPT store by OpenAI.
     ### Bullet Points

     * The paper analyzes the security and privacy issues arising from the custom GPT platform, categorizing potential attack scenarios into three threat models based on the role of the malicious actor, and identifying critical data exchange channels in the GPTs

     * We identify 26 potential attack vectors, with 19 being partially or fully validated in real-world settings, and emphasize the need for robust security measures in the ecosystem, especially in light of the official GPT store by OpenAI.


