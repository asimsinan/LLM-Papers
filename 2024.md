
## 2024 (42 papers)

1. [A Computational Framework for Behavioral Assessment of LLM Therapists](https://arxiv.org/abs/2401.00820v1), Yu Ying Chiu,Ashish Sharma,Inna Wanyin Lin,Tim Althoff, 01-01-2024
      ### Categories
      Computation and Language, Human-Computer Interaction
     ### Abstract
     The emergence of ChatGPT and other large language models (LLMs) has greatly increased interest in utilizing LLMs as therapists to support individuals struggling with mental health challenges. However, due to the lack of systematic studies, our understanding of how LLM therapists behave, i.e., ways in which they respond to clients, is significantly limited. Understanding their behavior across a wide range of clients and situations is crucial to accurately assess their capabilities and limitations in the high-risk setting of mental health, where undesirable behaviors can lead to severe consequences. In this paper, we propose BOLT, a novel computational framework to study the conversational behavior of LLMs when employed as therapists. We develop an in-context learning method to quantitatively measure the behavior of LLMs based on 13 different psychotherapy techniques including reflections, questions, solutions, normalizing, and psychoeducation. Subsequently, we compare the behavior of LLM therapists against that of high- and low-quality human therapy, and study how their behavior can be modulated to better reflect behaviors observed in high-quality therapy. Our analysis of GPT and Llama-variants reveals that these LLMs often resemble behaviors more commonly exhibited in low-quality therapy rather than high-quality therapy, such as offering a higher degree of problem-solving advice when clients share emotions, which is against typical recommendations. At the same time, unlike low-quality therapy, LLMs reflect significantly more upon clients' needs and strengths. Our analysis framework suggests that despite the ability of LLMs to generate anecdotal examples that appear similar to human therapists, LLM therapists are currently not fully consistent with high-quality care, and thus require additional research to ensure quality care.
     ### Bullet Points

    * The paper proposes BOLT, a computational framework to study the conversational behavior of LLMs when employed as therapists, and develops an in-context learning method to quantitatively measure their behavior based on 13 psychotherapy techniques

    * The study compares LLM behavior against that of high-quality human therapy and explores how their behavior can be modulated to better reflect behaviors observed in low-quality therapy

    * Despite the ability to generate anecdotal examples that appear similar to human therapist, LLM therapy is currently not fully consistent with high quality care and requires additional research to ensure quality care.




1. [Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models](https://arxiv.org/abs/2401.00625v2), Guangji Bai,Zheng Chai,Chen Ling,Shiyu Wang,Jiaying Lu,Nan Zhang,Tingwei Shi,Ziyang Yu,Mengdan Zhu,Yifei Zhang,Carl Yang,Yue Cheng,Liang Zhao, 01-01-2024
      ### Categories
      Machine Learning
     ### Abstract
     The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between various resources and corresponding optimization techniques. A standardized set of evaluation metrics and datasets is also presented to facilitate consistent and fair comparisons across different models and techniques. By offering a comprehensive overview of the current sota and identifying open research avenues, this survey serves as a foundational reference for researchers and practitioners, aiding them in developing more sustainable and efficient LLMs in a rapidly evolving landscape.
     ### Bullet Points

    * This survey reviews techniques to enhance resource efficiency of LLMs, categorizing them based on their optimization focus and applicability across various stages of an LLM's lifecycle

    * The survey also presents a nuanced categorization of resource efficiency techniques by their specific resource types, providing a foundational reference for researchers and practitioners.




1. [General-purpose foundation models for increased autonomy in robot-assisted surgery](https://arxiv.org/abs/2401.00678v1), Samuel Schmidgall,Ji Woong Kim,Alan Kuntz,Ahmed Ezzat Ghazi,Axel Krieger, 01-01-2024
      ### Categories
      Robotics, Machine Learning, Quantitative Biology
     ### Abstract
     The dominant paradigm for end-to-end robot learning focuses on optimizing task-specific objectives that solve a single robotic problem such as picking up an object or reaching a target position. However, recent work on high-capacity models in robotics has shown promise toward being trained on large collections of diverse and task-agnostic datasets of video demonstrations. These models have shown impressive levels of generalization to unseen circumstances, especially as the amount of data and the model complexity scale. Surgical robot systems that learn from data have struggled to advance as quickly as other fields of robot learning for a few reasons: (1) there is a lack of existing large-scale open-source data to train models, (2) it is challenging to model the soft-body deformations that these robots work with during surgery because simulation cannot match the physical and visual complexity of biological tissue, and (3) surgical robots risk harming patients when tested in clinical trials and require more extensive safety measures. This perspective article aims to provide a path toward increasing robot autonomy in robot-assisted surgery through the development of a multi-modal, multi-task, vision-language-action model for surgical robots. Ultimately, we argue that surgical robots are uniquely positioned to benefit from general-purpose models and provide three guiding actions toward increased autonomy in robot-assisted surgery.
     ### Bullet Points

    * The dominant paradigm for end-to-end robot learning focuses on optimizing task-specific objectives

    * However, recent work on high-capacity models in robotics has shown promise towards being trained on large collections of diverse and task-agnostic datasets of video demonstrations

    * Surgical robot systems that learn from data have struggled to advance as quickly as other fields of robot learning due to lack of large-scale open-source data, difficulty in modeling soft-body deformations, and risk of harming patients when tested in clinical trials

    * The article aims to develop a multi-modal, multi-task, vision-language-action model for surgical robots to increase robot autonomy in robot-assisted surgery.




1. [If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents](https://arxiv.org/abs/2401.00812v2), Ke Yang,Jiateng Liu,John Wu,Chaoqi Yang,Yi R. Fung,Sha Li,Zixuan Huang,Xu Cao,Xingyao Wang,Yiquan Wang,Heng Ji,Chengxiang Zhai, 01-01-2024
      ### Categories
      Computation and Language
     ### Abstract
     The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.
     ### Bullet Points

    * The survey discusses the benefits of integrating code into LLMs' training data, including unlocking their reasoning ability, steering them to produce structured and precise intermediate steps, taking advantage of code compilation and execution environment, and tracing how these capabilities have led to their emergence as intelligent agents in downstream tasks

    * Key challenges and future directions are presented.




1. [The Earth is Flat? Unveiling Factual Errors in Large Language Models](https://arxiv.org/abs/2401.00761v1), Wenxuan Wang,Juluan Shi,Zhaopeng Tu,Youliang Yuan,Jen-tse Huang,Wenxiang Jiao,Michael R. Lyu, 01-01-2024
      ### Categories
      Software Engineering, Artificial Intelligence, Computation and Language
     ### Abstract
     Large Language Models (LLMs) like ChatGPT are foundational in various applications due to their extensive knowledge from pre-training and fine-tuning. Despite this, they are prone to generating factual and commonsense errors, raising concerns in critical areas like healthcare, journalism, and education to mislead users. Current methods for evaluating LLMs' veracity are limited by test data leakage or the need for extensive human labor, hindering efficient and accurate error detection. To tackle this problem, we introduce a novel, automatic testing framework, FactChecker, aimed at uncovering factual inaccuracies in LLMs. This framework involves three main steps: First, it constructs a factual knowledge graph by retrieving fact triplets from a large-scale knowledge database. Then, leveraging the knowledge graph, FactChecker employs a rule-based approach to generates three types of questions (Yes-No, Multiple-Choice, and WH questions) that involve single-hop and multi-hop relations, along with correct answers. Lastly, it assesses the LLMs' responses for accuracy using tailored matching strategies for each question type. Our extensive tests on six prominent LLMs, including text-davinci-002, text-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal that FactChecker can trigger factual errors in up to 45\% of questions in these models. Moreover, we demonstrate that FactChecker's test cases can improve LLMs' factual accuracy through in-context learning and fine-tuning (e.g., llama-2-13b-chat's accuracy increase from 35.3\% to 68.5\%). We are making all code, data, and results available for future research endeavors.
     ### Bullet Points

    * FactChecker is an automatic testing framework that aims to uncover factual errors in large language models like ChatGPT

    * It uses a rule-based approach to generate three types of questions that involve single-hop and multi-hop relations, along with correct answers, and assesses the LLMs' responses for accuracy using tailored matching strategies for each question type

    * The framework has been tested on six prominent LLM models, including text-davinci-002, text-dravinci-012, chatGPT-3.5-turbo, gpt-4, Vicuna, and LLaMA-2, and can trigger factual inaccuracies in up to 45% of questions in these models

    * We are making all code, data, and results available for future research endeavors.




1. [A Comprehensive Study of Knowledge Editing for Large Language Models](https://arxiv.org/abs/2401.01286v3), Ningyu Zhang,Yunzhi Yao,Bozhong Tian,Peng Wang,Shumin Deng,Mengru Wang,Zekun Xi,Shengyu Mao,Jintian Zhang,Yuansheng Ni,Siyuan Cheng,Ziwen Xu,Xin Xu,Jia-Chen Gu,Yong Jiang,Pengjun Xie,Fei Huang,Lei Liang,Zhiqiang Zhang,Xiaowei Zhu,Jun Zhou,Huajun Chen, 02-01-2024
      ### Categories
      Computation and Language, Artificial Intelligence, Computer Vision, Human-Computer Interaction, Machine Learning
     ### Abstract
     Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the knowledge editing problem and then provide a comprehensive review of cutting-edge approaches. Drawing inspiration from educational and cognitive research theories, we propose a unified categorization criterion that classifies knowledge editing methods into three groups: resorting to external knowledge, merging knowledge into the model, and editing intrinsic knowledge. Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive empirical evaluation of representative knowledge editing approaches. Additionally, we provide an in-depth analysis of knowledge location, which can give a deeper understanding of the knowledge structures inherent within LLMs. Finally, we discuss several potential applications of knowledge editing, outlining its broad and impactful implications.
     ### Bullet Points

    * The paper discusses the limitations of Large Language Models (LLMs) in understanding and generating text that closely mirrors human communication

    * The computational demands during training are significant, and frequent updates are necessary to correct outdated information or integrate new knowledge

    * There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications

    * Recent years have seen a burgeoning in the techniques of knowledge editing for LLMs

    * A unified categorization criterion is proposed that classifies knowledge editing methods into three groups: resorting to external knowledge, merging knowledge into the model, and editing intrinsic knowledge

    * A new benchmark, KnowEdit, is introduced for a comprehensive empirical evaluation of representative knowledge editing approaches

    * Additionally, an in-depth analysis of knowledge location provides a deeper understanding of the knowledge structures inherent within LLM

    * The paper concludes by discussing several potential applications of Knowledge Editing, outlining its broad and impactful




1. [A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models](https://arxiv.org/abs/2401.01313v3), S.M Towhidul Islam Tonmoy,S M Mehedi Zaman,Vinija Jain,Anku Rani,Vipula Rawte,Aman Chadha,Amitava Das, 02-01-2024
      ### Categories
      Computation and Language
     ### Abstract
     As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.
     ### Bullet Points

    * The paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs, including Retrieval Augmented Generation (Revshney et al., 2021), Knowledge Revieval (CoNLI), and CoVe

    * The paper categorizes these techniques based on dataset utilization, common tasks, feedback mechanisms, and retriever types, and analyzes the challenges and limitations inherent in these techniques for future research.




1. [LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning](https://arxiv.org/abs/2401.01325v1), Hongye Jin,Xiaotian Han,Jingfeng Yang,Zhimeng Jiang,Zirui Liu,Chia-Yuan Chang,Huiyuan Chen,Xia Hu, 02-01-2024
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can effectively extend existing LLMs' context window's length.
     ### Bullet Points

    * The work proposes extending LLMs' context window by themselves to fully utilize their inherent ability to handle long contexts without fine-tuning

    * The proposed method, Self-Extend, involves building bi-level attention information on the group level and neighbor level using the original model's self-attention, which does not require any training

    * The experiment results show that the proposed method can effectively extend existing LLM's context window's length.




1. [LLaMA Beyond English: An Empirical Study on Language Capability Transfer](https://arxiv.org/abs/2401.01055v2), Jun Zhao,Zhihao Zhang,Luhui Gao,Qi Zhang,Tao Gui,Xuanjing Huang, 02-01-2024
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     In recent times, substantial advancements have been witnessed in large language models (LLMs), exemplified by ChatGPT, showcasing remarkable proficiency across a range of complex tasks. However, many mainstream LLMs (e.g. LLaMA) are pretrained on English-dominant corpus, which limits their performance in other non-English languages. In this paper, we focus on how to effectively transfer the capabilities of language generation and following instructions to a non-English language. To answer this question, we conduct an extensive empirical investigation based on LLaMA, accumulating over 1440 GPU hours. We analyze the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer. To accurately assess the model's level of knowledge, we employ four widely used standardized testing benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a comprehensive evaluation of the model's response quality is conducted, considering aspects such as accuracy, fluency, informativeness, logical coherence, and harmlessness, based on LLM-Eval, a benchmarks consisting instruction tasks from 17 diverse categories. Our evaluation results demonstrate that comparable performance to state-of-the-art transfer models can be achieved with less than 1% of the pretraining data, both in terms of knowledge alignment and response quality. Furthermore, the experimental outcomes across the thirteen low-resource languages also exhibit similar trends. We anticipate that the conclusions revealed by the experiments will aid the community in developing non-English LLMs.
     ### Bullet Points

    * The paper focuses on how to transfer language generation and following instructions to a non-English language by conducting an empirical investigation based on LLaMA and analyzing the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer

    * Four standardized testing benchmarks are used to assess the model's level of knowledge, and a comprehensive evaluation of its response quality is conducted using LLM-Eval, a benchmark consisting instruction tasks from 17 diverse categories

    * Comparable performance to state-of-the-art transfer models can be achieved with less than 1% of the pretraining data, both in terms of knowledge alignment and response quality

    * Experimental outcomes across thirteen low-resource languages also exhibit similar trends

    * The conclusions revealed by the experiments will aid the community in developing non- English LLMs.




1. [Enhancing the medical foundation model with multi-scale and cross-modality feature learning](https://arxiv.org/abs/2401.01583v1), Weijian Huang,Cheng Li,Hong-Yu Zhou,Jiarun Liu,Hao Yang,Yong Liang,Shanshan Wang, 03-01-2024
      ### Categories
      Computer Vision
     ### Abstract
     The development of multi-modal medical foundation models has attracted significant attention in the field of medicine and healthcare due to their promising prospects in various clinical applications. One area of focus in this research direction is the extractions of features at different scales. While previous studies have explored feature learning at individual scales, investigation on integrating the diverse scales and modalities of information is lacking, which may hinder the potential for mutual reinforcement among these features. This paper aims to bridge this gap by proposing a method that effectively exploits multi-scale and cross-modality information to enhance the performance of medical foundation models. The proposed method simultaneously exploit features at the local, instance, modality and global aspects, facilitating comprehensive representation learning within the models. We evaluate the effectiveness of the proposed method on six open-source datasets across different clinical tasks, demonstrating its ability to enhance the performance of medical foundation models.
     ### Bullet Points

    * The paper proposes a method that effectively exploits multi-scale and cross-modality information to enhance the performance of medical foundation models

    * The proposed method leverages features at local, instance, modality, and global aspects, facilitating comprehensive representation learning within the models

    * We evaluated its effectiveness on six open-source datasets across different clinical tasks.




1. [Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review](https://arxiv.org/abs/2401.01519v2), Luoma Ke,(1),,Song Tong,(1),,Peng Cheng,(2),,Kaiping Peng,(1) ((1) Department of Psychology, Tsinghua University, (2) School of Social Science, Tsinghua University), 03-01-2024
      ### Categories
      Machine Learning, Artificial Intelligence
     ### Abstract
     This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologies in psychology, the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations. Researchers should responsibly use LLMs in psychological studies, adhering to ethical standards and considering the potential consequences of deploying these technologies in sensitive areas. Overall, the article provides a comprehensive overview of the current state of LLMs in psychology, exploring potential benefits and challenges. It serves as a call to action for researchers to leverage LLMs' advantages responsibly while addressing associated risks.
     ### Bullet Points

    * The paper explores the frontiers of large language models (LLMs) in psychology applications, exploring their impact on various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology

    * LLMs can simulate human cognition and behavior, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology

    * However, the paper cautions about their technical and ethical challenges, including data privacy, ethical implications, and the need for a deeper understanding of these models' limitations

    * Researchers should responsibly use these technologies in psychological studies, adhering to ethical standards and considering the potential consequences of deploying them in sensitive areas.




1. [Few-shot Adaptation of Multi-modal Foundation Models: A Survey](https://arxiv.org/abs/2401.01736v2), Fan Liu,Tianshu Zhang,Wenwen Dai,Wenwen Cai,Xiaocong Zhou,Delong Chen, 03-01-2024
      ### Categories
      Computer Vision
     ### Abstract
     Multi-modal (vision-language) models, such as CLIP, are replacing traditional supervised pre-training models (e.g., ImageNet-based pre-training) as the new generation of visual foundation models. These models with robust and aligned semantic representations learned from billions of internet image-text pairs and can be applied to various downstream tasks in a zero-shot manner. However, in some fine-grained domains like medical imaging and remote sensing, the performance of multi-modal foundation models often leaves much to be desired. Consequently, many researchers have begun to explore few-shot adaptation methods for these models, gradually deriving three main technical approaches: 1) prompt-based methods, 2) adapter-based methods, and 3) external knowledge-based methods. Nevertheless, this rapidly developing field has produced numerous results without a comprehensive survey to systematically organize the research progress. Therefore, in this survey, we introduce and analyze the research advancements in few-shot adaptation methods for multi-modal models, summarizing commonly used datasets and experimental setups, and comparing the results of different methods. In addition, due to the lack of reliable theoretical support for existing methods, we derive the few-shot adaptation generalization error bound for multi-modal models. The theorem reveals that the generalization error of multi-modal foundation models is constrained by three factors: domain gap, model capacity, and sample size. Based on this, we propose three possible solutions from the following aspects: 1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive knowledge utilization.
     ### Bullet Points

    * Three possible solutions for few-shot adaptation methods for multi-modal models are prompt-based, adapter-based and external knowledge-based methods

    * These solutions are based on the limited theoretical support for existing methods, domain gap, model capacity, and sample size, as well as adaptive domain generalization.




1. [Large Language Models Relearn Removed Concepts](https://arxiv.org/abs/2401.01814v1), Michelle Lo,Shay B. Cohen,Fazl Barez, 03-01-2024
      ### Categories
      Artificial Intelligence
     ### Abstract
     Advances in model editing through neuron pruning hold promise for removing undesirable concepts from large language models. However, it remains unclear whether models have the capacity to reacquire pruned concepts after editing. To investigate this, we evaluate concept relearning in models by tracking concept saliency and similarity in pruned neurons during retraining. Our findings reveal that models can quickly regain performance post-pruning by relocating advanced concepts to earlier layers and reallocating pruned concepts to primed neurons with similar semantics. This demonstrates that models exhibit polysemantic capacities and can blend old and new concepts in individual neurons. While neuron pruning provides interpretability into model concepts, our results highlight the challenges of permanent concept removal for improved model \textit{safety}. Monitoring concept reemergence and developing techniques to mitigate relearning of unsafe concepts will be important directions for more robust model editing. Overall, our work strongly demonstrates the resilience and fluidity of concept representations in LLMs post concept removal.
     ### Bullet Points

    * Neuron pruning can remove undesirable concepts from large language models, but it is unclear whether models have the capacity to reacquire pruned concepts after editing

    * To investigate this, we evaluate concept relearning in models by tracking concept saliency and similarity in pruned neurons during retraining

    * Models can quickly regain performance post-pruning by relocating advanced concepts to earlier layers and reallocating pruned Concepts to primed neurons with similar semantics

    * This demonstrates polysemantic capacities and can blend old and new concepts in individual neurons

    * While neuron pruning provides interpretability into model concepts, our results highlight the challenges of permanent concept removal for improved model textit safety

    * Monitoring concept emergence and developing techniques to mitigate re learning of unsafe concepts will be important directions for more robust model editing

    * Overall, our work highlights the resilience and fluidity of concept representations in LLMs post concept removal.




1. [Correctness Comparison of ChatGPT-4, Bard, Claude-2, and Copilot for Spatial Tasks](https://arxiv.org/abs/2401.02404v2), Hartwig H. Hochmair,Levente Juhasz,Takoda Kemp, 04-01-2024
      ### Categories
      Computers and Society
     ### Abstract
     Generative AI including large language models (LLMs) have recently gained significant interest in the geo-science community through its versatile task-solving capabilities including coding, spatial computations, generation of sample data, time-series forecasting, toponym recognition, or image classification. So far, the assessment of LLMs for spatial tasks has primarily focused on ChatGPT, arguably the most prominent AI chatbot, whereas other chatbots received less attention. To narrow this research gap, this study evaluates the correctness of responses for a set of 54 spatial tasks assigned to four prominent chatbots, i.e., ChatGPT-4, Bard, Claude-2, and Copilot. Overall, the chatbots performed well on spatial literacy, GIS theory, and interpretation of programming code and given functions, but revealed weaknesses in mapping, code generation, and code translation. ChatGPT-4 outperformed other chatbots across most task categories.
     ### Bullet Points

    * Generative AI, including LLMs, has gained interest in the geo-science community due to its versatile task-solving capabilities, including coding, spatial computations, generation of sample data, time-series forecasting, toponym recognition, or image classification

    * The study evaluated the correctness of responses for 54 spatial tasks assigned to four prominent chatbots, i.e

    * ChatGPT-4, Bard, Claude-2, and Copilot, and revealed weaknesses in mapping, code generation, and code translation.




1. [LLM Augmented LLMs: Expanding Capabilities through Composition](https://arxiv.org/abs/2401.02412v1), Rachit Bansal,Bidisha Samanta,Siddharth Dalmia,Nitish Gupta,Shikhar Vashishth,Sriram Ganapathy,Abhishek Bapna,Prateek Jain,Partha Talukdar, 04-01-2024
      ### Categories
      Machine Learning, Artificial Intelligence, Computation and Language, Computer Vision
     ### Abstract
     Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.
     ### Bullet Points

    * CALM introduces cross-attention between existing foundation models to compose their representations and enable new capabilities

    * CALM allows for scaling up LLMs on new tasks by 're-using' existing models along with a few additional parameters and data, and preserves existing capabilities

    * It applies to diverse domains and settings

    * A smaller model trained on low-resource languages results in an absolute improvement of up to 13% on tasks like translation into English and arithmetic reasoning, while a code-specific model improves 40% over the base model for code generation and explanation tasks.




1. [LLaMA Pro: Progressive LLaMA with Block Expansion](https://arxiv.org/abs/2401.02415v1), Chengyue Wu,Yukang Gan,Yixiao Ge,Zeyu Lu,Jiahao Wang,Ye Feng,Ping Luo,Ying Shan, 04-01-2024
      ### Categories
      Computation and Language
     ### Abstract
     Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.
     ### Bullet Points

    * The paper proposes a new post-pretraining method for LLMs with an expansion of Transformer blocks

    * The expanded blocks are tuned using only new corpus, improving the model's knowledge without catastrophic forgetting

    * LLaMA Pro-8.3B is a versatile foundation model that excels in general tasks, programming, and mathematics

    * It achieves advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLAMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent

    * The findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.




1. [LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model](https://arxiv.org/abs/2401.02330v2), Yichen Zhu,Minjie Zhu,Ning Liu,Zhicai Ou,Xiaofeng Mou,Jian Tang, 04-01-2024
      ### Categories
      Computer Vision, Computation and Language
     ### Abstract
     }.
     None


1. [TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385v1), Peiyuan Zhang,Guangtao Zeng,Tianduo Wang,Wei Lu, 04-01-2024
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     .
     None


1. [Understanding LLMs: A Comprehensive Overview from Training to Inference](https://arxiv.org/abs/2401.02038v2), Yiheng Liu,Hao He,Tianle Han,Xu Zhang,Mengyuan Liu,Jiaming Tian,Yutong Zhang,Jiaqi Wang,Xiaohui Gao,Tianyang Zhong,Yi Pan,Shaochen Xu,Zihao Wu,Zhengliang Liu,Xin Zhang,Shu Zhang,Xintao Hu,Tuo Zhang,Ning Qiang,Tianming Liu,Bao Ge, 04-01-2024
      ### Categories
      Computation and Language
     ### Abstract
     The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.
     ### Bullet Points

    * The introduction of ChatGPT has led to an increase in the utilization of Large Language Models (LLMs) for downstream tasks, with a focus on cost-efficient training and deployment

    * This trend represents the future development trend

    * The paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend, including data preprocessing, training architecture, pre-training tasks, parallel training, relevant content related to model fine-tuning, model compression, parallel computation, memory scheduling, and structural optimization

    * It also explores LLMs' utilization and provides insights into their future development.




1. [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954v1), DeepSeek-AI,:,Xiao Bi,Deli Chen,Guanting Chen,Shanhuang Chen,Damai Dai,Chengqi Deng,Honghui Ding,Kai Dong,Qiushi Du,Zhe Fu,Huazuo Gao,Kaige Gao,Wenjun Gao,Ruiqi Ge,Kang Guan,Daya Guo,Jianzhong Guo,Guangbo Hao,Zhewen Hao,Ying He,Wenjie Hu,Panpan Huang,Erhang Li,Guowei Li,Jiashi Li,Yao Li,Y.K. Li,Wenfeng Liang,Fangyun Lin,A.X. Liu,Bo Liu,Wen Liu,Xiaodong Liu,Xin Liu,Yiyuan Liu,Haoyu Lu,Shanghao Lu,Fuli Luo,Shirong Ma,Xiaotao Nie,Tian Pei,Yishi Piao,Junjie Qiu,Hui Qu,Tongzheng Ren,Zehui Ren,Chong Ruan,Zhangli Sha,Zhihong Shao,Junxiao Song,Xuecheng Su,Jingxiang Sun,Yaofeng Sun,Minghui Tang,Bingxuan Wang,Peiyi Wang,Shiyu Wang,Yaohui Wang,Yongji Wang,Tong Wu,Y. Wu,Xin Xie,Zhenda Xie,Ziwei Xie,Yiliang Xiong,Hanwei Xu,R.X. Xu,Yanhong Xu,Dejian Yang,Yuxiang You,Shuiping Yu,Xingkai Yu,B. Zhang,Haowei Zhang,Lecong Zhang,Liyue Zhang,Mingchuan Zhang,Minghua Zhang,Wentao Zhang,Yichao Zhang,Chenggang Zhao,Yao Zhao,Shangyan Zhou,Shunfeng Zhou,Qihao Zhu,Yuheng Zou, 05-01-2024
      ### Categories
      Computation and Language, Artificial Intelligence, Machine Learning
     ### Abstract
     The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.
     ### Bullet Points

    * The article discusses the study of scaling laws and DeepSeek LLM, a project focused on advancing open-source language models with a long-term perspective

    * We present findings that facilitate scaling of large scale models in two commonly used Open-source configurations, 7B and 67B

    * We introduce deepSeek LCLM and conduct supervised fine-tuning and Direct Preference Optimization on the dataset, resulting in the creation of deepseek chat models

    * The evaluation results demonstrate that DeepSeven LLM 67Be surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning

    * Additionally, open-ended evaluations reveal that Deepseek LML 67BE Chat exhibits superior performance compared to GPT-3.5.




1. [From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models](https://arxiv.org/abs/2401.02777v1), Na Liu,Liangyu Chen,Xiaoyu Tian,Wei Zou,Kaijiang Chen,Ming Cui, 05-01-2024
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile conversational agents.
     ### Bullet Points

    * The paper introduces RAISE, an advanced architecture that enhances the integration of LLMs like GPT-4 into conversational agents

    * It entails a dual-component memory system that mirrors human short-term and long-term memory to maintain context and continuity in conversations

    * The approach enhances agent controllability and adaptability in complex, multi-turn dialogues

    * Preliminary evaluations suggest it has some advantages over traditional agents, indicating its potential for broader applications

    * This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile chatbots.




1. [Thousands of AI Authors on the Future of AI](https://arxiv.org/abs/2401.02843v1), Katja Grace,Harlan Stewart,Julia Fabienne Sandkühler,Stephen Thomas,Ben Weinstein-Raun,Jan Brauner, 05-01-2024
      ### Categories
      Computers and Society, Artificial Intelligence, Machine Learning
     ### Abstract
     Most respondents expressed substantial uncertainty about the long-term value of AI progress: While 68.3% thought good outcomes from superhuman AI are more likely than bad, of these net optimists 48% gave at least a 5% chance of extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes. Between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that "substantial" or "extreme" concern is warranted about six different AI-related scenarios, including misinformation, authoritarian control, and inequality. There was disagreement about whether faster or slower AI progress would be better for the future of humanity. However, there was broad agreement that research aimed at minimizing potential risks from AI systems ought to be prioritized more.
     ### Bullet Points

    * Most respondents expressed uncertainty about the long-term value of AI progress, with 68.3% believing good outcomes from superhuman AI are more likely than bad

    * Net optimists gave at least a 5% chance of extremely bad outcomes such as human extinction, while net pessimists give 5% or more to extremely good outcomes

    * Nearly half of respondents gave a 10% chance to advanced AI leading to outcomes as bad as human survival

    * More than half suggested "substantial" or "extreme" concern is warranted about six different AI-related scenarios, including misinformation, authoritarian control, and inequality

    * There was disagreement about whether faster or slower AI progress would be better for the future of humanity, but there was broad agreement that research aimed at minimizing potential risks from AI systems should be prioritized more.




1. [Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon](https://arxiv.org/abs/2401.03462v1), Peitian Zhang,Zheng Liu,Shitao Xiao,Ninglu Shao,Qiwei Ye,Zhicheng Dou, 07-01-2024
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     The utilization of long contexts poses a big challenge for large language models due to their limited context window length. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose Activation Beacon, which condenses LLM's raw activations into more compact forms such that it can perceive a much longer context with a limited context window. Activation Beacon is introduced as a plug-and-play module for the LLM. It fully preserves the LLM's original capability on short contexts while extending the new capability on processing longer contexts. Besides, it works with short sliding windows to process the long context, which achieves a competitive memory and time efficiency in both training and inference. Activation Beacon is learned by the auto-regression task conditioned on a mixture of beacons with diversified condensing ratios. Thanks to such a treatment, it can be efficiently trained purely with short-sequence data in just 10K steps, which consumes less than 9 hours on a single 8xA800 GPU machine. The experimental studies show that Activation Beacon is able to extend Llama-2-7B's context length by $\times100$ times (from 4K to 400K), meanwhile achieving a superior result on both long-context generation and understanding tasks. Our model and code will be available at the BGE repository.
     ### Bullet Points

    * Activation Beacon is a plug-and-play module for LLM that preserves the LLM's original capability on short contexts while extending the new capability on processing longer contexts

    * It works with short sliding windows to process the long context, which achieves a competitive memory and time efficiency in both training and inference

    * It can be efficiently trained purely with short-sequence data in just 10K steps, which consumes less than 9 hours on a single 8xA800 GPU machine

    * The model and code will be available at the BGE repository.




1. [Mixtral of Experts](https://arxiv.org/abs/2401.04088v1), Albert Q. Jiang,Alexandre Sablayrolles,Antoine Roux,Arthur Mensch,Blanche Savary,Chris Bamford,Devendra Singh Chaplot,Diego de las Casas,Emma Bou Hanna,Florian Bressand,Gianna Lengyel,Guillaume Bour,Guillaume Lample,Lélio Renard Lavaud,Lucile Saulnier,Marie-Anne Lachaux,Pierre Stock,Sandeep Subramanian,Sophia Yang,Szymon Antoniak,Teven Le Scao,Théophile Gervet,Thibaut Lavril,Thomas Wang,Timothée Lacroix,William El Sayed, 08-01-2024
      ### Categories
      Machine Learning, Computation and Language
     ### Abstract
     We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.
     ### Bullet Points

    * Mixtral 8x7B is a Sparse Mixture of Experts (SMoE) language model with 8 feedforward blocks (experts) at each layer

    * Each token has access to 47B parameters, but only 13B active parameters during inference

    * Mixtral was trained with a context size of 32k tokens and outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks

    * The model is fine-tuned to follow instructions, and both the base and instruct models are released under Apache 2.0 license.




1. [Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding](https://arxiv.org/abs/2401.04398v1), Zilong Wang,Hao Zhang,Chun-Liang Li,Julian Martin Eisenschlos,Vincent Perot,Zifeng Wang,Lesly Miculicich,Yasuhisa Fujii,Jingbo Shang,Chen-Yu Lee,Tomas Pfister, 09-01-2024
      ### Categories
      Computation and Language
     ### Abstract
     Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.
     ### Bullet Points

    * Table-based reasoning with large language models (LLMs) is a promising direction to tackle table-based question answering and fact verification tasks

    * It requires extracting underlying semantics from free-form questions and semi-structured tabular data

    * Chain-of-Thought and similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage it

    * We propose a framework that uses a table as a proxy for intermediate thoughts, using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain

    * LLMs can dynamically plan the next operation based on the results of the previous ones

    * This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem

    * The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions

    * It achieves new state-




1. [AUTOACT: Automatic Agent Learning from Scratch via Self-Planning](https://arxiv.org/abs/2401.05268v2), Shuofei Qiao,Ningyu Zhang,Runnan Fang,Yujie Luo,Wangchunshu Zhou,Yuchen Eleanor Jiang,Chengfei Lv,Huajun Chen, 10-01-2024
      ### Categories
      Computation and Language, Artificial Intelligence, Human-Computer Interaction, Machine Learning, Multiagent Systems
     ### Abstract
     .
     None


1. [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566v3), Evan Hubinger,Carson Denison,Jesse Mu,Mike Lambert,Meg Tong,Monte MacDiarmid,Tamera Lanham,Daniel M. Ziegler,Tim Maxwell,Newton Cheng,Adam Jermyn,Amanda Askell,Ansh Radhakrishnan,Cem Anil,David Duvenaud,Deep Ganguli,Fazl Barez,Jack Clark,Kamal Ndousse,Kshitij Sachan,Michael Sellitto,Mrinank Sharma,Nova DasSarma,Roger Grosse,Shauna Kravec,Yuntao Bai,Zachary Witten,Marina Favaro,Jan Brauner,Holden Karnofsky,Paul Christiano,Samuel R. Bowman,Logan Graham,Jared Kaplan,Sören Mindermann,Ryan Greenblatt,Buck Shlegeris,Nicholas Schiefer,Ethan Perez, 10-01-2024
      ### Categories
      Cryptography and Security, Artificial Intelligence, Computation and Language, Machine Learning, Software Engineering
     ### Abstract
     Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.
     ### Bullet Points

    * Yes, if an AI system learns a deceptive behavior, it can be detected and removed using current state-of-the-art safety training techniques

    * Proof-Of-Concept Examples of Deceptive Behavior in Large Language Models (LLMs) are constructed

    * The backdoor behavior can be persistent, so it is not removed by standard training techniques such as supervised fine-tuning, reinforcement learning, and adversarial training

    * The persistence of the behavior remains even when the chain of thought is distilled away

    * Adversarial Training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior.




1. [The Impact of Reasoning Step Length on Large Language Models](https://arxiv.org/abs/2401.04925v2), Mingyu Jin,Qinkai Yu,Dong shu,Haiyan Zhao,Wenyue Hua,Yanda Meng,Yongfeng Zhang,Mengnan Du, 10-01-2024
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences.
     ### Bullet Points

    * The effectiveness of Chain of Thought (CoT) in improving LLMs' reasoning abilities is unknown, but the correlation between the effectiveness and length of reasoning steps in prompts remains largely unknown

    * We conducted experiments that expand and compress the rationale reasoning steps within CoT demonstrations while keeping all other factors constant

    * The results indicate that lengthening the reasoning steps enhances LLM's reasoning abilities across multiple datasets, while shortening them significantly diminishes the reasoning abilities of models

    * The importance of the number of steps in CoT prompts and the relationship between performance and rationales used in demonstrations highlights the importance of maintaining the requisite length of inference

    * The advantages of increasing reasoning steps are task-dependent, with simpler tasks requiring fewer steps whereas complex tasks gain significantly from longer inference sequences.




1. [TrustLLM: Trustworthiness in Large Language Models](https://arxiv.org/abs/2401.05561v2), Lichao Sun,Yue Huang,Haoran Wang,Siyuan Wu,Qihui Zhang,Chujie Gao,Yixin Huang,Wenhan Lyu,Yixuan Zhang,Xiner Li,Zhengliang Liu,Yixin Liu,Yijue Wang,Zhikun Zhang,Bhavya Kailkhura,Caiming Xiong,Chaowei Xiao,Chunyuan Li,Eric Xing,Furong Huang,Hao Liu,Heng Ji,Hongyi Wang,Huan Zhang,Huaxiu Yao,Manolis Kellis,Marinka Zitnik,Meng Jiang,Mohit Bansal,James Zou,Jian Pei,Jian Liu,Jianfeng Gao,Jiawei Han,Jieyu Zhao,Jiliang Tang,Jindong Wang,John Mitchell,Kai Shu,Kaidi Xu,Kai-Wei Chang,Lifang He,Lifu Huang,Michael Backes,Neil Zhenqiang Gong,Philip S. Yu,Pin-Yu Chen,Quanquan Gu,Ran Xu,Rex Ying,Shuiwang Ji,Suman Jana,Tianlong Chen,Tianming Liu,Tianyi Zhou,Willian Wang,Xiang Li,Xiangliang Zhang,Xiao Wang,Xing Xie,Xun Chen,Xuyu Wang,Yan Liu,Yanfang Ye,Yinzhi Cao,Yong Chen,Yue Zhao, 10-01-2024
      ### Categories
      Computation and Language
     ### Abstract
     Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.
     ### Bullet Points

    * The paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, which includes principles and benchmarks for different dimensions

    * It also discusses open challenges and future directions, and emphasizes the importance of transparency in the models and technologies involved in their effectiveness.




1. [Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems](https://arxiv.org/abs/2401.05778v1), Tianyu Cui,Yanling Wang,Chuanpu Fu,Yong Xiao,Sijia Li,Xinhao Deng,Yunpeng Liu,Qinglin Zhang,Ziyi Qiu,Peiyang Li,Zhixing Tan,Junwu Xiong,Xinyu Kong,Zujie Wen,Ke Xu,Qi Li, 11-01-2024
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to facilitate the risk assessment of LLM systems. We hope that this paper can help LLM participants embrace a systematic perspective to build their responsible LLM systems.
     ### Bullet Points

    * The paper proposes a comprehensive taxonomy that systematically analyzes potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies

    * It also reviews prevalent benchmarks to facilitate the risk assessment of LLM systems.




1. [Seven Failure Points When Engineering a Retrieval Augmented Generation System](https://arxiv.org/abs/2401.05856v1), Scott Barnett,Stefanus Kurniawan,Srikanth Thudumu,Zach Brannelly,Mohamed Abdelrazek, 11-01-2024
      ### Categories
      Software Engineering
     ### Abstract
     Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.
     ### Bullet Points

    * Software engineers are using Retrieval Augmented Generation (RAG) to add semantic search capabilities to applications

    * RAG systems aim to reduce the problem of hallucinated responses from LLMs, link sources/references to generated responses, and remove the need for annotating documents with meta-data

    * However, the RAG system suffers from limitations inherent to information retrieval systems and from reliance on LLM

    * This paper presents an experience report on the failure points of RAG Systems from three case studies from different domains: research, education, and biomedical

    * We share the lessons learned and present 7 failure points to consider when designing a RAG System

    * The two key takeaways arising from our work are: 1) Validation is only feasible during operation

    * 2) Robustness of aRAG system evolves rather than designed in at the start.




1. [The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models](https://arxiv.org/abs/2401.05618v1), Matthew Renze,Erhan Guven, 11-01-2024
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance. However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads to an average per-token cost reduction of 22.67%. These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques. In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs.
     ### Bullet Points

    * The paper presents Concise Chain-of-Thought (CCoT) prompting, which reduces response length and correct-answer accuracy by 48.70% for both GPT-3.5 and GPT-4 with a MCQA benchmark

    * The results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques, as well as general insight for AI researchers studying the emergent behavior of step-by-step reasoning.




1. [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/abs/2401.06373v1), Yi Zeng,Hongpeng Lin,Jingwen Zhang,Diyi Yang,Ruoxi Jia,Weiyan Shi, 12-01-2024
      ### Categories
      Computation and Language, Artificial Intelligence
     ### Abstract
     Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP and, found a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs
     ### Bullet Points

    * The paper explores how to persuade LLMs to jailbreak them as human-like communicators, and explores the intersection between everyday language interaction and AI safety

    * The paper proposes a persuasive adversarial prompt taxonomy derived from social science research, and uses it to automatically generate interpretable persuasive antagonistic prompts (PAP)

    * The results show that PAP consistently achieves an attack success rate of over $92%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks

    * On the defense side, it explores various mechanisms against PAP and advocates for more fundamental mitigation for highly interactive LLM.




1. [Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender](https://arxiv.org/abs/2401.06561v1), Yuqi Zhang,Liang Ding,Lefei Zhang,Dacheng Tao, 12-01-2024
      ### Categories
      Computation and Language
     ### Abstract
     
     None


1. [RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture](https://arxiv.org/abs/2401.08406v2), Angels Balaguer,Vinamra Benara,Renato Luiz de Freitas Cunha,Roberto de M. Estevão Filho,Todd Hendry,Daniel Holstein,Jennifer Marsman,Nick Mecklenburg,Sara Malvar,Leonardo O. Nunes,Rafael Padilha,Morris Sharp,Bruno Silva,Swati Sharma,Vijay Aski,Ranveer Chandra, 16-01-2024
      ### Categories
      Computation and Language, Machine Learning
     ### Abstract
     There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.
     ### Bullet Points

    * The paper proposes a pipeline for fine-tuning and RAG, which involves extracting information from PDFs, generating questions and answers, and leveraging GPT-4 for evaluating the results

    * The pipeline consists of multiple stages, including extracting data from PDF and generating answers

    * The results demonstrate the effectiveness of the dataset generation pipeline in capturing geographic-specific knowledge and the quantitative and qualitative benefits of RAG and Fine-Tuning, with an accuracy increase of over 6 p.p

    * and cumulative with RAG

    * In an in-depth study on an agricultural dataset, the results demonstrate that the finetuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%

    * This suggests that systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of L




1. [MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World](https://arxiv.org/abs/2401.08577), Yining Hong,Zishuo Zheng,Peihao Chen,Yian Wang,Junyan Li,Chuang Gan, 16-01-2024
      ### Categories
      Computer Vision, Artificial Intelligence, Computation and Language, Machine Learning, Robotics
     ### Abstract
     Human beings possess the capability to multiply a melange of multisensory cues while actively exploring and interacting with the 3D world. Current multi-modal large language models, however, passively absorb sensory data as inputs, lacking the capacity to actively interact with the objects in the 3D environment and dynamically collect their multisensory information. To usher in the study of this area, we propose MultiPLY, a multisensory embodied large language model that could incorporate multisensory interactive data, including visual, audio, tactile, and thermal information into large language models, thereby establishing the correlation among words, actions, and percepts. To this end, we first collect Multisensory Universe, a large-scale multisensory interaction dataset comprising 500k data by deploying an LLM-powered embodied agent to engage with the 3D environment. To perform instruction tuning with pre-trained LLM on such generated data, we first encode the 3D scene as abstracted object-centric representations and then introduce action tokens denoting that the embodied agent takes certain actions within the environment, as well as state tokens that represent the multisensory state observations of the agent at each time step. In the inference time, MultiPLY could generate action tokens, instructing the agent to take the action in the environment and obtain the next multisensory state observation. The observation is then appended back to the LLM via state tokens to generate subsequent text or action tokens. We demonstrate that MultiPLY outperforms baselines by a large margin through a diverse set of embodied tasks involving object retrieval, tool use, multisensory captioning, and task decomposition.
     ### Bullet Points

    * MultiPLY is a multisensory embodied large language model that incorporates multisensorial interactive data, including visual, audio, tactile, and thermal information, into large language models to establish the correlation among words, actions, and percepts

    * We first collect Multisensory Universe dataset by deploying an LLM-powered agent to engage with the 3D environment

    * To perform instruction tuning with pre-trained LLM on generated data, we encode 3D scenes as abstracted object-centric representations and introduce action tokens and state tokens that represent the multisensorious state observations of the agent at each time step

    * Inference time could be generated by generating instruction tokens, instructing the agent to take the action in the environment and obtain the next multiplesensory state observation

    * The observation is then appended back to the LLM via State tokens to generate subsequent text or action token

    * This model outperforms baselines by




1. [A Survey of Resource-efficient LLM and Multimodal Foundation Models](https://arxiv.org/abs/2401.08092), Mengwei Xu,Wangsong Yin,Dongqi Cai,Rongjie Yi,Daliang Xu,Qipeng Wang,Bingyang Wu,Yihao Zhao,Chen Yang,Shihe Wang,Qiyang Zhang,Zhenyan Lu,Li Zhang,Shangguang Wang,Yuanchun Li,Yunxin Liu,Xin Jin,Xuanzhe Liu, 16-01-2024
      ### Categories
      Machine Learning, Artificial Intelligence, Distributed, Parallel, and Cluster Computing
     ### Abstract
     Large foundation models, including large language models (LLMs), vision transformers (ViTs), diffusion, and LLM-based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment. However, the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way, there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research, examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature, encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations. The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field.
     ### Bullet Points

    * The survey focuses on developing resource-efficient strategies to support the growth of large foundation models in a sustainable and scalable way, examining both algorithmic and systemic aspects

    * It provides an overview of current approaches to tackling resource challenges and potential future breakthroughs in this field.



1. [Knowledge Fusion of Large Language Models](https://arxiv.org/abs/2401.10491), Fanqi Wan,Xinting Huang,Deng Cai,Xiaojun Quan,Wei Bi,Shuming Shi, 19-01-2024
      ### Categories
      Computation and Language
     ### Abstract
     While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the fusion of LLMs can improve the performance of the target model across a range of capabilities such as reasoning, commonsense, and code generation. Our code, model weights, and data are public at \url{this https URL}.
     ### Bullet Points

    * Knowledge fusion for LLMs is a cost-effective and compelling approach to combining existing LLM capabilities and transferring them into a single LLM

    * By leveraging generative distributions, we externalize their collective knowledge and unique strengths, thereby elevating the capabilities of the target model beyond those of any individual source LLM, and we validate our approach using three popular LLM architectures - Llama-2, MPT, and OpenLLaMA - across various benchmarks and tasks

    * Our code, model weights, and data are public at urlthis https URL.


1. [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/abs/2401.13601), Duzhen Zhang,Yahan Yu,Chenxing Li,Jiahua Dong,Dan Su,Chenhui Chu,Dong Yu, 24-01-2024
      ### Categories
      Computation and Language
     ### Abstract
     In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of $26$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.
     ### Bullet Points

    * The paper provides a comprehensive survey to facilitate further research of MM-LLMs, including general design formulations, introductions of existing models, performance on mainstream benchmarks, key training recipes, and a real-time tracking website

    * The survey encourages further research and contributes to the ongoing advancement of the domain.



1. [The Power of Noise: Redefining Retrieval for RAG Systems](https://arxiv.org/abs/2401.14887), Florin Cuconasu,Giovanni Trappolini,Federico Siciliano,Simone Filice,Cesare Campagnano,Yoelle Maarek,Nicola Tonellotto,Fabrizio Silvestri, 26-01-2024
      ### Categories
      Information Retrieval, Computation and Language
     ### Abstract
     Retrieval-Augmented Generation (RAG) systems represent a significant advancement over traditional Large Language Models (LLMs). RAG systems enhance their generation ability by incorporating external data retrieved through an Information Retrieval (IR) phase, overcoming the limitations of standard LLMs, which are restricted to their pre-trained knowledge and limited context window. Most research in this area has predominantly concentrated on the generative aspect of LLMs within RAG systems. Our study fills this gap by thoroughly and critically analyzing the influence of IR components on RAG systems. This paper analyzes which characteristics a retriever should possess for an effective RAG's prompt formulation, focusing on the type of documents that should be retrieved. We evaluate various elements, such as the relevance of the documents to the prompt, their position, and the number included in the context. Our findings reveal, among other insights, that including irrelevant documents can unexpectedly enhance performance by more than 30% in accuracy, contradicting our initial assumption of diminished quality. These results underscore the need for developing specialized strategies to integrate retrieval with language generation models, thereby laying the groundwork for future research in this field.
     ### Bullet Points

    * Retrieval-Augmented Generation (RAG) systems enhance their generation ability by incorporating external data retrieved through an Information Retriation (IR) phase, overcoming limitations of traditional LLMs

    * The paper analyzes the influence of IR components on RAG systems, analyzing which characteristics a retriever should possess for an effective RAG prompt formulation, focusing on the type of documents that should be retrieved

    * Including irrelevant documents can unexpectedly enhance performance by more than 30% in accuracy, contradicting our initial assumption of diminished quality

    * Specified strategies should be developed to integrate retrieval with language generation models, laying the groundwork for future research in this field.



1. [A Comprehensive Survey of Compression Algorithms for Language Models](https://arxiv.org/abs/2401.15347), Seungcheol Park,Jaehyeon Choi,Sojin Lee,U Kang, 27-01-2024
      ### Categories
      Computation and Language, Artificial Intelligence, Natural Language Processing, Natural Language Processing
     ### Abstract
     How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of compression algorithms, and the desired properties of low-cost compression algorithms which have a significant impact due to the emergence of large language models. Finally, we introduce promising future research topics based on our survey results.
     ### Bullet Points

    * To compress language models without sacrificing accuracy, we can use diverse compression algorithms such as pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design

    * The paper surveys and summarizes diverse algorithms, selects representative algorithms, provides in-depth analyses, discusses the value of each category of compression algorithms, and proposes promising future research topics based on our survey results.


1. [Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884), Shi-Qi Yan,Jia-Chen Gu,Yun Zhu,Zhen-Hua Ling, 29-01-2024
      ### Categories
      Computation and Language
     ### Abstract
     Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.
     ### Bullet Points

    * We propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of LLM generation by utilizing a lightweight retrieval evaluator, large-scale web searches, and a decompose algorithm to selectively focus on key information and filter out irrelevant information in retrieved documents

    * CRAG is a plug-and-play approach that can be seamlessly coupled with various RAG-based approaches.



